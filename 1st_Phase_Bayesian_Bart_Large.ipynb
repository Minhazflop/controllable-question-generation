{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ab913c-7bdd-4fb7-bb98-e2d3e67f77d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     AutoTokenizer, BartForConditionalGeneration,\n\u001b[32m     13\u001b[39m     Seq2SeqTrainer, Seq2SeqTrainingArguments,\n\u001b[32m     14\u001b[39m     DataCollatorForSeq2Seq, EarlyStoppingCallback\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtranslate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleu_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpus_bleu, SmoothingFunction\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch121\\Lib\\site-packages\\datasets\\__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m4.0.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch121\\Lib\\site-packages\\datasets\\arrow_dataset.py:75\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontrib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_files\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch121\\Lib\\site-packages\\datasets\\arrow_reader.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Optional, Union\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpq\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontrib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch121\\Lib\\site-packages\\pyarrow\\parquet\\__init__.py:20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# distributed with this work for additional information\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\torch121\\Lib\\site-packages\\pyarrow\\parquet\\core.py:32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_parquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_parquet\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe pyarrow installation is not built with support \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor the Parquet file format (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(exc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1532\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1506\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1605\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import optuna\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, BartForConditionalGeneration,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    ")\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "import re\n",
    "import shutil\n",
    "import logging\n",
    "import sqlite3\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6ba46-cd4b-416c-a8c1-2c7bc565f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ffe86-0101-47ac-a00a-09fa2d96ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "optuna.trial.FixedTrial.seed = SEED\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Check CUDA\n",
    "logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "logger.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "logger.info(f\"CUDNN version: {torch.backends.cudnn.version()}\")\n",
    "\n",
    "# NLTK downloads\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "except Exception as e:\n",
    "    logger.error(f\"NLTK download failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Directories\n",
    "PROJECT_ROOT = r\"D:\\A_CSE499\"\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"outputLarge_B\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e9026-d0ef-42bb-82df-09c17edd6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and set device\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please check your PyTorch installation and NVIDIA drivers.\")\n",
    "\n",
    "# Punctuation cleaning function\n",
    "def fix_punctuation_spacing(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.replace(r'\\newline', ' ')\n",
    "    text = re.sub(r'\\s+([,.;:!?])', r'\\1', text)\n",
    "    text = re.sub(r'([,.;:!?])([^\\s\\W])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\(\\s+', '(', text)\n",
    "    text = re.sub(r'\\s+\\)', ')', text)\n",
    "    text = re.sub(r'\"\\s+', '\"', text)\n",
    "    text = re.sub(r'\\s+\"', '\"', text)\n",
    "    text = re.sub(r\"'\\s+\", \"'\", text)\n",
    "    text = re.sub(r\"\\s+'\", \"'\", text)\n",
    "    text = re.sub(r'\\s*[-–—]+\\s*', ' — ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391df25a-30e9-4909-a46f-5806d1626923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess_function(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for c, q in zip(batch['context'], batch['question']):\n",
    "        c_clean = fix_punctuation_spacing(str(c)) if c else \"\"\n",
    "        q_clean = fix_punctuation_spacing(str(q)) if q else \"\"\n",
    "        if q_clean.startswith(\"What is the\"):\n",
    "            q_clean = q_clean.replace(\"What is the\", \"What can you tell about\")\n",
    "        if c_clean and q_clean:\n",
    "            inputs.append(c_clean)\n",
    "            targets.append(q_clean)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n",
    "    return model_inputs\n",
    "\n",
    "# Load and validate datasets\n",
    "def validate_csv(file_path, required_columns):\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.error(f\"CSV file not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            logger.error(f\"Column '{col}' missing in {file_path}\")\n",
    "            raise ValueError(f\"Column '{col}' missing in {file_path}\")\n",
    "    return df\n",
    "\n",
    "# Load full datasets\n",
    "df_train_squad = validate_csv(os.path.join(DATA_DIR, \"squad_train_v1.csv\"), ['context', 'question'])\n",
    "df_val_squad = validate_csv(os.path.join(DATA_DIR, \"squad_validation_v1.csv\"), ['context', 'question'])\n",
    "df_train_mctest = validate_csv(os.path.join(DATA_DIR, \"mctest_train.csv\"), ['story', 'question'])\n",
    "df_val_mctest = validate_csv(os.path.join(DATA_DIR, \"mctest_validation.csv\"), ['story', 'question'])\n",
    "df_test_mctest = validate_csv(os.path.join(DATA_DIR, \"mctest_test.csv\"), ['story', 'question'])\n",
    "df_train_mctest = df_train_mctest.rename(columns={'story': 'context'})\n",
    "df_val_mctest = df_val_mctest.rename(columns={'story': 'context'})\n",
    "df_test_mctest = df_test_mctest.rename(columns={'story': 'context'})\n",
    "df_train_FairytaleQA = validate_csv(os.path.join(DATA_DIR, \"fairytaleqa_train.csv\"), ['content', 'question'])\n",
    "df_val_FairytaleQA = validate_csv(os.path.join(DATA_DIR, \"fairytaleqa_validation.csv\"), ['content', 'question'])\n",
    "df_test_FairytaleQA = validate_csv(os.path.join(DATA_DIR, \"fairytaleqa_test.csv\"), ['content', 'question'])\n",
    "df_train_FairytaleQA = df_train_FairytaleQA.rename(columns={'content': 'context'})\n",
    "df_val_FairytaleQA = df_val_FairytaleQA.rename(columns={'content': 'context'})\n",
    "df_test_FairytaleQA = df_test_FairytaleQA.rename(columns={'content': 'context'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41f5d7-ed4f-480a-80e2-5e384aab520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and shuffle datasets\n",
    "df_train = pd.concat([df_train_squad, df_train_mctest, df_train_FairytaleQA], ignore_index=True)\n",
    "df_val = pd.concat([df_val_squad, df_val_mctest, df_val_FairytaleQA], ignore_index=True)\n",
    "df_test = pd.concat([df_test_mctest, df_test_FairytaleQA], ignore_index=True)\n",
    "dataset_train = Dataset.from_pandas(df_train).shuffle(seed=SEED).select(range(min(5000, len(df_train))))\n",
    "dataset_val = Dataset.from_pandas(df_val).shuffle(seed=SEED).select(range(min(1000, len(df_val))))\n",
    "dataset_test = Dataset.from_pandas(df_test).shuffle(seed=SEED).select(range(min(500, len(df_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776df26c-c3aa-4611-af7a-1d213fbdc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "tokenized_dir = os.path.join(OUTPUT_DIR, \"tokenized_datasets\")\n",
    "os.makedirs(tokenized_dir, exist_ok=True)\n",
    "\n",
    "def save_tokenized_datasets(train_dataset, val_dataset, test_dataset):\n",
    "    if not os.path.exists(os.path.join(tokenized_dir, \"train\")):\n",
    "        train_dataset.save_to_disk(os.path.join(tokenized_dir, \"train\"))\n",
    "        logger.info(\"Saved tokenized training dataset\")\n",
    "    if not os.path.exists(os.path.join(tokenized_dir, \"val\")):\n",
    "        val_dataset.save_to_disk(os.path.join(tokenized_dir, \"val\"))\n",
    "        logger.info(\"Saved tokenized validation dataset\")\n",
    "    if not os.path.exists(os.path.join(tokenized_dir, \"test\")):\n",
    "        test_dataset.save_to_disk(os.path.join(tokenized_dir, \"test\"))\n",
    "        logger.info(\"Saved tokenized test dataset\")\n",
    "\n",
    "def load_tokenized_datasets():\n",
    "    global processed_train_dataset, processed_val_dataset, processed_test_dataset\n",
    "    if os.path.exists(os.path.join(tokenized_dir, \"train\")):\n",
    "        train_dataset = Dataset.load_from_disk(os.path.join(tokenized_dir, \"train\"))\n",
    "        temp = train_dataset.filter(lambda x: all(k in x for k in [\"input_ids\", \"attention_mask\", \"labels\"]))\n",
    "        if len(temp) == len(train_dataset):\n",
    "            logger.info(\"Loaded tokenized training dataset\")\n",
    "        else:\n",
    "            logger.warning(\"Invalid train dataset format, re-tokenizing...\")\n",
    "            train_dataset = dataset_train.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "            train_dataset.save_to_disk(os.path.join(tokenized_dir, \"train\"))\n",
    "            logger.info(\"Tokenized and saved training dataset\")\n",
    "    else:\n",
    "        train_dataset = dataset_train.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "        train_dataset.save_to_disk(os.path.join(tokenized_dir, \"train\"))\n",
    "        logger.info(\"Tokenized and saved training dataset\")\n",
    "\n",
    "    if os.path.exists(os.path.join(tokenized_dir, \"val\")):\n",
    "        val_dataset = Dataset.load_from_disk(os.path.join(tokenized_dir, \"val\"))\n",
    "        temp = val_dataset.filter(lambda x: all(k in x for k in [\"input_ids\", \"attention_mask\", \"labels\"]))\n",
    "        if len(temp) == len(val_dataset):\n",
    "            logger.info(\"Loaded tokenized validation dataset\")\n",
    "        else:\n",
    "            logger.warning(\"Invalid val dataset format, re-tokenizing...\")\n",
    "            val_dataset = dataset_val.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "            val_dataset.save_to_disk(os.path.join(tokenized_dir, \"val\"))\n",
    "            logger.info(\"Tokenized and saved validation dataset\")\n",
    "    else:\n",
    "        val_dataset = dataset_val.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "        val_dataset.save_to_disk(os.path.join(tokenized_dir, \"val\"))\n",
    "        logger.info(\"Tokenized and saved validation dataset\")\n",
    "\n",
    "    if os.path.exists(os.path.join(tokenized_dir, \"test\")):\n",
    "        test_dataset = Dataset.load_from_disk(os.path.join(tokenized_dir, \"test\"))\n",
    "        temp = test_dataset.filter(lambda x: all(k in x for k in [\"input_ids\", \"attention_mask\", \"labels\"]))\n",
    "        if len(temp) == len(test_dataset):\n",
    "            logger.info(\"Loaded tokenized test dataset\")\n",
    "        else:\n",
    "            logger.warning(\"Invalid test dataset format, re-tokenizing...\")\n",
    "            test_dataset = dataset_test.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "            test_dataset.save_to_disk(os.path.join(tokenized_dir, \"test\"))\n",
    "            logger.info(\"Tokenized and saved test dataset\")\n",
    "    else:\n",
    "        test_dataset = dataset_test.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "        test_dataset.save_to_disk(os.path.join(tokenized_dir, \"test\"))\n",
    "        logger.info(\"Tokenized and saved test dataset\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8b50e-d3f3-4db4-a6da-aa6ae3c858da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and assign tokenized datasets globally\n",
    "processed_train_dataset, processed_val_dataset, processed_test_dataset = load_tokenized_datasets()\n",
    "\n",
    "# Log dataset sizes for debugging\n",
    "logger.info(f\"Train dataset size: {len(processed_train_dataset)}\")\n",
    "logger.info(f\"Validation dataset size: {len(processed_val_dataset)}\")\n",
    "logger.info(f\"Test dataset size: {len(processed_test_dataset)}\")\n",
    "\n",
    "# Clean datasets\n",
    "for ds_name in [\"processed_train_dataset\", \"processed_val_dataset\", \"processed_test_dataset\"]:\n",
    "    if ds_name in globals():\n",
    "        ds = globals()[ds_name]\n",
    "        cleaned = ds.remove_columns([c for c in ds.column_names if c not in [\"input_ids\", \"attention_mask\", \"labels\"]])\n",
    "        globals()[ds_name] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48709a-b48d-4daf-8991-2e0192d7ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    ref_tokens = [[nltk.word_tokenize(ref)] for ref in decoded_labels]\n",
    "    pred_tokens = [nltk.word_tokenize(pred) for pred in decoded_preds]\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu1 = corpus_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu2 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "    bleu3 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "    bleu4 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l = sum(\n",
    "        scorer.score(ref, pred)['rougeL'].fmeasure\n",
    "        for ref, pred in zip(decoded_labels, decoded_preds)\n",
    "    ) / len(decoded_labels)\n",
    "    meteor = sum(\n",
    "        meteor_score([nltk.word_tokenize(ref)], nltk.word_tokenize(pred))\n",
    "        for ref, pred in zip(decoded_labels, decoded_preds)\n",
    "    ) / len(decoded_labels)\n",
    "    try:\n",
    "        P, R, F1 = score(decoded_preds, decoded_labels, lang=\"en\", verbose=False)\n",
    "        bertscore = F1.mean().item()\n",
    "    except Exception:\n",
    "        bertscore = 0.0\n",
    "    return {\n",
    "        \"bleu-1\": bleu1, \"bleu-2\": bleu2, \"bleu-3\": bleu3, \"bleu-4\": bleu4,\n",
    "        \"rouge-l\": rouge_l, \"meteor\": meteor, \"bertscore\": bertscore\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878ecee-d675-4167-b6d5-402ddf1f2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Early Stopping Callback\n",
    "class CustomEarlyStoppingCallback(EarlyStoppingCallback):\n",
    "    def __init__(self, early_stopping_patience, min_delta=0.01):\n",
    "        super().__init__(early_stopping_patience=early_stopping_patience)\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = float('inf')\n",
    "        self.early_stopping_patience_counter = 0\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        if args.load_best_model_at_end:\n",
    "            assert args.metric_for_best_model is not None, (\n",
    "                \"EarlyStoppingCallback requires metric_for_best_model to be defined when load_best_model_at_end=True\"\n",
    "            )\n",
    "        assert args.eval_strategy != \"no\", (\n",
    "            \"EarlyStoppingCallback requires eval_strategy to be 'steps' or 'epoch'\"\n",
    "        )\n",
    "        logger.info(\"Initialized CustomEarlyStoppingCallback\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        eval_loss = metrics.get('eval_loss', float('inf'))\n",
    "        if self.best_metric == float('inf') or eval_loss < self.best_metric - self.min_delta:\n",
    "            self.best_metric = eval_loss\n",
    "            self.early_stopping_patience_counter = 0\n",
    "        else:\n",
    "            self.early_stopping_patience_counter += 1\n",
    "        if self.early_stopping_patience_counter >= self.early_stopping_patience:\n",
    "            logger.info(f\"Early stopping triggered after {self.early_stopping_patience} evaluations with eval_loss={eval_loss}\")\n",
    "            control.should_training_stop = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56dc908-5bf0-4a0f-b93d-72f869cac52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trial results helper\n",
    "def save_trial_results(study, output_dir):\n",
    "    trials_data = []\n",
    "    for trial in study.trials:\n",
    "        trial_data = {\n",
    "            'trial_number': trial.number,\n",
    "            'eval_loss': trial.value if trial.value is not None else float('inf'),\n",
    "            'state': str(trial.state),\n",
    "            **trial.params\n",
    "        }\n",
    "        trials_data.append(trial_data)\n",
    "    best_trial_data = {\n",
    "        'trial_number': study.best_trial.number,\n",
    "        'eval_loss': study.best_trial.value,\n",
    "        'state': 'BEST',\n",
    "        **study.best_params\n",
    "    }\n",
    "    trials_data.append(best_trial_data)\n",
    "    output_path = os.path.join(output_dir, 'optuna_trials.csv')\n",
    "    mode = 'a' if os.path.exists(output_path) else 'w'\n",
    "    trials_df = pd.DataFrame(trials_data)\n",
    "    trials_df.to_csv(output_path, index=False, mode=mode, header=not os.path.exists(output_path))\n",
    "    logger.info(\"Saved trial results to optuna_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab9e03-1925-4098-8cb8-4fd203b2d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Load model\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "    model.to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    logger.info(f\"Gradient checkpointing enabled: {model.is_gradient_checkpointing}\")\n",
    "    \n",
    "    # Log VRAM usage\n",
    "    if torch.cuda.is_available():\n",
    "        vram_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        vram_total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
    "        logger.info(f\"VRAM usage after model load: {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "    \n",
    "    # Generation config\n",
    "    generation_config = model.generation_config\n",
    "    generation_config.no_repeat_ngram_size = 3\n",
    "    generation_config.min_length = 5\n",
    "    generation_config.max_length = 64\n",
    "    generation_config.num_beams = 3\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1, log=True)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 100, 500, step=50)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\"])\n",
    "    \n",
    "    logger.info(f\"Trial {trial.number} parameters: learning_rate={learning_rate}, \"\n",
    "                f\"weight_decay={weight_decay}, warmup_steps={warmup_steps}, \"\n",
    "                f\"lr_scheduler_type={lr_scheduler_type}\")\n",
    "    \n",
    "    # Define output directory for this trial\n",
    "    trial_output_dir = os.path.join(OUTPUT_DIR, f\"trial_{trial.number}\")\n",
    "    os.makedirs(trial_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=trial_output_dir,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=[],\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"no\",\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=64,\n",
    "        generation_num_beams=3,\n",
    "        load_best_model_at_end=False,\n",
    "        group_by_length=True,\n",
    "        skip_memory_metrics=True,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_train_dataset,\n",
    "        eval_dataset=processed_val_dataset,\n",
    "        compute_metrics=None,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n",
    "        callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train the model with mixed precision\n",
    "    logger.info(f\"Starting new training for trial {trial.number}\")\n",
    "    try:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed for trial {trial.number}: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Evaluate and report\n",
    "    eval_results = trainer.evaluate()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Log VRAM after training\n",
    "    if torch.cuda.is_available():\n",
    "        vram_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        logger.info(f\"VRAM usage after training: {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "    \n",
    "    for log in trainer.state.log_history:\n",
    "        if 'eval_loss' in log:\n",
    "            step = log.get(\"step\", 0)\n",
    "            trial.report(log['eval_loss'], step=step)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return eval_results[\"eval_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6e145-43f3-402b-ade5-7186b9b20494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Optuna study\n",
    "study_name = \"bart_question_generation\"\n",
    "storage_url = f\"sqlite:///{os.path.join(OUTPUT_DIR, 'optuna_study.db')}\"\n",
    "db_path = os.path.join(OUTPUT_DIR, \"optuna_study.db\")\n",
    "if os.path.exists(db_path):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        conn.close()\n",
    "        os.remove(db_path)\n",
    "        logger.info(\"Deleted existing Optuna database to start fresh.\")\n",
    "    except (PermissionError, sqlite3.OperationalError) as e:\n",
    "        logger.warning(f\"Could not delete existing database {db_path}: {e}. Reusing existing database.\")\n",
    "\n",
    "try:\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_url,\n",
    "        direction=\"minimize\",\n",
    "        sampler=TPESampler(seed=42),  # Bayesian optimization\n",
    "        pruner=MedianPruner(n_warmup_steps=2),\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective, n_trials=15)\n",
    "    save_trial_results(study, OUTPUT_DIR)\n",
    "    best_params = study.best_params\n",
    "    with open(os.path.join(OUTPUT_DIR, \"best_params.json\"), \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    logger.info(f\"Best hyperparameters: {best_params}\")\n",
    "    logger.info(f\"Best objective value (eval_loss): {study.best_value}\")\n",
    "\n",
    "    # Clean up non-best trials\n",
    "    best_trial_dir = os.path.join(OUTPUT_DIR, f\"trial_{study.best_trial.number}\")\n",
    "    for trial_dir in os.listdir(OUTPUT_DIR):\n",
    "        if trial_dir.startswith(\"trial_\") and trial_dir != os.path.basename(best_trial_dir):\n",
    "            try:\n",
    "                shutil.rmtree(os.path.join(OUTPUT_DIR, trial_dir))\n",
    "                logger.info(f\"Deleted non-best trial directory: {trial_dir}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to delete trial directory {trial_dir}: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Optuna optimization or file saving failed: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    if 'storage' in locals():\n",
    "        del storage\n",
    "    logger.info(\"Closed Optuna storage connections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0820b8-9607-471a-bf17-c63fe6b74280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Training with Best Parameters\n",
    "logger.info(\"Starting final training with best hyperparameters\")\n",
    "\n",
    "# Load best hyperparameters\n",
    "best_params = study.best_params\n",
    "logger.info(f\"Using best hyperparameters: {best_params}\")\n",
    "\n",
    "# Load model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "model.to(device)\n",
    "model.gradient_checkpointing_enable()\n",
    "logger.info(f\"Gradient checkpointing enabled: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "# Log VRAM usage before training\n",
    "if torch.cuda.is_available():\n",
    "    vram_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    vram_total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
    "    logger.info(f\"VRAM usage before final training: {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "\n",
    "# Define training arguments with best parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(OUTPUT_DIR, \"final_model\"),\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=torch.cuda.is_available(),\n",
    "    lr_scheduler_type=best_params[\"lr_scheduler_type\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    warmup_steps=best_params[\"warmup_steps\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    generation_num_beams=3,\n",
    "    group_by_length=True,\n",
    "    skip_memory_metrics=True,\n",
    "    disable_tqdm=True,\n",
    ")\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    logger.info(\"Starting final training\")\n",
    "    with torch.amp.autocast('cuda'):  # Updated to use correct autocast API\n",
    "        trainer.train()\n",
    "    logger.info(\"Final training completed successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Final training failed: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Log VRAM usage after training\n",
    "if torch.cuda.is_available():\n",
    "    vram_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    logger.info(f\"VRAM usage after final training: {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "\n",
    "# Clean up memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74ed2d-791b-4dcb-bd5c-05b6cdea0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and tokenizer\n",
    "final_model_dir = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "try:\n",
    "    model.save_pretrained(final_model_dir)\n",
    "    tokenizer.save_pretrained(final_model_dir)\n",
    "    logger.info(f\"Saved final model and tokenizer to {final_model_dir}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save model or tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Evaluate on test set\n",
    "logger.info(\"Evaluating on test set\")\n",
    "try:\n",
    "    test_results = trainer.evaluate(processed_test_dataset)\n",
    "    logger.info(f\"Test set evaluation results: {test_results}\")\n",
    "    results_path = os.path.join(OUTPUT_DIR, \"test_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(test_results, f, indent=4)\n",
    "    logger.info(f\"Saved test set results to {results_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Test set evaluation failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display example predictions\n",
    "logger.info(\"Generating example predictions\")\n",
    "num_examples = min(5, len(processed_test_dataset))  # Ensure enough samples\n",
    "sample_indices = random.sample(range(len(processed_test_dataset)), num_examples)\n",
    "for idx in sample_indices:\n",
    "    sample = processed_test_dataset[idx]\n",
    "    input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=64,\n",
    "            num_beams=3,\n",
    "            no_repeat_ngram_size=3,\n",
    "            min_length=5\n",
    "        )\n",
    "    predicted_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Filter out -100 from labels before decoding\n",
    "    valid_labels = [token for token in sample[\"labels\"] if token >= 0]\n",
    "    ground_truth_question = tokenizer.decode(valid_labels, skip_special_tokens=True)\n",
    "    \n",
    "    context = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "    \n",
    "    logger.info(f\"\\nExample {idx}:\")\n",
    "    logger.info(f\"Context: {context}\")\n",
    "    logger.info(f\"Ground Truth Question: {ground_truth_question}\")\n",
    "    logger.info(f\"Predicted Question: {predicted_question}\")\n",
    "\n",
    "# Clean up memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f9a0f-b922-4771-8628-92734efe158a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
