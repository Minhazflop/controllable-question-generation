{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a87b15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import optuna\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, BartForConditionalGeneration,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "import re\n",
    "import shutil\n",
    "import logging\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b704b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "optuna.trial.FixedTrial.seed = SEED\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb27f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "CUDNN version: 90701\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"CUDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e54685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK downloads\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "except Exception as e:\n",
    "    logger.error(f\"NLTK download failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7493e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "PROJECT_ROOT = r\"D:\\A_CSE499\"\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"output_bloom\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8aecf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bloom levels and corresponding special tokens\n",
    "BLOOM_LEVELS = {\n",
    "    'remembering': '<REM>',\n",
    "    'understanding': '<UND>',\n",
    "    'applying': '<APP>',\n",
    "    'analyzing': '<ANA>'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0ce4e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Converted D:\\A_CSE499\\data\\eduqg_903.json to D:\\A_CSE499\\data\\eduqg_903.csv\n",
      "INFO:__main__:Converted D:\\A_CSE499\\data\\merged_600_per_level.json to D:\\A_CSE499\\data\\merged_600_per_level.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert JSON to CSV\n",
    "def convert_json_to_csv(json_file, csv_file, required_fields):\n",
    "    if not os.path.exists(json_file):\n",
    "        logger.error(f\"JSON file not found: {json_file}\")\n",
    "        raise FileNotFoundError(f\"JSON file not found: {json_file}\")\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            logger.error(f\"Data in {json_file} must be a list of records\")\n",
    "            raise ValueError(f\"Data in {json_file} must be a list of records\")\n",
    "        for record in data:\n",
    "            if not all(field in record for field in required_fields):\n",
    "                logger.error(f\"Missing required fields in {json_file}: {record}\")\n",
    "                raise ValueError(f\"Missing required fields in {json_file}\")\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Converted {json_file} to {csv_file}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting {json_file} to CSV: {e}\")\n",
    "        raise\n",
    "\n",
    "# Convert JSON datasets to CSV\n",
    "required_fields = ['context', 'question', 'level']\n",
    "eduqg_json = os.path.join(DATA_DIR, \"eduqg_903.json\")\n",
    "merged_json = os.path.join(DATA_DIR, \"merged_600_per_level.json\")\n",
    "eduqg_csv = os.path.join(DATA_DIR, \"eduqg_903.csv\")\n",
    "merged_csv = os.path.join(DATA_DIR, \"merged_600_per_level.csv\")\n",
    "eduqg_df = convert_json_to_csv(eduqg_json, eduqg_csv, required_fields)\n",
    "merged_df = convert_json_to_csv(merged_json, merged_csv, required_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08bb058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation cleaning function\n",
    "def fix_punctuation_spacing(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.replace(r'\\newline', ' ').replace('\\n', ' ')\n",
    "    text = text.replace(r'\\\"', '\"')\n",
    "    text = re.sub(r'\\s+([,.;:!?])', r'\\1', text)\n",
    "    text = re.sub(r'([,.;:!?])([^\\s\\W])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([.;:!?])(\")', r'\\2\\1', text)\n",
    "    text = re.sub(r'\\(\\s+', '(', text)\n",
    "    text = re.sub(r'\\s+\\)', ')', text)\n",
    "    text = re.sub(r'\"\\s+', '\"', text)\n",
    "    text = re.sub(r'\\s+\"', '\"', text)\n",
    "    text = re.sub(r\"'\\s+\", \"'\", text)\n",
    "    text = re.sub(r\"\\s+'\", \"'\", text)\n",
    "    text = re.sub(r'\\s*[-–—]+\\s*', ' — ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8c2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and add special tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:\\\\A_CSE499\\\\outputLarge_B_phase2\\\\final_model\\\\reload_model\")\n",
    "special_tokens = list(BLOOM_LEVELS.values())\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "691af580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_function(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for c, q, level in zip(batch['context'], batch['question'], batch['level']):\n",
    "        c_clean = fix_punctuation_spacing(str(c)) if c else \"\"\n",
    "        q_clean = fix_punctuation_spacing(str(q)) if q else \"\"\n",
    "        if q_clean.startswith(\"What is the\"):\n",
    "            q_clean = q_clean.replace(\"What is the\", \"What can you tell about\")\n",
    "        bloom_token = BLOOM_LEVELS.get(level.lower(), '<REM>')\n",
    "        inputs.append(f\"{bloom_token} {c_clean}\")\n",
    "        targets.append(f\"{bloom_token} {q_clean}\")\n",
    "    model_inputs = tokenizer(inputs, truncation=True, max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, truncation=True, max_length=64)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "263d3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to validate CSV\n",
    "def validate_csv(file_path, required_columns):\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.error(f\"CSV file not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            logger.error(f\"Column '{col}' missing in {file_path}\")\n",
    "            raise ValueError(f\"Column '{col}' missing in {file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d568b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloom level distribution in full dataset:\n",
      "level\n",
      "applying         729\n",
      "understanding    716\n",
      "analyzing        634\n",
      "remembering      623\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and merge datasets\n",
    "eduqg_df = validate_csv(os.path.join(DATA_DIR, \"eduqg_903.csv\"), ['context', 'question', 'level'])\n",
    "merged_df = validate_csv(os.path.join(DATA_DIR, \"merged_600_per_level.csv\"), ['context', 'question', 'level'])\n",
    "df = pd.concat([eduqg_df, merged_df], ignore_index=True)\n",
    "\n",
    "# Normalize level and filter\n",
    "df['level'] = df['level'].str.lower()\n",
    "df = df[df['level'].isin(BLOOM_LEVELS.keys())]\n",
    "\n",
    "# Print Bloom level distribution\n",
    "bloom_counts = df['level'].value_counts()\n",
    "print(\"Bloom level distribution in full dataset:\")\n",
    "print(bloom_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6f3849d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2161, Validation: 270, Test: 271\n",
      "Bloom level distribution in train set:\n",
      "level\n",
      "applying         583\n",
      "understanding    573\n",
      "analyzing        507\n",
      "remembering      498\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split into train (80%), temp (20%) with stratification\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df['level'])\n",
    "\n",
    "# Split temp into validation (10%) and test (10%)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df['level'])\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
    "print(\"Bloom level distribution in train set:\")\n",
    "print(train_df['level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc4ee9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The demographics of the British Isles today are characterised by a generally high density of population in England, which accounts for almost 80% of the total population of the islands. In elsewhere on Great Britain and on Ireland, high density of population is limited to areas around, or close to, \n",
      "Question: What percentage of the population does England have compared to the other areas in the British Isles?\n",
      "Level: analyzing\n",
      "================================================================================\n",
      "Context: Many years ago my student asked me the question, \"Mrs. Kindred, why do you teach?\" Without taking time to reflect, I answered, \"Because someday I might say something that might make a difference in someone's life.\" Even though I was sincere, that wasn't a very good answer and my student didn't let i\n",
      "Question: Why did the student continue to ask the question about the writer's being a teacher?\n",
      "Level: applying\n",
      "================================================================================\n",
      "Context: \" yes , \" said the king 's daughter , \" indeed i will be careful , and not touch the least thing , just as you have told me . \" but when they entered the forest , it was so thick that it was almost impossible to win through . she was as careful as she could be , and avoided the branches , and thrust\n",
      "Question: how was the six headed troll different from the first one ?\n",
      "Level: analyzing\n",
      "================================================================================\n",
      "Context: Tupolev Tu-204: The Tupolev Tu-204 is a twin-engined medium-range jet airliner capable of carrying 210 passengers, designed by Tupolev and produced by Aviastar SP and Kazan Aircraft Production Association.  First introduced in 1989, it is intended to be broadly equivalent to the Boeing 757, with sli\n",
      "Question: how is Belgorod International Airport and Tupolev Tu-204 related?\n",
      "Level: analyzing\n",
      "================================================================================\n",
      "Context: The USA\n",
      "Well, imagine how my sister felt when she went off for the first time to New York for a business trip. After a few days she was still amazed at how friendly everyone was to her until she took a taxi back to her hotel one night. First she gave just the name of the hotel, but when asked she sa\n",
      "Question: What do the three stories have in common?\n",
      "Level: understanding\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display sample data\n",
    "sample = df.sample(5)\n",
    "for i, row in sample.iterrows():\n",
    "    print(f\"Context: {row['context'][:300]}\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Level: {row['level']}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcec242c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16848d909dcd451eb5b2e04c670b29f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ed783a2f9a4999a77093d703e0fd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126c17b445164b25b208c7421f0c8fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to HuggingFace Datasets\n",
    "dataset_train = Dataset.from_pandas(train_df)\n",
    "dataset_val = Dataset.from_pandas(val_df)\n",
    "dataset_test = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize datasets\n",
    "processed_train_dataset = dataset_train.map(preprocess_function, batched=True, batch_size=50)\n",
    "processed_val_dataset = dataset_val.map(preprocess_function, batched=True, batch_size=50)\n",
    "processed_test_dataset = dataset_test.map(preprocess_function, batched=True, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ced799ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=False)\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "\n",
    "    def remove_bloom_token(text):\n",
    "        for token in BLOOM_LEVELS.values():\n",
    "            text = text.replace(token, '').strip()\n",
    "        return text\n",
    "\n",
    "    decoded_preds_clean = [remove_bloom_token(pred) for pred in decoded_preds]\n",
    "    decoded_labels_clean = [remove_bloom_token(ref) for ref in decoded_labels]\n",
    "\n",
    "    ref_tokens = [[nltk.word_tokenize(ref)] for ref in decoded_labels_clean]\n",
    "    pred_tokens = [nltk.word_tokenize(pred) for pred in decoded_preds_clean]\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu1 = corpus_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu4 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l = sum(\n",
    "        scorer.score(ref, pred)['rougeL'].fmeasure\n",
    "        for ref, pred in zip(decoded_labels_clean, decoded_preds_clean)\n",
    "    ) / len(decoded_labels_clean)\n",
    "\n",
    "    meteor = sum(\n",
    "        meteor_score([nltk.word_tokenize(ref)], nltk.word_tokenize(pred))\n",
    "        for ref, pred in zip(decoded_labels_clean, decoded_preds_clean)\n",
    "    ) / len(decoded_labels_clean)\n",
    "\n",
    "    try:\n",
    "        P, R, F1 = score(decoded_preds_clean, decoded_labels_clean, lang=\"en\", verbose=False)\n",
    "        bertscore = F1.mean().item()\n",
    "    except Exception:\n",
    "        bertscore = 0.0\n",
    "\n",
    "    return {\n",
    "        \"bleu-1\": bleu1, \"bleu-4\": bleu4,\n",
    "        \"rouge-l\": rouge_l, \"meteor\": meteor, \"bertscore\": bertscore\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cd3204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "class CustomEarlyStoppingCallback(EarlyStoppingCallback):\n",
    "    def __init__(self, early_stopping_patience, min_delta=0.005):\n",
    "        super().__init__(early_stopping_patience=early_stopping_patience)\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = float('inf')\n",
    "        self.early_stopping_patience_counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        eval_loss = metrics.get('eval_loss', float('inf'))\n",
    "        if self.best_metric == float('inf') or eval_loss < self.best_metric - self.min_delta:\n",
    "            self.best_metric = eval_loss\n",
    "            self.early_stopping_patience_counter = 0\n",
    "        else:\n",
    "            self.early_stopping_patience_counter += 1\n",
    "        if self.early_stopping_patience_counter >= self.early_stopping_patience:\n",
    "            logger.info(f\"Early stopping triggered after {self.early_stopping_patience} evaluations with eval_loss={eval_loss}\")\n",
    "            control.should_training_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "859afe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trial results helper\n",
    "def save_trial_results(study, output_dir):\n",
    "    trials_data = []\n",
    "    for trial in study.trials:\n",
    "        trial_data = {\n",
    "            'trial_number': trial.number,\n",
    "            'eval_loss': trial.value if trial.value is not None else float('inf'),\n",
    "            'state': str(trial.state),\n",
    "            **trial.params\n",
    "        }\n",
    "        trials_data.append(trial_data)\n",
    "    best_trial_data = {\n",
    "        'trial_number': study.best_trial.number,\n",
    "        'eval_loss': study.best_trial.value,\n",
    "        'state': 'BEST',\n",
    "        **study.best_params\n",
    "    }\n",
    "    trials_data.append(best_trial_data)\n",
    "    output_path = os.path.join(output_dir, 'optuna_trials.csv')\n",
    "    trials_df = pd.DataFrame(trials_data)\n",
    "    mode = 'a' if os.path.exists(output_path) else 'w'\n",
    "    trials_df.to_csv(output_path, index=False, mode=mode, header=not os.path.exists(output_path))\n",
    "    logger.info(\"Saved trial results to optuna_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "890a6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for Optuna tuning\n",
    "def objective(trial):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"D:\\\\A_CSE499\\\\outputLarge_B_phase2\\\\final_model\\\\reload_model\", trust_remote_code=True, use_safetensors=True)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.generation_config.no_repeat_ngram_size = 3\n",
    "    model.generation_config.min_length = 5\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1, log=True)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 100, 500, step=50)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"cosine_with_restarts\"])\n",
    "\n",
    "    trial_output_dir = os.path.join(OUTPUT_DIR, f\"trial_{trial.number}\")\n",
    "    os.makedirs(trial_output_dir, exist_ok=True)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=trial_output_dir,\n",
    "        num_train_epochs=8,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        remove_unused_columns=True,\n",
    "        report_to=[],\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=64,\n",
    "        generation_num_beams=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        group_by_length=True\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_train_dataset,\n",
    "        eval_dataset=processed_val_dataset,\n",
    "        compute_metrics=None,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    checkpoint = None\n",
    "    try:\n",
    "        for checkpoint_dir in sorted(os.listdir(trial_output_dir), reverse=True):\n",
    "            if checkpoint_dir.startswith(\"checkpoint\"):\n",
    "                checkpoint = os.path.join(trial_output_dir, checkpoint_dir)\n",
    "                break\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"No checkpoints found in {trial_output_dir}\")\n",
    "\n",
    "    if checkpoint:\n",
    "        try:\n",
    "            logger.info(f\"Resuming training from {checkpoint}\")\n",
    "            trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to resume from checkpoint {checkpoint}: {e}. Starting new training.\")\n",
    "            trainer.train()\n",
    "    else:\n",
    "        logger.info(f\"Starting new training for trial {trial.number}\")\n",
    "        trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for log in trainer.state.log_history:\n",
    "        if 'eval_loss' in log:\n",
    "            step = log.get(\"epoch\", 0)\n",
    "            trial.report(log['eval_loss'], step=int(step))\n",
    "\n",
    "    return eval_results[\"eval_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "272027b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 20:03:43,798] A new study created in RDB with name: bart_bloom_question_generation\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/544 40:50 < 13:40, 0.17 it/s, Epoch 6/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.088900</td>\n",
       "      <td>2.417607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.316900</td>\n",
       "      <td>2.183061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.143000</td>\n",
       "      <td>2.158609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.012300</td>\n",
       "      <td>2.145115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.869500</td>\n",
       "      <td>2.148754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.703100</td>\n",
       "      <td>2.150242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.1502420902252197\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.1502861976623535\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 6 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-25 20:44:51,753] Trial 0 finished with value: 2.1502861976623535 and parameters: {'learning_rate': 1.827226177606625e-05, 'weight_decay': 0.08927180304353628, 'warmup_steps': 400, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 2.1502861976623535.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/544 37:20 < 12:30, 0.18 it/s, Epoch 6/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.188300</td>\n",
       "      <td>2.543861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.380200</td>\n",
       "      <td>2.198973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.189000</td>\n",
       "      <td>2.165307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.077600</td>\n",
       "      <td>2.145302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.960400</td>\n",
       "      <td>2.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.832400</td>\n",
       "      <td>2.149708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.149707555770874\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.1504056453704834\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 6 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-25 21:22:35,744] Trial 1 finished with value: 2.1504056453704834 and parameters: {'learning_rate': 1.097990803659665e-05, 'weight_decay': 0.07348118405270448, 'warmup_steps': 350, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 2.1502861976623535.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/544 31:25 < 10:31, 0.22 it/s, Epoch 6/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.699800</td>\n",
       "      <td>2.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.175000</td>\n",
       "      <td>2.187758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.867700</td>\n",
       "      <td>2.251009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.398200</td>\n",
       "      <td>1.818248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.985200</td>\n",
       "      <td>1.852001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.798900</td>\n",
       "      <td>1.956798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.9567984342575073\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.8243459463119507\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 6 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-25 21:54:24,480] Trial 2 finished with value: 1.8243459463119507 and parameters: {'learning_rate': 3.818145165896868e-05, 'weight_decay': 0.016305687346221478, 'warmup_steps': 150, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 2 with value: 1.8243459463119507.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='544' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [544/544 42:17, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.026800</td>\n",
       "      <td>2.339916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.290500</td>\n",
       "      <td>2.177272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.114600</td>\n",
       "      <td>2.158310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.969200</td>\n",
       "      <td>2.152182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.805000</td>\n",
       "      <td>2.146192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.588500</td>\n",
       "      <td>2.107433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.323900</td>\n",
       "      <td>2.000950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.141600</td>\n",
       "      <td>1.985947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 8 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-25 22:37:03,172] Trial 3 finished with value: 1.9899410009384155 and parameters: {'learning_rate': 2.0040871876541563e-05, 'weight_decay': 0.019553708662745254, 'warmup_steps': 350, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 2 with value: 1.8243459463119507.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='544' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [544/544 49:31, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.806100</td>\n",
       "      <td>2.231433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.212500</td>\n",
       "      <td>2.161540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.983600</td>\n",
       "      <td>2.186859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.730700</td>\n",
       "      <td>2.131996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.489200</td>\n",
       "      <td>2.071246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.253800</td>\n",
       "      <td>2.071425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.118100</td>\n",
       "      <td>1.968015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.060500</td>\n",
       "      <td>1.957440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 8 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-25 23:26:56,327] Trial 4 finished with value: 1.9620593786239624 and parameters: {'learning_rate': 2.083431561152948e-05, 'weight_decay': 0.06097839109531514, 'warmup_steps': 150, 'lr_scheduler_type': 'cosine'}. Best is trial 2 with value: 1.8243459463119507.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='272' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [272/544 20:57 < 21:07, 0.21 it/s, Epoch 4/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.706200</td>\n",
       "      <td>2.220086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.166700</td>\n",
       "      <td>2.167386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.845900</td>\n",
       "      <td>2.211313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.515400</td>\n",
       "      <td>2.221535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.2215352058410645\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.1644623279571533\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 4 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-25 23:48:15,482] Trial 5 finished with value: 2.1644623279571533 and parameters: {'learning_rate': 2.658616083788978e-05, 'weight_decay': 0.014808945119975192, 'warmup_steps': 100, 'lr_scheduler_type': 'cosine'}. Best is trial 2 with value: 1.8243459463119507.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/544 31:12 < 10:27, 0.22 it/s, Epoch 6/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.119200</td>\n",
       "      <td>2.455489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.333100</td>\n",
       "      <td>2.186971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.156400</td>\n",
       "      <td>2.158960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.032800</td>\n",
       "      <td>2.144551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.898400</td>\n",
       "      <td>2.142253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.747600</td>\n",
       "      <td>2.152521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.1525213718414307\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.148099660873413\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 6 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-26 00:19:47,053] Trial 6 finished with value: 2.148099660873413 and parameters: {'learning_rate': 1.632735695468795e-05, 'weight_decay': 0.012521954287060391, 'warmup_steps': 400, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 2 with value: 1.8243459463119507.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/544 31:59 < 10:42, 0.21 it/s, Epoch 6/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.048600</td>\n",
       "      <td>2.362625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.298900</td>\n",
       "      <td>2.177549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.123000</td>\n",
       "      <td>2.159670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.983100</td>\n",
       "      <td>2.144153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.846400</td>\n",
       "      <td>2.151132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.743900</td>\n",
       "      <td>2.163075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.1630754470825195\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.14947247505188\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 6 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-26 00:52:09,353] Trial 7 finished with value: 2.14947247505188 and parameters: {'learning_rate': 1.0569064414047021e-05, 'weight_decay': 0.08115595675970502, 'warmup_steps': 200, 'lr_scheduler_type': 'linear'}. Best is trial 2 with value: 1.8243459463119507.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/544 32:26 < 10:52, 0.21 it/s, Epoch 6/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.074000</td>\n",
       "      <td>2.397157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.310100</td>\n",
       "      <td>2.182395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.135600</td>\n",
       "      <td>2.158985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.000800</td>\n",
       "      <td>2.146745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.854200</td>\n",
       "      <td>2.154955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.679000</td>\n",
       "      <td>2.151036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.151036024093628\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=2.1514883041381836\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 6 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-26 01:24:58,053] Trial 8 finished with value: 2.1514883041381836 and parameters: {'learning_rate': 2.4106495902171624e-05, 'weight_decay': 0.015305744365500184, 'warmup_steps': 500, 'lr_scheduler_type': 'cosine'}. Best is trial 2 with value: 1.8243459463119507.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='476' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [476/544 37:39 < 05:24, 0.21 it/s, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.695200</td>\n",
       "      <td>2.218149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.170500</td>\n",
       "      <td>2.166160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.856100</td>\n",
       "      <td>2.151522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.556400</td>\n",
       "      <td>2.144971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.231100</td>\n",
       "      <td>1.863189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>1.909464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>1.896120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.8961198329925537\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.87299382686615\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 7 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-26 02:03:00,218] Trial 9 finished with value: 1.87299382686615 and parameters: {'learning_rate': 2.6176655097040075e-05, 'weight_decay': 0.0835361075531176, 'warmup_steps': 100, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 2 with value: 1.8243459463119507.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='476' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [476/544 36:30 < 05:14, 0.22 it/s, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.745200</td>\n",
       "      <td>2.207074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.189400</td>\n",
       "      <td>2.171610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.935500</td>\n",
       "      <td>2.217925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.562400</td>\n",
       "      <td>1.917864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.009600</td>\n",
       "      <td>1.802283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.807200</td>\n",
       "      <td>1.932120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>1.965615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.9656147956848145\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.8131524324417114\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 7 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-26 02:39:52,443] Trial 10 finished with value: 1.8131524324417114 and parameters: {'learning_rate': 4.8260289205439004e-05, 'weight_decay': 0.02973906577570737, 'warmup_steps': 250, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 10 with value: 1.8131524324417114.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='476' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [476/544 37:36 < 05:23, 0.21 it/s, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.743600</td>\n",
       "      <td>2.205958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.190900</td>\n",
       "      <td>2.199370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.932200</td>\n",
       "      <td>2.175875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.581800</td>\n",
       "      <td>1.884568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.036800</td>\n",
       "      <td>1.784130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.816300</td>\n",
       "      <td>1.954260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>1.992813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.992812991142273\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.7933887243270874\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 7 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-26 03:17:52,363] Trial 11 finished with value: 1.7933887243270874 and parameters: {'learning_rate': 4.8625815893389155e-05, 'weight_decay': 0.030098158477190287, 'warmup_steps': 250, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 11 with value: 1.7933887243270874.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\692185169.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new training for trial 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='408' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [408/544 31:42 < 10:37, 0.21 it/s, Epoch 6/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.744900</td>\n",
       "      <td>2.207433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.191800</td>\n",
       "      <td>2.173019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.931600</td>\n",
       "      <td>2.183483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.571900</td>\n",
       "      <td>1.897065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.042700</td>\n",
       "      <td>1.972192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>1.989672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.9896717071533203\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Early stopping triggered after 2 evaluations with eval_loss=1.9050681591033936\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 6 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-26 03:49:56,934] Trial 12 finished with value: 1.9050681591033936 and parameters: {'learning_rate': 4.765821424574219e-05, 'weight_decay': 0.033664587127253655, 'warmup_steps': 250, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 11 with value: 1.7933887243270874.\n",
      "INFO:__main__:Saved trial results to optuna_trials.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 4.8625815893389155e-05, 'weight_decay': 0.030098158477190287, 'warmup_steps': 250, 'lr_scheduler_type': 'cosine_with_restarts'}\n",
      "Best objective value (eval_loss): 1.7933887243270874\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna hyperparameter search\n",
    "study_name = \"bart_bloom_question_generation\"\n",
    "storage = f\"sqlite:///{os.path.join(OUTPUT_DIR, 'optuna_study.db')}\"\n",
    "try:\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage,\n",
    "        direction=\"minimize\",\n",
    "        sampler=TPESampler(seed=42),  # Bayesian optimization\n",
    "        pruner=MedianPruner(n_warmup_steps=2),\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective, n_trials=13)\n",
    "    save_trial_results(study, OUTPUT_DIR)\n",
    "    best_params = study.best_params\n",
    "    with open(os.path.join(OUTPUT_DIR, \"best_params.json\"), \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    print(\"Best objective value (eval_loss):\", study.best_value)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Optuna optimization or file saving failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3e69ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up non-best trials\n",
    "best_trial_dir = os.path.join(OUTPUT_DIR, f\"trial_{study.best_trial.number}\")\n",
    "for trial_dir in os.listdir(OUTPUT_DIR):\n",
    "    if trial_dir.startswith(\"trial_\") and trial_dir != os.path.basename(best_trial_dir):\n",
    "        shutil.rmtree(os.path.join(OUTPUT_DIR, trial_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b00900bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11476\\617955923.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "INFO:__main__:Starting new final training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='544' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [544/680 1:12:19 < 18:08, 0.12 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu-1</th>\n",
       "      <th>Bleu-4</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Bertscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.743400</td>\n",
       "      <td>2.207138</td>\n",
       "      <td>0.613560</td>\n",
       "      <td>0.197788</td>\n",
       "      <td>0.141655</td>\n",
       "      <td>0.580153</td>\n",
       "      <td>0.855882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.175500</td>\n",
       "      <td>2.182361</td>\n",
       "      <td>0.613555</td>\n",
       "      <td>0.184598</td>\n",
       "      <td>0.132780</td>\n",
       "      <td>0.582881</td>\n",
       "      <td>0.856541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.920600</td>\n",
       "      <td>2.149946</td>\n",
       "      <td>0.613435</td>\n",
       "      <td>0.188020</td>\n",
       "      <td>0.135964</td>\n",
       "      <td>0.579550</td>\n",
       "      <td>0.856892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.541400</td>\n",
       "      <td>1.990693</td>\n",
       "      <td>0.454158</td>\n",
       "      <td>0.164250</td>\n",
       "      <td>0.159382</td>\n",
       "      <td>0.447152</td>\n",
       "      <td>0.860341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.037100</td>\n",
       "      <td>1.971221</td>\n",
       "      <td>0.424009</td>\n",
       "      <td>0.162269</td>\n",
       "      <td>0.164989</td>\n",
       "      <td>0.429214</td>\n",
       "      <td>0.864315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>2.172569</td>\n",
       "      <td>0.577612</td>\n",
       "      <td>0.236932</td>\n",
       "      <td>0.181978</td>\n",
       "      <td>0.536924</td>\n",
       "      <td>0.865397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.694100</td>\n",
       "      <td>1.966817</td>\n",
       "      <td>0.601289</td>\n",
       "      <td>0.219109</td>\n",
       "      <td>0.157713</td>\n",
       "      <td>0.598908</td>\n",
       "      <td>0.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.577600</td>\n",
       "      <td>2.132075</td>\n",
       "      <td>0.603524</td>\n",
       "      <td>0.217710</td>\n",
       "      <td>0.152872</td>\n",
       "      <td>0.600486</td>\n",
       "      <td>0.856471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Early stopping triggered after 3 evaluations with eval_loss=2.132075071334839\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "# Final Training with best params\n",
    "best_model_dir = os.path.join(OUTPUT_DIR, \"best_model\")\n",
    "os.makedirs(best_model_dir, exist_ok=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"D:\\\\A_CSE499\\\\outputLarge_B_phase2\\\\final_model\\\\reload_model\", trust_remote_code=True, use_safetensors=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.generation_config.no_repeat_ngram_size = 3\n",
    "model.generation_config.min_length = 5\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=best_model_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    dataloader_num_workers=0,\n",
    "    lr_scheduler_type=best_params[\"lr_scheduler_type\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    warmup_steps=best_params[\"warmup_steps\"],\n",
    "    remove_unused_columns=True,\n",
    "    report_to=[],\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    fp16=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    generation_num_beams=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "checkpoint = None\n",
    "try:\n",
    "    for checkpoint_dir in sorted(os.listdir(best_model_dir), reverse=True):\n",
    "        if checkpoint_dir.startswith(\"checkpoint\"):\n",
    "            checkpoint = os.path.join(best_model_dir, checkpoint_dir)\n",
    "            break\n",
    "except FileNotFoundError:\n",
    "    logger.info(f\"No checkpoints found in {best_model_dir}\")\n",
    "\n",
    "if checkpoint:\n",
    "    try:\n",
    "        logger.info(f\"Resuming final training from {checkpoint}\")\n",
    "        trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to resume from checkpoint {checkpoint}: {e}. Starting new training.\")\n",
    "        trainer.train()\n",
    "else:\n",
    "    logger.info(\"Starting new final training\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "248aee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saved model and tokenizer to D:\\A_CSE499\\output_bloom\\best_model\\reload_model\n"
     ]
    }
   ],
   "source": [
    "# Save final model and tokenizer\n",
    "reload_model_dir = os.path.join(best_model_dir, \"reload_model\")\n",
    "os.makedirs(reload_model_dir, exist_ok=True)\n",
    "try:\n",
    "    model.save_pretrained(reload_model_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(reload_model_dir)\n",
    "    necessary_files = [\n",
    "        \"model.safetensors\",\n",
    "        \"config.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\",\n",
    "        \"special_tokens_map.json\"\n",
    "    ]\n",
    "    for file in os.listdir(reload_model_dir):\n",
    "        if file not in necessary_files:\n",
    "            os.remove(os.path.join(reload_model_dir, file))\n",
    "    logger.info(f\"Saved model and tokenizer to {reload_model_dir}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save model/tokenizer to {reload_model_dir}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42d1aaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Saved test results to D:\\A_CSE499\\output_bloom\\test_results.csv\n",
      "INFO:__main__:Saved test predictions to D:\\A_CSE499\\output_bloom\\test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set and save results\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "test_results = trainer.predict(processed_test_dataset)\n",
    "test_metrics = test_results.metrics\n",
    "test_results_df = pd.DataFrame([test_metrics])\n",
    "test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "try:\n",
    "    test_results_df.to_csv(test_results_path, index=False)\n",
    "    logger.info(f\"Saved test results to {test_results_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save test results: {e}\")\n",
    "    raise\n",
    "\n",
    "test_preds = test_results.predictions[0] if isinstance(test_results.predictions, tuple) else test_results.predictions\n",
    "test_preds = np.clip(test_preds, 0, tokenizer.vocab_size - 1)\n",
    "decoded_preds = tokenizer.batch_decode(test_preds, skip_special_tokens=False)\n",
    "label_ids = np.clip(test_results.label_ids, 0, tokenizer.vocab_size - 1)\n",
    "decoded_refs = tokenizer.batch_decode(label_ids, skip_special_tokens=False)\n",
    "pred_ref_df = pd.DataFrame({\n",
    "    \"context\": processed_test_dataset[\"context\"],\n",
    "    \"level\": processed_test_dataset[\"level\"],\n",
    "    \"predicted_question\": decoded_preds,\n",
    "    \"reference_question\": decoded_refs\n",
    "})\n",
    "pred_ref_path = os.path.join(OUTPUT_DIR, \"test_predictions.csv\")\n",
    "try:\n",
    "    pred_ref_df.to_csv(pred_ref_path, index=False)\n",
    "    logger.info(f\"Saved test predictions to {pred_ref_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save test predictions: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed184fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Saved per-level metrics to D:\\A_CSE499\\output_bloom\\metrics_by_level.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics by Bloom level:\n",
      "                 bleu-1    bleu-4   rouge-l    meteor  bertscore  num_examples\n",
      "analyzing      0.554460  0.267330  0.233436  0.587353   0.884223          64.0\n",
      "applying       0.544616  0.267717  0.244837  0.583565   0.884355          73.0\n",
      "remembering    0.538796  0.277847  0.245975  0.576238   0.888526          62.0\n",
      "understanding  0.541083  0.284006  0.256050  0.585837   0.887610          72.0\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics per Bloom level\n",
    "metrics_by_level = {}\n",
    "for level, group in pred_ref_df.groupby('level'):\n",
    "    preds = [p.replace(BLOOM_LEVELS.get(level.lower(), '<REM>'), '').strip() for p in group['predicted_question']]\n",
    "    refs = [r.replace(BLOOM_LEVELS.get(level.lower(), '<REM>'), '').strip() for r in group['reference_question']]\n",
    "    ref_tokens = [[nltk.word_tokenize(ref)] for ref in refs]\n",
    "    pred_tokens = [nltk.word_tokenize(pred) for pred in preds]\n",
    "    smoothie = SmoothingFunction().method4\n",
    "\n",
    "    bleu1 = corpus_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu4 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l = sum(scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(refs, preds)) / len(refs)\n",
    "    meteor = sum(meteor_score([nltk.word_tokenize(ref)], nltk.word_tokenize(pred)) for ref, pred in zip(refs, preds)) / len(refs)\n",
    "    try:\n",
    "        P, R, F1 = score(preds, refs, lang=\"en\", verbose=False)\n",
    "        bertscore = F1.mean().item()\n",
    "    except Exception:\n",
    "        bertscore = 0.0\n",
    "\n",
    "    metrics_by_level[level] = {\n",
    "        'bleu-1': bleu1,\n",
    "        'bleu-4': bleu4,\n",
    "        'rouge-l': rouge_l,\n",
    "        'meteor': meteor,\n",
    "        'bertscore': bertscore,\n",
    "        'num_examples': len(group)\n",
    "    }\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_by_level).T\n",
    "metrics_path = os.path.join(OUTPUT_DIR, \"metrics_by_level.csv\")\n",
    "metrics_df.to_csv(metrics_path)\n",
    "logger.info(f\"Saved per-level metrics to {metrics_path}\")\n",
    "print(\"Metrics by Bloom level:\")\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0ed6f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Example Predictions and References:\n",
      "INFO:__main__:\n",
      "Example 1:\n",
      "INFO:__main__:Context: The Puritans   get a bad reputation in America--especially when it comes to alcohol.\n",
      "Mayflower, the first ship that came over from England to Massachusetts Bay, actually carried more beer than water.In fact the Founding Fathers of America liked a drink--Samuel Adams was a partner in his father's brewery, and Thomas Jefferson was famous for importing European wines.\n",
      "Early Americans took a healthful small drink for breakfast, whiskey was a typical lunchtime drink, ale   accompanied supper and the day ended with another drink called nightcap.Most Americans in 1790 consumed an average of 5.8 gallons of pure alcohol a year.In 1830, consumption reached 7.1 gallons a year and alcoholism was starting to have a serious influence on communities.Women and children might be in physical danger if the man of the house began drinking.If he became ill or lost his job through drinking, there was no social safety net to support or protect his family.Eventually, alcoholism was being treated as a disease.\n",
      "By the late 19th Century, support for Prohibition, banning the manufacture and sale of alcohol, was powerful.The first arrest for driving under the influence of alcohol was in 1897.On 16 January 1919, Prohibition was set into law.However, by the 1930s when American economy was experiencing a hard time it was widely believed that making alcohol legal again would provide badly-needed jobs and taxes.So in February of 1933, Prohibition was endeD. Still, Prohibition had a great influence on alcohol drinking in this country.In 1955, Americans drank an average of 2.3 gallons of pure alcohol a year.The Prohibition movement was still quite strong after Prohibition ended and it led to a lot of local prohibition on alcohol.\n",
      "The American presidency has done a lot to rehabilitate alcohol and make it respectable again.Presidents Richard Nixon, Jimmy Carter, Bill Clinton, Ronald Reagan and Barack Obama can all be seen on film drinking socially and making official toasts with international celebrities.\n",
      "INFO:__main__:Level: analyzing\n",
      "INFO:__main__:Prediction: </s><s><s><s><mask> What does the fovea contain?</s><pad><pad><pad><pad><pad><pad><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "INFO:__main__:Reference: <s><mask> Axons from which neuron in the retina make up the optic nerve?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "INFO:__main__:\n",
      "Example 2:\n",
      "INFO:__main__:Context: The gyromagnetic ratio γ is the constant of proportionality between the frequency ν of nuclear magnetic resonance (or electron paramagnetic resonance for electrons) and the applied magnetic field B: ν = γB. It is difficult to measure gyromagnetic ratios precisely because of the difficulties in precisely measuring B, but the value for protons in water at 7002298150000000000♠25 °C is known to better than one part per million. The protons are said to be \"shielded\" from the applied magnetic field by the electrons in the water molecule, the same effect that gives rise to chemical shift in NMR spectroscopy, and this is indicated by a prime on the symbol for the gyromagnetic ratio, γ′p. The gyromagnetic ratio is related to the shielded proton magnetic moment μ′p, the spin number I (I = 1⁄2 for protons) and the reduced Planck constant.\n",
      "INFO:__main__:Level: applying\n",
      "INFO:__main__:Prediction: </s><s><s><s><mask> What are proto — oncogenes?</s><pad><pad><pad><pad><pad><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "INFO:__main__:Reference: <s><mask> Which protein is a positive regulator that phosphorylates other proteins when activated?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "INFO:__main__:\n",
      "Example 3:\n",
      "INFO:__main__:Context: Have you ever heard of the saying, \"If you want a friend, be one.\"? What does it mean?\n",
      "There are many different things that you can do to make friends. You may find out what they are if you watch someone make friends.\n",
      "Here is how one new teacher made friends with the students in her class on the first day of the school. When the bell rang, the teacher smiled at all the students. Then she said, \" Good morning! How nice it is to have all of you in my class this year! I want to know each of you very much. I am sure we will enjoy working together.\"\n",
      "The teacher smiled, used a pleasant voice, and acted in a friendly way. She told the students her name and wrote it on the blackboard. Then she told them something she liked to do and hoped to do with them during the year. The students knew that she liked many of the same things they liked. _ Each of them wanted to know her better and be her friends. Then she let the students tell something about themselves. So they felt that the teacher knew them. Could you make friends as the teacher did?\n",
      "How do you know and like your classmates? One way is to find out more about them. During the break you can talk to them. You may ask them their names and the names of the schools they went to last year. They want to know about you, too. You may tell them about your interests or your holiday experiences. It is often easy to be friends with people who have the same interests and play the same games. As you talk, the others may be thinking, \" I like to do the same things you do. It should be fun to be friends with you.\"\n",
      "Remember! Just talking together in a friendly way is one good way to make friends.\n",
      "INFO:__main__:Level: applying\n",
      "INFO:__main__:Prediction: </s><s><s><s><mask> how is Belgorod International Airport and Kolavia Flight 348 related?</s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "INFO:__main__:Reference: <s><mask> how is Belgorod International Airport and Tupolev Tu — 204 related?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "INFO:__main__:\n",
      "Example 4:\n",
      "INFO:__main__:Context: Researchers have developed new software using smart phones'GPS and imaging abilities that determine the exact location of distant objects as well as monitor the speed and direction of moving objects.The software could eventually allow smart phone-armed soldiers to target the location of their enemies.On the home front,the software could be used by everyone,including golfers judging distance to the green and biologists documenting the location of a rare animal without disturbing it.\n",
      "\"The great advantage of a Smartphone is that it provides so many tools in a single,readily available,relatively inexpensive package,\"said Qia Wang,a doctoral student who led the development of the software.\"For example,on the battlefield,a soldier needs a rangefinder,compass,GPS and other tools to get information before calling in an air strike.With our software,the soldier can have all those instruments in one device that can be purchased off the shelf.When that soldier returns from War,she can use the same Software to protect her family by clocking a speeder near her children's school and catching the criminal on video.\"\n",
      "Wang and his colleagues developed their software to locate and track:\n",
      "Targets of known size--when the size of the target is known.a single image is enough to pinpoint the target's location.\n",
      "Targets of unknown size--If the exact size of a target is unknown,the software uses two images to triangulate the location of the target*\n",
      "Moving targets--By taking a short video of a moving target,the smartphone software can calculate how fast the target is moving and in what direction it is going.\n",
      "\"Currently,our software is limited by the physical abilities of smartphone hardware,but the devices are improving rapidly,\"Wang said.\"We expect that improvements in GPS accuracy,battery  life  and  camera  resolution  will  allow  our  software to  make  even  more  accurate observations.\"\n",
      "INFO:__main__:Level: applying\n",
      "INFO:__main__:Prediction: </s><s><s><s><mask> What do the four articles have in common?</s><pad><pad><pad><pad><pad><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "INFO:__main__:Reference: <s><mask> In what way is Baidu summer internships different from the other three?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "INFO:__main__:\n",
      "Example 5:\n",
      "INFO:__main__:Context: Linux Magazine: Linux Magazine is an international magazine for Linux software enthusiasts and professionals.  It is published by the Linux New Media division of the German media company Medialinx AG.\n",
      "Muse (children's magazine): Muse is a children's magazine published by Carus Publishing, the publishers of \"Cricket\".  Launched in January 1997, it is published in Chicago, Illinois, and has readers throughout the United States and around the world.  From 1997 to 2006, it was published in collaboration between \"Cricket\" and \"Smithsonian\".  Recommended for ages nine and above, it features articles about science, history, and the arts.  Nine cartoon characters, known as the Muses, used to appear in the margins throughout the magazine as well as in the Kokopelli & Company comic strip.  \"Muse\" now has a comic named \"Parallel U\" that replaced Muse's muses, featuring new characters, as well as new content and a different layout, as they recently joined with a sister magazine, \"Odyssey\".\n",
      "Communication planning: Communication planning is the art and science of reaching target audiences using marketing communication channels such as advertising, public relations, experiences or direct mail for example.  It is concerned with deciding who to target, when, with what message and how.\n",
      "Conectiva: Conectiva was a company founded on August 28, 1995, in Curitiba, Paraná, Brazil, by a group of friends, among them Arnaldo Carvalho de Melo, who was a pioneer in the distribution of Linux and open source software in Brazilian Portuguese, Spanish and English for all of Latin America .  Besides a customized Linux distribution for the Latin American market, Conectiva developed a series of products and additional services directed to meet the market demand for open source tools, including books, manuals, additional software like Linux Tools and embedded systems, OEM programs, applications port, training kits and the \"Revista do Linux\" Linux magazine.  In addition, the company provided consulting services, training and technical support in all of Latin America through its own service centers and certified partners.\n",
      "INFO:__main__:Level: analyzing\n",
      "INFO:__main__:Prediction: </s><s><s><s><mask> What can you tell about largest bone in the brain case?</s><pad><pad><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "INFO:__main__:Reference: <s><mask> What are the bony openings of the skull called?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# Display example predictions\n",
    "try:\n",
    "    num_examples = min(5, len(decoded_preds), len(decoded_refs), len(processed_test_dataset['context']))\n",
    "    logger.info(\"\\nExample Predictions and References:\")\n",
    "    for i in range(num_examples):\n",
    "        logger.info(f\"\\nExample {i+1}:\")\n",
    "        logger.info(f\"Context: {processed_test_dataset['context'][i]}\")\n",
    "        logger.info(f\"Level: {processed_test_dataset['level'][i]}\")\n",
    "        logger.info(f\"Prediction: {decoded_preds[i]}\")\n",
    "        logger.info(f\"Reference: {decoded_refs[i]}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to print example predictions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d33ad1-3fe5-4f34-aa7c-3329eec88fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
