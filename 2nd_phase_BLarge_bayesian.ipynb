{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28132a9b-ce68-4195-a2ed-5d5a4b8866c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\envs\\torch121\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\user\\anaconda3\\envs\\torch121\\lib\\site-packages (from scikit-learn) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\user\\anaconda3\\envs\\torch121\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\envs\\torch121\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\envs\\torch121\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2481f4-162f-47b6-93a2-9cd08bb6b170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, BartForConditionalGeneration,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    ")\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import sqlite3\n",
    "import optuna\n",
    "import logging\n",
    "import shutil\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff47c02-cdde-4dd3-b871-f25c28463035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70868550-08d1-40e3-8794-0fcac840c73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 03:15:32,381 - INFO - CUDA available: True\n",
      "2025-07-24 03:15:32,382 - INFO - CUDA version: 12.8\n",
      "2025-07-24 03:15:32,383 - INFO - CUDNN version: 90701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model path exists: D:\\A_CSE499\\outputLarge_B\\final_model\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Check CUDA\n",
    "logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "logger.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "logger.info(f\"CUDNN version: {torch.backends.cudnn.version()}\")\n",
    "\n",
    "# NLTK downloads\n",
    "try:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "except Exception as e:\n",
    "    logger.error(f\"NLTK download failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Directories\n",
    "PROJECT_ROOT = r\"D:\\A_CSE499\"\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data_phase2\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"outputLarge_B_phase2\")\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT, \"outputLarge_B\", \"final_model\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Validate model path\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    logger.error(f\"Model path does not exist: {MODEL_PATH}\")\n",
    "    raise FileNotFoundError(f\"Model path does not exist: {MODEL_PATH}. Please ensure the model is saved or update MODEL_PATH.\")\n",
    "\n",
    "    \n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"✅ Model path exists: {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"❌ Model path does NOT exist: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d41e92-31a6-47cc-b941-9f2f4ed67bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and set device\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(PROJECT_ROOT, \"outputLarge_B\", \"final_model\"))\n",
    "device = torch.device(\"cuda:0\")\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please check your PyTorch installation and NVIDIA drivers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f1dbea-b821-4cd0-80ff-fc88b4d2e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation cleaning function\n",
    "def fix_punctuation_spacing(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.replace(r'\\newline', ' ')\n",
    "    text = re.sub(r'\\s+([,.;:!?])', r'\\1', text)\n",
    "    text = re.sub(r'([,.;:!?])([^\\s\\W])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\(\\s+', '(', text)\n",
    "    text = re.sub(r'\\s+\\)', ')', text)\n",
    "    text = re.sub(r'\"\\s+', '\"', text)\n",
    "    text = re.sub(r'\\s+\"', '\"', text)\n",
    "    text = re.sub(r\"'\\s+\", \"'\", text)\n",
    "    text = re.sub(r\"\\s+'\", \"'\", text)\n",
    "    text = re.sub(r'\\s*[-–—]+\\s*', ' — ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e531a7-e89f-4940-9ec1-2a838dc4b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess_function(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for c, q in zip(batch['context'], batch['question']):\n",
    "        c_clean = fix_punctuation_spacing(str(c)) if c else \"\"\n",
    "        q_clean = fix_punctuation_spacing(str(q)) if q else \"\"\n",
    "        if q_clean.startswith(\"What is the\"):\n",
    "            q_clean = q_clean.replace(\"What is the\", \"What can you tell about\")\n",
    "        if c_clean and q_clean:\n",
    "            inputs.append(c_clean)\n",
    "            targets.append(q_clean)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3ac5b3-7dfc-413f-af72-f2cecb06ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate dataset\n",
    "def validate_csv(file_path, required_columns):\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.error(f\"CSV file not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            logger.error(f\"Column '{col}' missing in {file_path}\")\n",
    "            raise ValueError(f\"Column '{col}' missing in {file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6703b82-17b3-4d6f-af92-d7cd0e031832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:31:57,758 - INFO - Train dataset size: 80000\n",
      "2025-07-22 16:31:57,759 - INFO - Validation dataset size: 10000\n",
      "2025-07-22 16:31:57,759 - INFO - Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load combined 100k dataset\n",
    "df = validate_csv(os.path.join(DATA_DIR, \"final_100k_dataset.csv\"), ['context', 'question'])\n",
    "\n",
    "# Shuffle and reset index\n",
    "df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Split into train (80%), validation (10%), test (10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED)\n",
    "\n",
    "# Log dataset sizes\n",
    "logger.info(f\"Train dataset size: {len(train_df)}\")\n",
    "logger.info(f\"Validation dataset size: {len(val_df)}\")\n",
    "logger.info(f\"Test dataset size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a060be6e-4933-4b69-9a26-d01c6e298379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The big problem that you have in looking at polls of people who watched conventions is something called sorting . Republicans watch Republican conventions and Democrats watch Democratic conventions . The problem is probably even worse than that because it is usually the more hardcore in those groups\n",
      "Question: Why do the republicans and democrats watch the conventions ?\n",
      "================================================================================\n",
      "Context: Peter Holmström: Peter Holmström is an American rock musician.  He is in the bands The Dandy Warhols, Pete International Airport, Radis Noir and Rebel Drones.  Holmström's first guitar was a Gibson SG.  Holmström is an amateur photographer.  In the past he has also composed music for his sister's th\n",
      "Question: \"Plan A\" is a song by American rock band The Dandy Warhols, an American alternative rock band, formed in Portland, Oregon in which year, by singer-guitarist Courtney Taylor-Taylor and guitarist Peter Holmström?\n",
      "================================================================================\n",
      "Context: Strangers (Halsey song): \"Strangers\" is a song recorded by American singer and songwriter Halsey featuring Lauren Jauregui from Fifth Harmony.  It was released on May 26, 2017 by Astralwerks as the second promotional single from Halsey's second studio album, \"Hopeless Fountain Kingdom\" (2017).\n",
      "Hopel\n",
      "Question: Bad at Love is from Halsey's second studio album that was released on June 2, 2017, by who?\n",
      "================================================================================\n",
      "Context: I bought the LP . And somehow , in a local music store , I found a condensed conductor 's score of all the album cuts . Not a full , orchestral score ; more like the semi - boiled - down score you 'd see a theatre conductor work from . Between two and four staves , written out piano - style , with l\n",
      "Question: What 's a possible reason the writer bought the LP ?\n",
      "================================================================================\n",
      "Context: Brunswick Commercial Historic District: The Brunswick Commercial Historic District encompasses the historic late-19th century commercial core of Brunswick, Maine.  It includes the northern four blocks of Maine Street, the town's principal commercial thoroughfare, which was laid out in the late 17th \n",
      "Question: Name one journalist that served Maine's largest and principal commercial city?\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "sample = df.sample(5)\n",
    "for i, row in sample.iterrows():\n",
    "    print(f\"Context: {row['context'][:300]}\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    # print(f\"Answer: {row['answer']}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "644aa8c4-bce6-45e3-adbc-50aa548ac66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After splitting your dataframes:\n",
    "train_df = train_df\n",
    "val_df = val_df\n",
    "test_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a661318b-5077-4ca4-9de7-c28d4e2e8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Datasets\n",
    "dataset_train = Dataset.from_pandas(train_df)\n",
    "dataset_val = Dataset.from_pandas(val_df)\n",
    "dataset_test = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c75b7e7c-6d08-4d10-b62c-496d2ff2ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 14:56:52,002 - INFO - Loaded tokenized training dataset\n",
      "2025-07-24 14:56:52,009 - INFO - Loaded tokenized validation dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d1a042557445528d2bff75fb093cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 14:56:53,156 - INFO - Loaded tokenized test dataset\n"
     ]
    }
   ],
   "source": [
    "# Tokenize datasets\n",
    "tokenized_dir = os.path.join(OUTPUT_DIR, \"tokenized_datasets\")\n",
    "os.makedirs(tokenized_dir, exist_ok=True)\n",
    "\n",
    "def save_tokenized_datasets(train_dataset, val_dataset, test_dataset):\n",
    "    if not os.path.exists(os.path.join(tokenized_dir, \"train\")):\n",
    "        train_dataset.save_to_disk(os.path.join(tokenized_dir, \"train\"))\n",
    "        logger.info(\"Saved tokenized training dataset\")\n",
    "    if not os.path.exists(os.path.join(tokenized_dir, \"val\")):\n",
    "        val_dataset.save_to_disk(os.path.join(tokenized_dir, \"val\"))\n",
    "        logger.info(\"Saved tokenized validation dataset\")\n",
    "    if not os.path.exists(os.path.join(tokenized_dir, \"test\")):\n",
    "        test_dataset.save_to_disk(os.path.join(tokenized_dir, \"test\"))\n",
    "        logger.info(\"Saved tokenized test dataset\")\n",
    "\n",
    "def load_tokenized_datasets():\n",
    "    global processed_train_dataset, processed_val_dataset, processed_test_dataset\n",
    "    if os.path.exists(os.path.join(tokenized_dir, \"train\")):\n",
    "        train_dataset = Dataset.load_from_disk(os.path.join(tokenized_dir, \"train\"))\n",
    "        temp = train_dataset.filter(lambda x: all(k in x for k in [\"input_ids\", \"attention_mask\", \"labels\"]))\n",
    "        if len(temp) == len(train_dataset):\n",
    "            logger.info(\"Loaded tokenized training dataset\")\n",
    "        else:\n",
    "            logger.warning(\"Invalid train dataset format, re-tokenizing...\")\n",
    "            train_dataset = dataset_train.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "            train_dataset.save_to_disk(os.path.join(tokenized_dir, \"train\"))\n",
    "            logger.info(\"Tokenized and saved training dataset\")\n",
    "    else:\n",
    "        train_dataset = dataset_train.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "        train_dataset.save_to_disk(os.path.join(tokenized_dir, \"train\"))\n",
    "        logger.info(\"Tokenized and saved training dataset\")\n",
    "\n",
    "    if os.path.exists(os.path.join(tokenized_dir, \"val\")):\n",
    "        val_dataset = Dataset.load_from_disk(os.path.join(tokenized_dir, \"val\"))\n",
    "        temp = val_dataset.filter(lambda x: all(k in x for k in [\"input_ids\", \"attention_mask\", \"labels\"]))\n",
    "        if len(temp) == len(val_dataset):\n",
    "            logger.info(\"Loaded tokenized validation dataset\")\n",
    "        else:\n",
    "            logger.warning(\"Invalid val dataset format, re-tokenizing...\")\n",
    "            val_dataset = dataset_val.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "            val_dataset.save_to_disk(os.path.join(tokenized_dir, \"val\"))\n",
    "            logger.info(\"Tokenized and saved validation dataset\")\n",
    "    else:\n",
    "        val_dataset = dataset_val.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "        val_dataset.save_to_disk(os.path.join(tokenized_dir, \"val\"))\n",
    "        logger.info(\"Tokenized and saved validation dataset\")\n",
    "\n",
    "    if os.path.exists(os.path.join(tokenized_dir, \"test\")):\n",
    "        test_dataset = Dataset.load_from_disk(os.path.join(tokenized_dir, \"test\"))\n",
    "        temp = test_dataset.filter(lambda x: all(k in x for k in [\"input_ids\", \"attention_mask\", \"labels\"]))\n",
    "        if len(temp) == len(test_dataset):\n",
    "            logger.info(\"Loaded tokenized test dataset\")\n",
    "        else:\n",
    "            logger.warning(\"Invalid test dataset format, re-tokenizing...\")\n",
    "            test_dataset = dataset_test.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "            test_dataset.save_to_disk(os.path.join(tokenized_dir, \"test\"))\n",
    "            logger.info(\"Tokenized and saved test dataset\")\n",
    "    else:\n",
    "        test_dataset = dataset_test.map(preprocess_function, batched=True, batch_size=50, remove_columns=['context', 'question'])\n",
    "        test_dataset.save_to_disk(os.path.join(tokenized_dir, \"test\"))\n",
    "        logger.info(\"Tokenized and saved test dataset\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Load and assign tokenized datasets globally\n",
    "processed_train_dataset, processed_val_dataset, processed_test_dataset = load_tokenized_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d73da0ca-2c04-41b6-91b1-d3cc7c0c31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:32:54,490 - INFO - Train dataset size: 80000\n",
      "2025-07-22 16:32:54,490 - INFO - Validation dataset size: 10000\n",
      "2025-07-22 16:32:54,490 - INFO - Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Log dataset sizes\n",
    "logger.info(f\"Train dataset size: {len(processed_train_dataset)}\")\n",
    "logger.info(f\"Validation dataset size: {len(processed_val_dataset)}\")\n",
    "logger.info(f\"Test dataset size: {len(processed_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea3b475-2363-42ff-8c4c-52538c15a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean datasets\n",
    "for ds_name in [\"processed_train_dataset\", \"processed_val_dataset\", \"processed_test_dataset\"]:\n",
    "    if ds_name in globals():\n",
    "        ds = globals()[ds_name]\n",
    "        cleaned = ds.remove_columns([c for c in ds.column_names if c not in [\"input_ids\", \"attention_mask\", \"labels\"]])\n",
    "        globals()[ds_name] = cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38e01c37-89c8-4828-a2a0-70cc09ff57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    ref_tokens = [[nltk.word_tokenize(ref)] for ref in decoded_labels]\n",
    "    pred_tokens = [nltk.word_tokenize(pred) for pred in decoded_preds]\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu1 = corpus_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu2 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "    bleu3 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "    bleu4 = corpus_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l = sum(\n",
    "        scorer.score(ref, pred)['rougeL'].fmeasure\n",
    "        for ref, pred in zip(decoded_labels, decoded_preds)\n",
    "    ) / len(decoded_labels)\n",
    "    meteor = sum(\n",
    "        meteor_score([nltk.word_tokenize(ref)], nltk.word_tokenize(pred))\n",
    "        for ref, pred in zip(decoded_labels, decoded_preds)\n",
    "    ) / len(decoded_labels)\n",
    "    try:\n",
    "        P, R, F1 = score(decoded_preds, decoded_labels, lang=\"en\", verbose=False)\n",
    "        bertscore = F1.mean().item()\n",
    "    except Exception:\n",
    "        bertscore = 0.0\n",
    "    return {\n",
    "        \"bleu-1\": bleu1, \"bleu-2\": bleu2, \"bleu-3\": bleu3, \"bleu-4\": bleu4,\n",
    "        \"rouge-l\": rouge_l, \"meteor\": meteor, \"bertscore\": bertscore\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0d0c14c-e4ff-4257-9dab-9b907b4d9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Early Stopping Callback\n",
    "class CustomEarlyStoppingCallback(EarlyStoppingCallback):\n",
    "    def __init__(self, early_stopping_patience, min_delta=0.01):\n",
    "        super().__init__(early_stopping_patience=early_stopping_patience)\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = float('inf')\n",
    "        self.early_stopping_patience_counter = 0\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        if args.load_best_model_at_end:\n",
    "            assert args.metric_for_best_model is not None, (\n",
    "                \"EarlyStoppingCallback requires metric_for_best_model to be defined when load_best_model_at_end=True\"\n",
    "            )\n",
    "        assert args.eval_strategy != \"no\", (\n",
    "            \"EarlyStoppingCallback requires eval_strategy to be 'steps' or 'epoch'\"\n",
    "        )\n",
    "        logger.info(\"Initialized CustomEarlyStoppingCallback\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        eval_loss = metrics.get('eval_loss', float('inf'))\n",
    "        if self.best_metric == float('inf') or eval_loss < self.best_metric - self.min_delta:\n",
    "            self.best_metric = eval_loss\n",
    "            self.early_stopping_patience_counter = 0\n",
    "        else:\n",
    "            self.early_stopping_patience_counter += 1\n",
    "        if self.early_stopping_patience_counter >= self.early_stopping_patience:\n",
    "            logger.info(f\"Early stopping triggered after {self.early_stopping_patience} evaluations with eval_loss={eval_loss}\")\n",
    "            control.should_training_stop = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e211d85e-7d42-492d-b0d0-2b18d4b45ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trial results helper\n",
    "def save_trial_results(study, output_dir):\n",
    "    trials_data = []\n",
    "    for trial in study.trials:\n",
    "        trial_data = {\n",
    "            'trial_number': trial.number,\n",
    "            'eval_loss': trial.value if trial.value is not None else float('inf'),\n",
    "            'state': str(trial.state),\n",
    "            **trial.params\n",
    "        }\n",
    "        trials_data.append(trial_data)\n",
    "    best_trial_data = {\n",
    "        'trial_number': study.best_trial.number,\n",
    "        'eval_loss': study.best_trial.value,\n",
    "        'state': 'BEST',\n",
    "        **study.best_params\n",
    "    }\n",
    "    trials_data.append(best_trial_data)\n",
    "    output_path = os.path.join(output_dir, 'optuna_trials.csv')\n",
    "    trials_df = pd.DataFrame(trials_data)\n",
    "    mode = 'a' if os.path.exists(output_path) else 'w'\n",
    "    trials_df.to_csv(output_path, index=False, mode=mode, header=not os.path.exists(output_path))\n",
    "    logger.info(\"Saved trial results to optuna_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e70db87-9c4f-496a-bf10-6b3423603a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for Optuna tuning\n",
    "def objective(trial):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Normalize and validate model path\n",
    "    model_path = os.path.abspath(os.path.normpath(MODEL_PATH))\n",
    "    if not os.path.exists(model_path):\n",
    "        logger.error(f\"Model path does not exist: {model_path}\")\n",
    "        raise FileNotFoundError(f\"Model path does not exist: {model_path}\")\n",
    "\n",
    "    # Load model\n",
    "    try:\n",
    "        model = BartForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=True,\n",
    "            use_safetensors=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model from {model_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "    model.to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    logger.info(f\"Gradient checkpointing enabled: {model.is_gradient_checkpointing}\")\n",
    "    generation_config = model.generation_config\n",
    "    generation_config.no_repeat_ngram_size = 3\n",
    "    generation_config.min_length = 5\n",
    "    generation_config.max_length = 64\n",
    "    generation_config.num_beams = 3\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1, log=True)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 100, 500, step=50)\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\"])\n",
    "\n",
    "    logger.info(f\"Trial {trial.number} parameters: learning_rate={learning_rate}, \"\n",
    "                f\"weight_decay={weight_decay}, warmup_steps={warmup_steps}, \"\n",
    "                f\"lr_scheduler_type={lr_scheduler_type}\")\n",
    "\n",
    "    # Log VRAM usage before training\n",
    "    if torch.cuda.is_available():\n",
    "        vram_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        vram_total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
    "        logger.info(f\"VRAM usage before training (trial {trial.number}): {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "\n",
    "    trial_output_dir = os.path.join(OUTPUT_DIR, f\"trial_{trial.number}\")\n",
    "    os.makedirs(trial_output_dir, exist_ok=True)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=trial_output_dir,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=[],\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"no\",\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=64,\n",
    "        generation_num_beams=3,\n",
    "        load_best_model_at_end=False,\n",
    "        group_by_length=True,\n",
    "        skip_memory_metrics=True,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_train_dataset,\n",
    "        eval_dataset=processed_val_dataset,\n",
    "        compute_metrics=None,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n",
    "        callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model with mixed precision\n",
    "    logger.info(f\"Starting new training for trial {trial.number}\")\n",
    "    try:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed for trial {trial.number}: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Evaluate and report\n",
    "    eval_results = trainer.evaluate()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Log VRAM after training\n",
    "    if torch.cuda.is_available():\n",
    "        vram_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "        logger.info(f\"VRAM usage after training: {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "    \n",
    "    for log in trainer.state.log_history:\n",
    "        if 'eval_loss' in log:\n",
    "            step = log.get(\"step\", 0)\n",
    "            trial.report(log['eval_loss'], step=step)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return eval_results[\"eval_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d61f6f6-8310-4dd5-ba42-2fa32d7ad1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 16:36:30,004] A new study created in RDB with name: bart_question_generation\n",
      "2025-07-22 16:36:36,274 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-22 16:36:36,304 - INFO - Trial 0 parameters: learning_rate=1.827226177606625e-05, weight_decay=0.08927180304353628, warmup_steps=400, lr_scheduler_type=linear\n",
      "2025-07-22 16:36:36,306 - INFO - VRAM usage before training (trial 0): 1.51GB / 7.96GB\n",
      "2025-07-22 16:36:36,431 - INFO - Starting new training for trial 0\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3680\\2499197808.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "2025-07-22 16:36:46,750 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7963, 'grad_norm': 8.041173934936523, 'learning_rate': 2.0556294498074535e-06, 'epoch': 0.02}\n",
      "{'loss': 2.6685, 'grad_norm': 6.349306106567383, 'learning_rate': 4.3396621718157345e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5531, 'grad_norm': 5.658001899719238, 'learning_rate': 6.623694893824016e-06, 'epoch': 0.06}\n",
      "{'loss': 2.5554, 'grad_norm': 5.723142623901367, 'learning_rate': 8.862046961392131e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 2.371518850326538, 'eval_runtime': 109.2292, 'eval_samples_per_second': 91.551, 'eval_steps_per_second': 45.775, 'epoch': 0.08}\n",
      "{'loss': 2.5198, 'grad_norm': 10.649038314819336, 'learning_rate': 1.1146079683400413e-05, 'epoch': 0.1}\n",
      "{'loss': 2.46, 'grad_norm': 9.487222671508789, 'learning_rate': 1.3430112405408695e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4502, 'grad_norm': 11.642400741577148, 'learning_rate': 1.5714145127416976e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4304, 'grad_norm': 6.292455196380615, 'learning_rate': 1.7998177849425257e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.2800655364990234, 'eval_runtime': 109.7009, 'eval_samples_per_second': 91.157, 'eval_steps_per_second': 45.578, 'epoch': 0.16}\n",
      "{'loss': 2.4331, 'grad_norm': 6.247132778167725, 'learning_rate': 1.8205817187789645e-05, 'epoch': 0.18}\n",
      "{'loss': 2.3966, 'grad_norm': 5.26241397857666, 'learning_rate': 1.8130311973838958e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3558, 'grad_norm': 7.323211193084717, 'learning_rate': 1.805480675988827e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3346, 'grad_norm': 4.717714309692383, 'learning_rate': 1.7979301545937585e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.302747964859009, 'eval_runtime': 109.5827, 'eval_samples_per_second': 91.255, 'eval_steps_per_second': 45.628, 'epoch': 0.24}\n",
      "{'loss': 2.4231, 'grad_norm': 4.952661037445068, 'learning_rate': 1.79037963319869e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2644, 'grad_norm': 6.509766101837158, 'learning_rate': 1.7828291118036212e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2942, 'grad_norm': 5.408895969390869, 'learning_rate': 1.7752785904085525e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3748, 'grad_norm': 7.307462692260742, 'learning_rate': 1.767728069013484e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.200348138809204, 'eval_runtime': 109.2293, 'eval_samples_per_second': 91.551, 'eval_steps_per_second': 45.775, 'epoch': 0.32}\n",
      "{'loss': 2.2767, 'grad_norm': 5.498708724975586, 'learning_rate': 1.7601775476184152e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3535, 'grad_norm': 5.846469402313232, 'learning_rate': 1.7526270262233465e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3126, 'grad_norm': 5.65612268447876, 'learning_rate': 1.7450765048282775e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2556, 'grad_norm': 5.32232666015625, 'learning_rate': 1.737525983433209e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.176410436630249, 'eval_runtime': 109.4598, 'eval_samples_per_second': 91.358, 'eval_steps_per_second': 45.679, 'epoch': 0.4}\n",
      "{'loss': 2.2446, 'grad_norm': 9.23885440826416, 'learning_rate': 1.7299754620381402e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2177, 'grad_norm': 6.369492053985596, 'learning_rate': 1.7224249406430715e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2777, 'grad_norm': 4.9866437911987305, 'learning_rate': 1.714874419248003e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2538, 'grad_norm': 5.600339412689209, 'learning_rate': 1.7073238978529342e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.1588335037231445, 'eval_runtime': 109.2494, 'eval_samples_per_second': 91.534, 'eval_steps_per_second': 45.767, 'epoch': 0.48}\n",
      "{'loss': 2.2534, 'grad_norm': 6.495411396026611, 'learning_rate': 1.6997733764578656e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2719, 'grad_norm': 4.484342098236084, 'learning_rate': 1.692222855062797e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2325, 'grad_norm': 17.470256805419922, 'learning_rate': 1.6846723336677282e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2851, 'grad_norm': 5.22712516784668, 'learning_rate': 1.6771218122726596e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1449756622314453, 'eval_runtime': 109.9868, 'eval_samples_per_second': 90.92, 'eval_steps_per_second': 45.46, 'epoch': 0.56}\n",
      "{'loss': 2.2665, 'grad_norm': 7.199024200439453, 'learning_rate': 1.669571290877591e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2395, 'grad_norm': 5.765564918518066, 'learning_rate': 1.662020769482522e-05, 'epoch': 0.6}\n",
      "{'loss': 2.2013, 'grad_norm': 5.865303039550781, 'learning_rate': 1.6544702480874533e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2839, 'grad_norm': 4.544154167175293, 'learning_rate': 1.6469197266923846e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.1351301670074463, 'eval_runtime': 109.5306, 'eval_samples_per_second': 91.299, 'eval_steps_per_second': 45.649, 'epoch': 0.64}\n",
      "{'loss': 2.1959, 'grad_norm': 6.240779399871826, 'learning_rate': 1.639369205297316e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2356, 'grad_norm': 5.200198173522949, 'learning_rate': 1.6318186839022473e-05, 'epoch': 0.68}\n",
      "{'loss': 2.195, 'grad_norm': 4.261333465576172, 'learning_rate': 1.6242681625071786e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2167, 'grad_norm': 8.502999305725098, 'learning_rate': 1.61671764111211e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.1235532760620117, 'eval_runtime': 109.7018, 'eval_samples_per_second': 91.156, 'eval_steps_per_second': 45.578, 'epoch': 0.72}\n",
      "{'loss': 2.2047, 'grad_norm': 6.116044998168945, 'learning_rate': 1.6091671197170413e-05, 'epoch': 0.74}\n",
      "{'loss': 2.202, 'grad_norm': 6.0651373863220215, 'learning_rate': 1.6016165983219726e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2362, 'grad_norm': 4.287887096405029, 'learning_rate': 1.5940660769269036e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2353, 'grad_norm': 4.62035608291626, 'learning_rate': 1.586515555531835e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.152543067932129, 'eval_runtime': 109.5583, 'eval_samples_per_second': 91.276, 'eval_steps_per_second': 45.638, 'epoch': 0.8}\n",
      "{'loss': 2.1983, 'grad_norm': 5.662720680236816, 'learning_rate': 1.5789650341367663e-05, 'epoch': 0.82}\n",
      "{'loss': 2.217, 'grad_norm': 5.0974578857421875, 'learning_rate': 1.5714145127416976e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1193, 'grad_norm': 8.061209678649902, 'learning_rate': 1.563863991346629e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2051, 'grad_norm': 4.517706394195557, 'learning_rate': 1.5563134699515603e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.102539300918579, 'eval_runtime': 109.9305, 'eval_samples_per_second': 90.967, 'eval_steps_per_second': 45.483, 'epoch': 0.88}\n",
      "{'loss': 2.2036, 'grad_norm': 4.853235244750977, 'learning_rate': 1.5487629485564917e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1559, 'grad_norm': 5.864267826080322, 'learning_rate': 1.541212427161423e-05, 'epoch': 0.92}\n",
      "{'loss': 2.209, 'grad_norm': 8.9264497756958, 'learning_rate': 1.5336619057663543e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1505, 'grad_norm': 4.594913959503174, 'learning_rate': 1.5261113843712853e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.1309866905212402, 'eval_runtime': 109.2891, 'eval_samples_per_second': 91.5, 'eval_steps_per_second': 45.75, 'epoch': 0.96}\n",
      "{'loss': 2.1902, 'grad_norm': 4.983923435211182, 'learning_rate': 1.5185608629762167e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2145, 'grad_norm': 5.434694766998291, 'learning_rate': 1.511010341581148e-05, 'epoch': 1.0}\n",
      "{'loss': 2.008, 'grad_norm': 5.941095352172852, 'learning_rate': 1.5034598201860794e-05, 'epoch': 1.02}\n",
      "{'loss': 2.1028, 'grad_norm': 7.009422302246094, 'learning_rate': 1.4959092987910107e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.0966155529022217, 'eval_runtime': 109.5052, 'eval_samples_per_second': 91.32, 'eval_steps_per_second': 45.66, 'epoch': 1.04}\n",
      "{'loss': 2.0657, 'grad_norm': 4.866135597229004, 'learning_rate': 1.488358777395942e-05, 'epoch': 1.06}\n",
      "{'loss': 2.0592, 'grad_norm': 10.550084114074707, 'learning_rate': 1.4808082560008734e-05, 'epoch': 1.08}\n",
      "{'loss': 2.0702, 'grad_norm': 6.733802318572998, 'learning_rate': 1.4732577346058045e-05, 'epoch': 1.1}\n",
      "{'loss': 2.0515, 'grad_norm': 20.047866821289062, 'learning_rate': 1.4657072132107357e-05, 'epoch': 1.12}\n",
      "{'eval_loss': 2.091601610183716, 'eval_runtime': 109.7558, 'eval_samples_per_second': 91.111, 'eval_steps_per_second': 45.556, 'epoch': 1.12}\n",
      "{'loss': 2.0669, 'grad_norm': 42.28401565551758, 'learning_rate': 1.458156691815667e-05, 'epoch': 1.1400000000000001}\n",
      "{'loss': 2.0243, 'grad_norm': 13.692486763000488, 'learning_rate': 1.4506061704205984e-05, 'epoch': 1.16}\n",
      "{'loss': 2.0564, 'grad_norm': 17.43425941467285, 'learning_rate': 1.443206659453431e-05, 'epoch': 1.18}\n",
      "{'loss': 2.0675, 'grad_norm': 10.99371337890625, 'learning_rate': 1.4356561380583623e-05, 'epoch': 1.2}\n",
      "{'eval_loss': 2.111788511276245, 'eval_runtime': 109.6459, 'eval_samples_per_second': 91.203, 'eval_steps_per_second': 45.601, 'epoch': 1.2}\n",
      "{'loss': 2.0388, 'grad_norm': 4.651650905609131, 'learning_rate': 1.4281056166632937e-05, 'epoch': 1.22}\n",
      "{'loss': 2.0466, 'grad_norm': 5.065827369689941, 'learning_rate': 1.420555095268225e-05, 'epoch': 1.24}\n",
      "{'loss': 2.0704, 'grad_norm': 4.851912498474121, 'learning_rate': 1.4130045738731564e-05, 'epoch': 1.26}\n",
      "{'loss': 2.0555, 'grad_norm': 7.996484279632568, 'learning_rate': 1.4054540524780877e-05, 'epoch': 1.28}\n",
      "{'eval_loss': 2.0828964710235596, 'eval_runtime': 109.8471, 'eval_samples_per_second': 91.036, 'eval_steps_per_second': 45.518, 'epoch': 1.28}\n",
      "{'loss': 2.0183, 'grad_norm': 5.798264503479004, 'learning_rate': 1.3979035310830189e-05, 'epoch': 1.3}\n",
      "{'loss': 2.0385, 'grad_norm': 9.007881164550781, 'learning_rate': 1.3903530096879502e-05, 'epoch': 1.32}\n",
      "{'loss': 2.0544, 'grad_norm': 4.334831237792969, 'learning_rate': 1.3828024882928814e-05, 'epoch': 1.34}\n",
      "{'loss': 2.0474, 'grad_norm': 5.174055099487305, 'learning_rate': 1.3752519668978127e-05, 'epoch': 1.3599999999999999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 18:54:21,414 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.091510534286499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.091510534286499, 'eval_runtime': 109.8784, 'eval_samples_per_second': 91.01, 'eval_steps_per_second': 45.505, 'epoch': 1.3599999999999999}\n",
      "{'train_runtime': 8254.6836, 'train_samples_per_second': 48.457, 'train_steps_per_second': 1.514, 'train_loss': 2.2362316086713006, 'epoch': 1.3599999999999999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 18:56:35,343 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.091402530670166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.091402530670166, 'eval_runtime': 132.6596, 'eval_samples_per_second': 75.381, 'eval_steps_per_second': 37.69, 'epoch': 1.3599999999999999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 18:56:35,673 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 3400 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-22 18:56:36,103] Trial 0 finished with value: 2.091402530670166 and parameters: {'learning_rate': 1.827226177606625e-05, 'weight_decay': 0.08927180304353628, 'warmup_steps': 400, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 2.091402530670166.\n",
      "2025-07-22 18:56:37,293 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-22 18:56:37,323 - INFO - Trial 1 parameters: learning_rate=1.2853916978930139e-05, weight_decay=0.011430983876313222, warmup_steps=450, lr_scheduler_type=cosine\n",
      "2025-07-22 18:56:37,323 - INFO - VRAM usage before training (trial 1): 1.53GB / 7.96GB\n",
      "2025-07-22 18:56:37,373 - INFO - Starting new training for trial 1\n",
      "2025-07-22 18:56:47,883 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8115, 'grad_norm': 7.2612762451171875, 'learning_rate': 1.285391697893014e-06, 'epoch': 0.02}\n",
      "{'loss': 2.7058, 'grad_norm': 6.481954574584961, 'learning_rate': 2.713604695551918e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5877, 'grad_norm': 5.4879150390625, 'learning_rate': 4.141817693210823e-06, 'epoch': 0.06}\n",
      "{'loss': 2.5881, 'grad_norm': 6.2297821044921875, 'learning_rate': 5.541466430916549e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 2.3953311443328857, 'eval_runtime': 108.8703, 'eval_samples_per_second': 91.852, 'eval_steps_per_second': 45.926, 'epoch': 0.08}\n",
      "{'loss': 2.5421, 'grad_norm': 8.587467193603516, 'learning_rate': 6.969679428575454e-06, 'epoch': 0.1}\n",
      "{'loss': 2.4847, 'grad_norm': 9.54176139831543, 'learning_rate': 8.397892426234358e-06, 'epoch': 0.12}\n",
      "{'loss': 2.4725, 'grad_norm': 12.026006698608398, 'learning_rate': 9.826105423893262e-06, 'epoch': 0.14}\n",
      "{'loss': 2.4494, 'grad_norm': 5.933807849884033, 'learning_rate': 1.1254318421552166e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.2946600914001465, 'eval_runtime': 108.7678, 'eval_samples_per_second': 91.939, 'eval_steps_per_second': 45.969, 'epoch': 0.16}\n",
      "{'loss': 2.4452, 'grad_norm': 6.067215919494629, 'learning_rate': 1.268253141921107e-05, 'epoch': 0.18}\n",
      "{'loss': 2.4183, 'grad_norm': 5.85799503326416, 'learning_rate': 1.2853494113929514e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3746, 'grad_norm': 10.304692268371582, 'learning_rate': 1.2851987077528022e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3551, 'grad_norm': 5.161556243896484, 'learning_rate': 1.2849388262533352e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.240123748779297, 'eval_runtime': 108.7362, 'eval_samples_per_second': 91.966, 'eval_steps_per_second': 45.983, 'epoch': 0.24}\n",
      "{'loss': 2.4399, 'grad_norm': 5.190505504608154, 'learning_rate': 1.2845698110551412e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2839, 'grad_norm': 4.872314929962158, 'learning_rate': 1.28409172486345e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3109, 'grad_norm': 6.173828601837158, 'learning_rate': 1.2835046489174752e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3929, 'grad_norm': 7.4169535636901855, 'learning_rate': 1.2828086829766093e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.2084171772003174, 'eval_runtime': 108.6083, 'eval_samples_per_second': 92.074, 'eval_steps_per_second': 46.037, 'epoch': 0.32}\n",
      "{'loss': 2.2937, 'grad_norm': 7.819201946258545, 'learning_rate': 1.2820039453034729e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3728, 'grad_norm': 8.802614212036133, 'learning_rate': 1.2810905726438175e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3282, 'grad_norm': 6.183273792266846, 'learning_rate': 1.28006872020329e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2719, 'grad_norm': 7.5512566566467285, 'learning_rate': 1.2789385616210586e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.184946298599243, 'eval_runtime': 108.7918, 'eval_samples_per_second': 91.919, 'eval_steps_per_second': 45.959, 'epoch': 0.4}\n",
      "{'loss': 2.2549, 'grad_norm': 8.765888214111328, 'learning_rate': 1.2777002889403075e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2369, 'grad_norm': 14.483659744262695, 'learning_rate': 1.2763541125756027e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2947, 'grad_norm': 5.223300933837891, 'learning_rate': 1.2749002612771388e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2727, 'grad_norm': 6.3255615234375, 'learning_rate': 1.2733389820918673e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.179997205734253, 'eval_runtime': 108.7368, 'eval_samples_per_second': 91.965, 'eval_steps_per_second': 45.983, 'epoch': 0.48}\n",
      "{'loss': 2.2696, 'grad_norm': 6.925248146057129, 'learning_rate': 1.271670540321517e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2888, 'grad_norm': 6.655501365661621, 'learning_rate': 1.2698952194775123e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2486, 'grad_norm': 8.722651481628418, 'learning_rate': 1.2680133212327983e-05, 'epoch': 0.54}\n",
      "{'loss': 2.3004, 'grad_norm': 5.3252763748168945, 'learning_rate': 1.266025165370577e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1623334884643555, 'eval_runtime': 110.951, 'eval_samples_per_second': 90.13, 'eval_steps_per_second': 45.065, 'epoch': 0.56}\n",
      "{'loss': 2.2801, 'grad_norm': 7.178212642669678, 'learning_rate': 1.26393108972997e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2517, 'grad_norm': 13.903724670410156, 'learning_rate': 1.2617314501486085e-05, 'epoch': 0.6}\n",
      "{'loss': 2.2067, 'grad_norm': 6.717864990234375, 'learning_rate': 1.2594266204021693e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2968, 'grad_norm': 9.470752716064453, 'learning_rate': 1.2570169921408597e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.142608642578125, 'eval_runtime': 108.3666, 'eval_samples_per_second': 92.279, 'eval_steps_per_second': 46.14, 'epoch': 0.64}\n",
      "{'loss': 2.2032, 'grad_norm': 6.534360885620117, 'learning_rate': 1.254502974822866e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2537, 'grad_norm': 6.007073402404785, 'learning_rate': 1.2518849956447763e-05, 'epoch': 0.68}\n",
      "{'loss': 2.2093, 'grad_norm': 8.16948413848877, 'learning_rate': 1.2491634994689884e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2306, 'grad_norm': 7.9189324378967285, 'learning_rate': 1.2463389487481164e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.133033037185669, 'eval_runtime': 108.3964, 'eval_samples_per_second': 92.254, 'eval_steps_per_second': 46.127, 'epoch': 0.72}\n",
      "{'loss': 2.2209, 'grad_norm': 5.382977485656738, 'learning_rate': 1.243411823446408e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2201, 'grad_norm': 6.424050331115723, 'learning_rate': 1.2403826209581863e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2525, 'grad_norm': 4.436100482940674, 'learning_rate': 1.2372518560233285e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2529, 'grad_norm': 4.806380748748779, 'learning_rate': 1.2340200606398005e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1505544185638428, 'eval_runtime': 108.4276, 'eval_samples_per_second': 92.227, 'eval_steps_per_second': 46.114, 'epoch': 0.8}\n",
      "{'loss': 2.2157, 'grad_norm': 7.0358052253723145, 'learning_rate': 1.230687783973255e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2292, 'grad_norm': 5.491242408752441, 'learning_rate': 1.2272555922637138e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1363, 'grad_norm': 6.535735130310059, 'learning_rate': 1.2237240687293504e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2217, 'grad_norm': 5.103600978851318, 'learning_rate': 1.2200938134673854e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1217308044433594, 'eval_runtime': 111.7118, 'eval_samples_per_second': 89.516, 'eval_steps_per_second': 44.758, 'epoch': 0.88}\n",
      "{'loss': 2.2199, 'grad_norm': 4.828455448150635, 'learning_rate': 1.2163654433521136e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1735, 'grad_norm': 7.442209720611572, 'learning_rate': 1.2125395919300828e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2249, 'grad_norm': 4.763263702392578, 'learning_rate': 1.2086169093124366e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1686, 'grad_norm': 4.597787380218506, 'learning_rate': 1.204598062064444e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.117999315261841, 'eval_runtime': 108.3334, 'eval_samples_per_second': 92.308, 'eval_steps_per_second': 46.154, 'epoch': 0.96}\n",
      "{'loss': 2.207, 'grad_norm': 6.071952819824219, 'learning_rate': 1.2004837330922329e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2306, 'grad_norm': 6.150991439819336, 'learning_rate': 1.1962746215267466e-05, 'epoch': 1.0}\n",
      "{'loss': 2.048, 'grad_norm': 5.195823669433594, 'learning_rate': 1.1919714426049433e-05, 'epoch': 1.02}\n",
      "{'loss': 2.1439, 'grad_norm': 5.677699089050293, 'learning_rate': 1.1875749275482587e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.1041760444641113, 'eval_runtime': 108.2803, 'eval_samples_per_second': 92.353, 'eval_steps_per_second': 46.176, 'epoch': 1.04}\n",
      "{'loss': 2.106, 'grad_norm': 5.090940952301025, 'learning_rate': 1.183085823438353e-05, 'epoch': 1.06}\n",
      "{'loss': 2.0933, 'grad_norm': 10.794146537780762, 'learning_rate': 1.1785048930901618e-05, 'epoch': 1.08}\n",
      "{'loss': 2.1069, 'grad_norm': 11.520638465881348, 'learning_rate': 1.1738329149222748e-05, 'epoch': 1.1}\n",
      "{'loss': 2.0941, 'grad_norm': 7.033361434936523, 'learning_rate': 1.1690706828246617e-05, 'epoch': 1.12}\n",
      "{'eval_loss': 2.102748394012451, 'eval_runtime': 108.2811, 'eval_samples_per_second': 92.352, 'eval_steps_per_second': 46.176, 'epoch': 1.12}\n",
      "{'loss': 2.1004, 'grad_norm': 7.561352252960205, 'learning_rate': 1.16421900602377e-05, 'epoch': 1.1400000000000001}\n",
      "{'loss': 2.0636, 'grad_norm': 8.226988792419434, 'learning_rate': 1.1592787089450165e-05, 'epoch': 1.16}\n",
      "{'loss': 2.0938, 'grad_norm': 6.908562183380127, 'learning_rate': 1.1543520473801173e-05, 'epoch': 1.18}\n",
      "{'loss': 2.1028, 'grad_norm': 7.089141845703125, 'learning_rate': 1.1492387731737084e-05, 'epoch': 1.2}\n",
      "{'eval_loss': 2.0958518981933594, 'eval_runtime': 108.3585, 'eval_samples_per_second': 92.286, 'eval_steps_per_second': 46.143, 'epoch': 1.2}\n",
      "{'loss': 2.0707, 'grad_norm': 4.510036468505859, 'learning_rate': 1.1440394242185854e-05, 'epoch': 1.22}\n",
      "{'loss': 2.0776, 'grad_norm': 3.9544365406036377, 'learning_rate': 1.1387548840186599e-05, 'epoch': 1.24}\n",
      "{'loss': 2.1032, 'grad_norm': 6.077111721038818, 'learning_rate': 1.1333860505540399e-05, 'epoch': 1.26}\n",
      "{'loss': 2.0878, 'grad_norm': 8.547806739807129, 'learning_rate': 1.1279338361284397e-05, 'epoch': 1.28}\n",
      "{'eval_loss': 2.0879745483398438, 'eval_runtime': 108.2265, 'eval_samples_per_second': 92.399, 'eval_steps_per_second': 46.199, 'epoch': 1.28}\n",
      "{'loss': 2.0541, 'grad_norm': 6.914714813232422, 'learning_rate': 1.1223991672141567e-05, 'epoch': 1.3}\n",
      "{'loss': 2.073, 'grad_norm': 10.756634712219238, 'learning_rate': 1.116782984294639e-05, 'epoch': 1.32}\n",
      "{'loss': 2.0873, 'grad_norm': 4.360254764556885, 'learning_rate': 1.1110862417046731e-05, 'epoch': 1.34}\n",
      "{'loss': 2.0829, 'grad_norm': 5.137163162231445, 'learning_rate': 1.105309907468218e-05, 'epoch': 1.3599999999999999}\n",
      "{'eval_loss': 2.098146915435791, 'eval_runtime': 108.8636, 'eval_samples_per_second': 91.858, 'eval_steps_per_second': 45.929, 'epoch': 1.3599999999999999}\n",
      "{'loss': 2.1248, 'grad_norm': 7.954415321350098, 'learning_rate': 1.0994549631339131e-05, 'epoch': 1.38}\n",
      "{'loss': 2.1171, 'grad_norm': 5.1021318435668945, 'learning_rate': 1.0935224036082867e-05, 'epoch': 1.4}\n",
      "{'loss': 2.062, 'grad_norm': 8.12111759185791, 'learning_rate': 1.0875132369866963e-05, 'epoch': 1.42}\n",
      "{'loss': 2.0808, 'grad_norm': 9.46099853515625, 'learning_rate': 1.0814284843820279e-05, 'epoch': 1.44}\n",
      "{'eval_loss': 2.1077663898468018, 'eval_runtime': 109.4882, 'eval_samples_per_second': 91.334, 'eval_steps_per_second': 45.667, 'epoch': 1.44}\n",
      "{'loss': 2.1001, 'grad_norm': 4.460006237030029, 'learning_rate': 1.0752691797511801e-05, 'epoch': 1.46}\n",
      "{'loss': 2.1074, 'grad_norm': 10.664267539978027, 'learning_rate': 1.0690363697193717e-05, 'epoch': 1.48}\n",
      "{'loss': 2.1174, 'grad_norm': 5.185907363891602, 'learning_rate': 1.0627311134022893e-05, 'epoch': 1.5}\n",
      "{'loss': 2.1058, 'grad_norm': 4.799854278564453, 'learning_rate': 1.0563544822261184e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:32:07,175 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.087785243988037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.087785243988037, 'eval_runtime': 110.1594, 'eval_samples_per_second': 90.778, 'eval_steps_per_second': 45.389, 'epoch': 1.52}\n",
      "{'train_runtime': 9319.292, 'train_samples_per_second': 42.922, 'train_steps_per_second': 1.341, 'train_loss': 2.2421884657207287, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:34:30,456 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.0840227603912354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0840227603912354, 'eval_runtime': 142.0121, 'eval_samples_per_second': 70.417, 'eval_steps_per_second': 35.208, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 21:34:30,816 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 3800 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-22 21:34:31,256] Trial 1 finished with value: 2.0840227603912354 and parameters: {'learning_rate': 1.2853916978930139e-05, 'weight_decay': 0.011430983876313222, 'warmup_steps': 450, 'lr_scheduler_type': 'cosine'}. Best is trial 1 with value: 2.0840227603912354.\n",
      "2025-07-22 21:34:32,467 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-22 21:34:32,497 - INFO - Trial 2 parameters: learning_rate=1.0336843570697396e-05, weight_decay=0.09330606024425668, warmup_steps=450, lr_scheduler_type=linear\n",
      "2025-07-22 21:34:32,497 - INFO - VRAM usage before training (trial 2): 1.53GB / 7.96GB\n",
      "2025-07-22 21:34:32,537 - INFO - Starting new training for trial 2\n",
      "2025-07-22 21:34:43,479 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8171, 'grad_norm': 7.255630016326904, 'learning_rate': 1.0336843570697397e-06, 'epoch': 0.02}\n",
      "{'loss': 2.7232, 'grad_norm': 6.611057281494141, 'learning_rate': 2.1822225315916726e-06, 'epoch': 0.04}\n",
      "{'loss': 2.6048, 'grad_norm': 5.606945037841797, 'learning_rate': 3.3307607061136057e-06, 'epoch': 0.06}\n",
      "{'loss': 2.6048, 'grad_norm': 6.262466907501221, 'learning_rate': 4.4563281171450995e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 2.413343906402588, 'eval_runtime': 115.5106, 'eval_samples_per_second': 86.572, 'eval_steps_per_second': 43.286, 'epoch': 0.08}\n",
      "{'loss': 2.557, 'grad_norm': 8.0410737991333, 'learning_rate': 5.6048662916670335e-06, 'epoch': 0.1}\n",
      "{'loss': 2.5013, 'grad_norm': 9.267122268676758, 'learning_rate': 6.7304337026985265e-06, 'epoch': 0.12}\n",
      "{'loss': 2.4841, 'grad_norm': 11.373714447021484, 'learning_rate': 7.87897187722046e-06, 'epoch': 0.14}\n",
      "{'loss': 2.4614, 'grad_norm': 5.951822280883789, 'learning_rate': 9.027510051742392e-06, 'epoch': 0.16}\n",
      "{'eval_loss': 2.3089540004730225, 'eval_runtime': 115.7588, 'eval_samples_per_second': 86.387, 'eval_steps_per_second': 43.193, 'epoch': 0.16}\n",
      "{'loss': 2.4565, 'grad_norm': 5.96776819229126, 'learning_rate': 1.0176048226264325e-05, 'epoch': 0.18}\n",
      "{'loss': 2.4307, 'grad_norm': 5.762742042541504, 'learning_rate': 1.0299956908992832e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3841, 'grad_norm': 9.457642555236816, 'learning_rate': 1.0257065441894503e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3647, 'grad_norm': 5.032062530517578, 'learning_rate': 1.0214173974796174e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.252444267272949, 'eval_runtime': 110.3901, 'eval_samples_per_second': 90.588, 'eval_steps_per_second': 45.294, 'epoch': 0.24}\n",
      "{'loss': 2.4482, 'grad_norm': 5.217120170593262, 'learning_rate': 1.0171282507697845e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2931, 'grad_norm': 4.831495761871338, 'learning_rate': 1.0128391040599516e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3207, 'grad_norm': 5.094015121459961, 'learning_rate': 1.0085499573501186e-05, 'epoch': 0.3}\n",
      "{'loss': 2.4033, 'grad_norm': 7.2875075340271, 'learning_rate': 1.0042608106402855e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.2116246223449707, 'eval_runtime': 114.1049, 'eval_samples_per_second': 87.639, 'eval_steps_per_second': 43.819, 'epoch': 0.32}\n",
      "{'loss': 2.3062, 'grad_norm': 6.46658182144165, 'learning_rate': 9.999716639304526e-06, 'epoch': 0.34}\n",
      "{'loss': 2.3806, 'grad_norm': 8.336435317993164, 'learning_rate': 9.956825172206197e-06, 'epoch': 0.36}\n",
      "{'loss': 2.3381, 'grad_norm': 5.845487117767334, 'learning_rate': 9.913933705107868e-06, 'epoch': 0.38}\n",
      "{'loss': 2.2819, 'grad_norm': 5.686656951904297, 'learning_rate': 9.871042238009539e-06, 'epoch': 0.4}\n",
      "{'eval_loss': 2.1932644844055176, 'eval_runtime': 114.6319, 'eval_samples_per_second': 87.236, 'eval_steps_per_second': 43.618, 'epoch': 0.4}\n",
      "{'loss': 2.2634, 'grad_norm': 8.764763832092285, 'learning_rate': 9.82815077091121e-06, 'epoch': 0.42}\n",
      "{'loss': 2.2466, 'grad_norm': 12.54744815826416, 'learning_rate': 9.78525930381288e-06, 'epoch': 0.44}\n",
      "{'loss': 2.3069, 'grad_norm': 5.096960067749023, 'learning_rate': 9.742367836714551e-06, 'epoch': 0.46}\n",
      "{'loss': 2.2806, 'grad_norm': 6.995055198669434, 'learning_rate': 9.69947636961622e-06, 'epoch': 0.48}\n",
      "{'eval_loss': 2.1735591888427734, 'eval_runtime': 110.1417, 'eval_samples_per_second': 90.792, 'eval_steps_per_second': 45.396, 'epoch': 0.48}\n",
      "{'loss': 2.2802, 'grad_norm': 5.183615684509277, 'learning_rate': 9.656584902517891e-06, 'epoch': 0.5}\n",
      "{'loss': 2.2958, 'grad_norm': 8.215585708618164, 'learning_rate': 9.613693435419562e-06, 'epoch': 0.52}\n",
      "{'loss': 2.2582, 'grad_norm': 17.241289138793945, 'learning_rate': 9.570801968321232e-06, 'epoch': 0.54}\n",
      "{'loss': 2.3082, 'grad_norm': 5.546267032623291, 'learning_rate': 9.527910501222903e-06, 'epoch': 0.56}\n",
      "{'eval_loss': 2.179142475128174, 'eval_runtime': 110.163, 'eval_samples_per_second': 90.775, 'eval_steps_per_second': 45.387, 'epoch': 0.56}\n",
      "{'loss': 2.2909, 'grad_norm': 8.004666328430176, 'learning_rate': 9.485019034124574e-06, 'epoch': 0.58}\n",
      "{'loss': 2.2593, 'grad_norm': 14.287330627441406, 'learning_rate': 9.442127567026245e-06, 'epoch': 0.6}\n",
      "{'loss': 2.2229, 'grad_norm': 6.374972343444824, 'learning_rate': 9.399236099927914e-06, 'epoch': 0.62}\n",
      "{'loss': 2.3058, 'grad_norm': 10.777483940124512, 'learning_rate': 9.356344632829585e-06, 'epoch': 0.64}\n",
      "{'eval_loss': 2.1512889862060547, 'eval_runtime': 110.005, 'eval_samples_per_second': 90.905, 'eval_steps_per_second': 45.452, 'epoch': 0.64}\n",
      "{'loss': 2.213, 'grad_norm': 7.820773124694824, 'learning_rate': 9.313453165731256e-06, 'epoch': 0.66}\n",
      "{'loss': 2.262, 'grad_norm': 5.482919216156006, 'learning_rate': 9.270561698632926e-06, 'epoch': 0.68}\n",
      "{'loss': 2.2192, 'grad_norm': 4.789795875549316, 'learning_rate': 9.227670231534597e-06, 'epoch': 0.7}\n",
      "{'loss': 2.2393, 'grad_norm': 6.820813179016113, 'learning_rate': 9.184778764436268e-06, 'epoch': 0.72}\n",
      "{'eval_loss': 2.1389403343200684, 'eval_runtime': 109.5225, 'eval_samples_per_second': 91.305, 'eval_steps_per_second': 45.653, 'epoch': 0.72}\n",
      "{'loss': 2.2319, 'grad_norm': 5.990503311157227, 'learning_rate': 9.141887297337939e-06, 'epoch': 0.74}\n",
      "{'loss': 2.2294, 'grad_norm': 8.722650527954102, 'learning_rate': 9.09899583023961e-06, 'epoch': 0.76}\n",
      "{'loss': 2.2626, 'grad_norm': 4.826677322387695, 'learning_rate': 9.056104363141279e-06, 'epoch': 0.78}\n",
      "{'loss': 2.2635, 'grad_norm': 4.763473033905029, 'learning_rate': 9.01321289604295e-06, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1587536334991455, 'eval_runtime': 111.3789, 'eval_samples_per_second': 89.784, 'eval_steps_per_second': 44.892, 'epoch': 0.8}\n",
      "{'loss': 2.226, 'grad_norm': 6.763558864593506, 'learning_rate': 8.97032142894462e-06, 'epoch': 0.82}\n",
      "{'loss': 2.24, 'grad_norm': 5.62898588180542, 'learning_rate': 8.927429961846291e-06, 'epoch': 0.84}\n",
      "{'loss': 2.1496, 'grad_norm': 8.919591903686523, 'learning_rate': 8.884538494747962e-06, 'epoch': 0.86}\n",
      "{'loss': 2.2304, 'grad_norm': 4.982077121734619, 'learning_rate': 8.841647027649633e-06, 'epoch': 0.88}\n",
      "{'eval_loss': 2.114313840866089, 'eval_runtime': 113.7236, 'eval_samples_per_second': 87.932, 'eval_steps_per_second': 43.966, 'epoch': 0.88}\n",
      "{'loss': 2.2303, 'grad_norm': 6.5623345375061035, 'learning_rate': 8.798755560551303e-06, 'epoch': 0.9}\n",
      "{'loss': 2.1855, 'grad_norm': 7.383406162261963, 'learning_rate': 8.755864093452974e-06, 'epoch': 0.92}\n",
      "{'loss': 2.2373, 'grad_norm': 7.893999099731445, 'learning_rate': 8.712972626354643e-06, 'epoch': 0.94}\n",
      "{'loss': 2.1778, 'grad_norm': 4.741122245788574, 'learning_rate': 8.670081159256314e-06, 'epoch': 0.96}\n",
      "{'eval_loss': 2.1169207096099854, 'eval_runtime': 111.934, 'eval_samples_per_second': 89.338, 'eval_steps_per_second': 44.669, 'epoch': 0.96}\n",
      "{'loss': 2.2172, 'grad_norm': 6.111512184143066, 'learning_rate': 8.627189692157985e-06, 'epoch': 0.98}\n",
      "{'loss': 2.2433, 'grad_norm': 8.801122665405273, 'learning_rate': 8.584298225059656e-06, 'epoch': 1.0}\n",
      "{'loss': 2.0788, 'grad_norm': 7.173072814941406, 'learning_rate': 8.541406757961326e-06, 'epoch': 1.02}\n",
      "{'loss': 2.1753, 'grad_norm': 9.259408950805664, 'learning_rate': 8.498515290862997e-06, 'epoch': 1.04}\n",
      "{'eval_loss': 2.1187453269958496, 'eval_runtime': 114.2178, 'eval_samples_per_second': 87.552, 'eval_steps_per_second': 43.776, 'epoch': 1.04}\n",
      "{'loss': 2.1371, 'grad_norm': 6.5605034828186035, 'learning_rate': 8.455623823764668e-06, 'epoch': 1.06}\n",
      "{'loss': 2.1253, 'grad_norm': 12.4487886428833, 'learning_rate': 8.412732356666337e-06, 'epoch': 1.08}\n",
      "{'loss': 2.1349, 'grad_norm': 17.685396194458008, 'learning_rate': 8.369840889568008e-06, 'epoch': 1.1}\n",
      "{'loss': 2.1244, 'grad_norm': 5.600189208984375, 'learning_rate': 8.326949422469679e-06, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 23:31:12,132 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.107339382171631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.107339382171631, 'eval_runtime': 109.5999, 'eval_samples_per_second': 91.241, 'eval_steps_per_second': 45.62, 'epoch': 1.12}\n",
      "{'train_runtime': 6988.6631, 'train_samples_per_second': 57.236, 'train_steps_per_second': 1.789, 'train_loss': 2.3091955402919226, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 23:33:25,696 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.1087207794189453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1087207794189453, 'eval_runtime': 132.2933, 'eval_samples_per_second': 75.59, 'eval_steps_per_second': 37.795, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 23:33:26,056 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 2800 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-22 23:33:26,166] Trial 2 finished with value: 2.1087207794189453 and parameters: {'learning_rate': 1.0336843570697396e-05, 'weight_decay': 0.09330606024425668, 'warmup_steps': 450, 'lr_scheduler_type': 'linear'}. Best is trial 1 with value: 2.0840227603912354.\n",
      "2025-07-22 23:33:27,296 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-22 23:33:27,316 - INFO - Trial 3 parameters: learning_rate=1.34336568680343e-05, weight_decay=0.02014847788415866, warmup_steps=300, lr_scheduler_type=linear\n",
      "2025-07-22 23:33:27,326 - INFO - VRAM usage before training (trial 3): 1.53GB / 7.96GB\n",
      "2025-07-22 23:33:27,366 - INFO - Starting new training for trial 3\n",
      "2025-07-22 23:33:37,929 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7971, 'grad_norm': 7.976375102996826, 'learning_rate': 2.015048530205145e-06, 'epoch': 0.02}\n",
      "{'loss': 2.67, 'grad_norm': 6.334558486938477, 'learning_rate': 4.253991341544195e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5543, 'grad_norm': 5.585394382476807, 'learning_rate': 6.492934152883245e-06, 'epoch': 0.06}\n",
      "{'loss': 2.556, 'grad_norm': 5.596184253692627, 'learning_rate': 8.642319251768733e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 2.3708176612854004, 'eval_runtime': 109.3592, 'eval_samples_per_second': 91.442, 'eval_steps_per_second': 45.721, 'epoch': 0.08}\n",
      "{'loss': 2.5172, 'grad_norm': 14.524303436279297, 'learning_rate': 1.0881262063107785e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4598, 'grad_norm': 8.856276512145996, 'learning_rate': 1.3120204874446833e-05, 'epoch': 0.12}\n",
      "{'loss': 2.453, 'grad_norm': 9.52880573272705, 'learning_rate': 1.3386308733171556e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4283, 'grad_norm': 5.561245918273926, 'learning_rate': 1.3331252762400925e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.2647476196289062, 'eval_runtime': 109.6614, 'eval_samples_per_second': 91.19, 'eval_steps_per_second': 45.595, 'epoch': 0.16}\n",
      "{'loss': 2.4207, 'grad_norm': 7.8128557205200195, 'learning_rate': 1.3276196791630292e-05, 'epoch': 0.18}\n",
      "{'loss': 2.3954, 'grad_norm': 7.562075614929199, 'learning_rate': 1.3221140820859659e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3532, 'grad_norm': 8.443074226379395, 'learning_rate': 1.3166084850089027e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3338, 'grad_norm': 4.921164512634277, 'learning_rate': 1.3111028879318394e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.22257661819458, 'eval_runtime': 110.879, 'eval_samples_per_second': 90.188, 'eval_steps_per_second': 45.094, 'epoch': 0.24}\n",
      "{'loss': 2.42, 'grad_norm': 5.061107635498047, 'learning_rate': 1.3055972908547763e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2648, 'grad_norm': 5.459494113922119, 'learning_rate': 1.300091693777713e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2991, 'grad_norm': 7.859596252441406, 'learning_rate': 1.2945860967006497e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3796, 'grad_norm': 7.649373531341553, 'learning_rate': 1.2890804996235864e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.216935873031616, 'eval_runtime': 109.9374, 'eval_samples_per_second': 90.961, 'eval_steps_per_second': 45.48, 'epoch': 0.32}\n",
      "{'loss': 2.2809, 'grad_norm': 7.1403632164001465, 'learning_rate': 1.2835749025465233e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3574, 'grad_norm': 5.962576389312744, 'learning_rate': 1.27806930546946e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3197, 'grad_norm': 6.001961708068848, 'learning_rate': 1.2725637083923969e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2602, 'grad_norm': 5.038066387176514, 'learning_rate': 1.2671682232568749e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.1777091026306152, 'eval_runtime': 112.6331, 'eval_samples_per_second': 88.784, 'eval_steps_per_second': 44.392, 'epoch': 0.4}\n",
      "{'loss': 2.2492, 'grad_norm': 8.347604751586914, 'learning_rate': 1.2616626261798116e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2267, 'grad_norm': 5.62959098815918, 'learning_rate': 1.2561570291027483e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2865, 'grad_norm': 4.9730119705200195, 'learning_rate': 1.250651432025685e-05, 'epoch': 0.46}\n",
      "{'loss': 2.257, 'grad_norm': 7.64113712310791, 'learning_rate': 1.2451458349486219e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.159986972808838, 'eval_runtime': 115.7393, 'eval_samples_per_second': 86.401, 'eval_steps_per_second': 43.201, 'epoch': 0.48}\n",
      "{'loss': 2.2595, 'grad_norm': 5.331308364868164, 'learning_rate': 1.2396402378715587e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2807, 'grad_norm': 5.413486480712891, 'learning_rate': 1.2341346407944954e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2399, 'grad_norm': 20.236494064331055, 'learning_rate': 1.2286290437174321e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2934, 'grad_norm': 5.230902671813965, 'learning_rate': 1.2231234466403688e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1788413524627686, 'eval_runtime': 113.9795, 'eval_samples_per_second': 87.735, 'eval_steps_per_second': 43.868, 'epoch': 0.56}\n",
      "{'loss': 2.2712, 'grad_norm': 4.787906169891357, 'learning_rate': 1.2176178495633057e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2436, 'grad_norm': 12.258155822753906, 'learning_rate': 1.2121122524862424e-05, 'epoch': 0.6}\n",
      "{'loss': 2.2, 'grad_norm': 4.957180500030518, 'learning_rate': 1.2066066554091793e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2895, 'grad_norm': 9.579063415527344, 'learning_rate': 1.2011010583321158e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.136239528656006, 'eval_runtime': 116.8989, 'eval_samples_per_second': 85.544, 'eval_steps_per_second': 42.772, 'epoch': 0.64}\n",
      "{'loss': 2.202, 'grad_norm': 5.7881574630737305, 'learning_rate': 1.1955954612550527e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2471, 'grad_norm': 5.4748992919921875, 'learning_rate': 1.1900898641779894e-05, 'epoch': 0.68}\n",
      "{'loss': 2.2026, 'grad_norm': 4.312294006347656, 'learning_rate': 1.1845842671009263e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2244, 'grad_norm': 7.615719318389893, 'learning_rate': 1.1790786700238631e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.129998207092285, 'eval_runtime': 110.6106, 'eval_samples_per_second': 90.407, 'eval_steps_per_second': 45.204, 'epoch': 0.72}\n",
      "{'loss': 2.2168, 'grad_norm': 5.948770523071289, 'learning_rate': 1.1735730729467997e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2165, 'grad_norm': 7.87469482421875, 'learning_rate': 1.1680674758697366e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2472, 'grad_norm': 4.631524085998535, 'learning_rate': 1.1625618787926733e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2471, 'grad_norm': 5.264957427978516, 'learning_rate': 1.1570562817156101e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.158094644546509, 'eval_runtime': 110.6936, 'eval_samples_per_second': 90.339, 'eval_steps_per_second': 45.17, 'epoch': 0.8}\n",
      "{'loss': 2.2083, 'grad_norm': 6.301600456237793, 'learning_rate': 1.1515506846385468e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2243, 'grad_norm': 4.7065110206604, 'learning_rate': 1.1460450875614835e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1314, 'grad_norm': 6.619247913360596, 'learning_rate': 1.1405394904844202e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2135, 'grad_norm': 4.720184326171875, 'learning_rate': 1.1350338934073571e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1038920879364014, 'eval_runtime': 110.7785, 'eval_samples_per_second': 90.27, 'eval_steps_per_second': 45.135, 'epoch': 0.88}\n",
      "{'loss': 2.2145, 'grad_norm': 4.975099563598633, 'learning_rate': 1.1295282963302938e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1695, 'grad_norm': 7.456912517547607, 'learning_rate': 1.1240226992532307e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2203, 'grad_norm': 6.145691871643066, 'learning_rate': 1.1185171021761674e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1631, 'grad_norm': 4.527063369750977, 'learning_rate': 1.1130115050991041e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.1128273010253906, 'eval_runtime': 116.9794, 'eval_samples_per_second': 85.485, 'eval_steps_per_second': 42.743, 'epoch': 0.96}\n",
      "{'loss': 2.2028, 'grad_norm': 5.342030048370361, 'learning_rate': 1.107505908022041e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2258, 'grad_norm': 10.82805061340332, 'learning_rate': 1.1020003109449777e-05, 'epoch': 1.0}\n",
      "{'loss': 2.0434, 'grad_norm': 5.012837886810303, 'learning_rate': 1.0964947138679145e-05, 'epoch': 1.02}\n",
      "{'loss': 2.1378, 'grad_norm': 12.1392183303833, 'learning_rate': 1.0909891167908512e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.108334541320801, 'eval_runtime': 110.8303, 'eval_samples_per_second': 90.228, 'eval_steps_per_second': 45.114, 'epoch': 1.04}\n",
      "{'loss': 2.1013, 'grad_norm': 6.704287052154541, 'learning_rate': 1.085483519713788e-05, 'epoch': 1.06}\n",
      "{'loss': 2.087, 'grad_norm': 11.681108474731445, 'learning_rate': 1.0799779226367247e-05, 'epoch': 1.08}\n",
      "{'loss': 2.1002, 'grad_norm': 11.376936912536621, 'learning_rate': 1.0744723255596615e-05, 'epoch': 1.1}\n",
      "{'loss': 2.0884, 'grad_norm': 6.579276084899902, 'learning_rate': 1.0689667284825982e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 01:30:02,535 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.0984644889831543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0984644889831543, 'eval_runtime': 110.5059, 'eval_samples_per_second': 90.493, 'eval_steps_per_second': 45.246, 'epoch': 1.12}\n",
      "{'train_runtime': 6984.6061, 'train_samples_per_second': 57.269, 'train_steps_per_second': 1.79, 'train_loss': 2.2848770414079937, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 01:32:36,880 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.099980354309082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.099980354309082, 'eval_runtime': 153.0305, 'eval_samples_per_second': 65.346, 'eval_steps_per_second': 32.673, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 01:32:37,223 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-23 01:32:37,623] Trial 3 finished with value: 2.099980354309082 and parameters: {'learning_rate': 1.34336568680343e-05, 'weight_decay': 0.02014847788415866, 'warmup_steps': 300, 'lr_scheduler_type': 'linear'}. Best is trial 1 with value: 2.0840227603912354.\n",
      "2025-07-23 01:32:38,773 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 01:32:38,797 - INFO - Trial 4 parameters: learning_rate=2.6771137242145903e-05, weight_decay=0.013787764619353767, warmup_steps=200, lr_scheduler_type=cosine\n",
      "2025-07-23 01:32:38,798 - INFO - VRAM usage before training (trial 4): 1.53GB / 7.96GB\n",
      "2025-07-23 01:32:38,849 - INFO - Starting new training for trial 4\n",
      "2025-07-23 01:32:49,513 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7474, 'grad_norm': 7.37462854385376, 'learning_rate': 6.023505879482828e-06, 'epoch': 0.02}\n",
      "{'loss': 2.593, 'grad_norm': 6.3125081062316895, 'learning_rate': 1.2716290190019304e-05, 'epoch': 0.04}\n",
      "{'loss': 2.4957, 'grad_norm': 5.938993453979492, 'learning_rate': 1.940907450055578e-05, 'epoch': 0.06}\n",
      "{'loss': 2.5078, 'grad_norm': 4.848028659820557, 'learning_rate': 2.5968003124881524e-05, 'epoch': 0.08}\n",
      "{'eval_loss': 2.3206734657287598, 'eval_runtime': 108.642, 'eval_samples_per_second': 92.045, 'eval_steps_per_second': 46.023, 'epoch': 0.08}\n",
      "{'loss': 2.4865, 'grad_norm': 13.955236434936523, 'learning_rate': 2.6770291968879354e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4308, 'grad_norm': 7.160529136657715, 'learning_rate': 2.6767279517753222e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4295, 'grad_norm': 7.3774824142456055, 'learning_rate': 2.67622099417975e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4013, 'grad_norm': 5.519293308258057, 'learning_rate': 2.6754877149203204e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.251462936401367, 'eval_runtime': 108.449, 'eval_samples_per_second': 92.209, 'eval_steps_per_second': 46.105, 'epoch': 0.16}\n",
      "{'loss': 2.4067, 'grad_norm': 7.107909679412842, 'learning_rate': 2.6745363974677155e-05, 'epoch': 0.18}\n",
      "{'loss': 2.3669, 'grad_norm': 7.889579772949219, 'learning_rate': 2.6733671969709758e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3488, 'grad_norm': 9.441373825073242, 'learning_rate': 2.6719803041133834e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3294, 'grad_norm': 4.660879135131836, 'learning_rate': 2.6703759450813657e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.2179183959960938, 'eval_runtime': 108.3307, 'eval_samples_per_second': 92.31, 'eval_steps_per_second': 46.155, 'epoch': 0.24}\n",
      "{'loss': 2.4143, 'grad_norm': 5.13654088973999, 'learning_rate': 2.6685543815276057e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2743, 'grad_norm': 4.444699764251709, 'learning_rate': 2.66651591052837e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2918, 'grad_norm': 5.715793609619141, 'learning_rate': 2.664260864535058e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3717, 'grad_norm': 6.005204677581787, 'learning_rate': 2.6617896113199838e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.2011005878448486, 'eval_runtime': 108.8607, 'eval_samples_per_second': 91.861, 'eval_steps_per_second': 45.93, 'epoch': 0.32}\n",
      "{'loss': 2.2666, 'grad_norm': 4.958570957183838, 'learning_rate': 2.6591025539163974e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3514, 'grad_norm': 7.101336479187012, 'learning_rate': 2.656200130552753e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3087, 'grad_norm': 7.544090270996094, 'learning_rate': 2.6530828145812392e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2556, 'grad_norm': 5.143069744110107, 'learning_rate': 2.649751114400583e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.179485321044922, 'eval_runtime': 108.6877, 'eval_samples_per_second': 92.007, 'eval_steps_per_second': 46.003, 'epoch': 0.4}\n",
      "{'loss': 2.2513, 'grad_norm': 5.3401055335998535, 'learning_rate': 2.646205573373132e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2113, 'grad_norm': 5.528096675872803, 'learning_rate': 2.6424467697362402e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2674, 'grad_norm': 4.552468299865723, 'learning_rate': 2.6384753165079654e-05, 'epoch': 0.46}\n",
      "{'loss': 2.249, 'grad_norm': 4.929957389831543, 'learning_rate': 2.6342918613870897e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.163719892501831, 'eval_runtime': 109.4791, 'eval_samples_per_second': 91.342, 'eval_steps_per_second': 45.671, 'epoch': 0.48}\n",
      "{'loss': 2.2495, 'grad_norm': 4.80724573135376, 'learning_rate': 2.6298970866474896e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2754, 'grad_norm': 5.908523082733154, 'learning_rate': 2.6252917090268657e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2296, 'grad_norm': 17.339279174804688, 'learning_rate': 2.6204764796098466e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2806, 'grad_norm': 5.158090591430664, 'learning_rate': 2.6154521837055017e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1549434661865234, 'eval_runtime': 109.0797, 'eval_samples_per_second': 91.676, 'eval_steps_per_second': 45.838, 'epoch': 0.56}\n",
      "{'loss': 2.2591, 'grad_norm': 7.191159725189209, 'learning_rate': 2.6102196407192605e-05, 'epoch': 0.58}\n",
      "{'loss': 2.236, 'grad_norm': 7.010985374450684, 'learning_rate': 2.6047797040192808e-05, 'epoch': 0.6}\n",
      "{'loss': 2.2085, 'grad_norm': 8.732150077819824, 'learning_rate': 2.5991332607972718e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2843, 'grad_norm': 7.482856273651123, 'learning_rate': 2.5932812319238054e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.1497268676757812, 'eval_runtime': 109.0986, 'eval_samples_per_second': 91.66, 'eval_steps_per_second': 45.83, 'epoch': 0.64}\n",
      "{'loss': 2.1863, 'grad_norm': 8.05283260345459, 'learning_rate': 2.58722457179813e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2373, 'grad_norm': 5.12925910949707, 'learning_rate': 2.5809642681925215e-05, 'epoch': 0.68}\n",
      "{'loss': 2.1961, 'grad_norm': 3.7747812271118164, 'learning_rate': 2.5745013420911865e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2132, 'grad_norm': 6.582552433013916, 'learning_rate': 2.567836847523754e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.1455795764923096, 'eval_runtime': 109.1179, 'eval_samples_per_second': 91.644, 'eval_steps_per_second': 45.822, 'epoch': 0.72}\n",
      "{'loss': 2.2019, 'grad_norm': 4.773434162139893, 'learning_rate': 2.5609718713933723e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2024, 'grad_norm': 6.873374938964844, 'learning_rate': 2.5539075332994502e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2377, 'grad_norm': 4.6308064460754395, 'learning_rate': 2.5466449853550606e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2496, 'grad_norm': 4.424716472625732, 'learning_rate': 2.5391854119990464e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1191656589508057, 'eval_runtime': 109.195, 'eval_samples_per_second': 91.579, 'eval_steps_per_second': 45.79, 'epoch': 0.8}\n",
      "{'loss': 2.199, 'grad_norm': 4.209176063537598, 'learning_rate': 2.5315300298028503e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2139, 'grad_norm': 4.262684345245361, 'learning_rate': 2.523680087272106e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1154, 'grad_norm': 9.833944320678711, 'learning_rate': 2.5156368646430224e-05, 'epoch': 0.86}\n",
      "{'loss': 2.1976, 'grad_norm': 4.095247268676758, 'learning_rate': 2.5074016736735903e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1627111434936523, 'eval_runtime': 110.07, 'eval_samples_per_second': 90.851, 'eval_steps_per_second': 45.426, 'epoch': 0.88}\n",
      "{'loss': 2.2022, 'grad_norm': 8.93297004699707, 'learning_rate': 2.4989758574296498e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1498, 'grad_norm': 7.524846076965332, 'learning_rate': 2.4903607900658517e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2105, 'grad_norm': 6.719858646392822, 'learning_rate': 2.4815578766015506e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1472, 'grad_norm': 4.099581241607666, 'learning_rate': 2.472568552691658e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.109602689743042, 'eval_runtime': 110.1924, 'eval_samples_per_second': 90.75, 'eval_steps_per_second': 45.375, 'epoch': 0.96}\n",
      "{'loss': 2.1908, 'grad_norm': 4.642303466796875, 'learning_rate': 2.4633942843925068e-05, 'epoch': 0.98}\n",
      "{'loss': 2.213, 'grad_norm': 7.649129867553711, 'learning_rate': 2.4540365679227525e-05, 'epoch': 1.0}\n",
      "{'loss': 1.9639, 'grad_norm': 4.912815570831299, 'learning_rate': 2.4444969294193586e-05, 'epoch': 1.02}\n",
      "{'loss': 2.0491, 'grad_norm': 7.350557804107666, 'learning_rate': 2.4347769246886993e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.097339391708374, 'eval_runtime': 109.072, 'eval_samples_per_second': 91.683, 'eval_steps_per_second': 45.841, 'epoch': 1.04}\n",
      "{'loss': 2.0141, 'grad_norm': 4.13563346862793, 'learning_rate': 2.424878138952826e-05, 'epoch': 1.06}\n",
      "{'loss': 2.0051, 'grad_norm': 10.806028366088867, 'learning_rate': 2.4148021865909355e-05, 'epoch': 1.08}\n",
      "{'loss': 2.0209, 'grad_norm': 10.535234451293945, 'learning_rate': 2.404550710876083e-05, 'epoch': 1.1}\n",
      "{'loss': 2.006, 'grad_norm': 11.753091812133789, 'learning_rate': 2.3941253837071845e-05, 'epoch': 1.12}\n",
      "{'eval_loss': 2.106954574584961, 'eval_runtime': 108.8499, 'eval_samples_per_second': 91.87, 'eval_steps_per_second': 45.935, 'epoch': 1.12}\n",
      "{'loss': 2.0152, 'grad_norm': 5.866226673126221, 'learning_rate': 2.3835279053363456e-05, 'epoch': 1.1400000000000001}\n",
      "{'loss': 1.9767, 'grad_norm': 8.016417503356934, 'learning_rate': 2.372760004091574e-05, 'epoch': 1.16}\n",
      "{'loss': 2.0058, 'grad_norm': 4.751996040344238, 'learning_rate': 2.361823436094906e-05, 'epoch': 1.18}\n",
      "{'loss': 2.0129, 'grad_norm': 4.958000183105469, 'learning_rate': 2.3507199849760032e-05, 'epoch': 1.2}\n",
      "{'eval_loss': 2.0939130783081055, 'eval_runtime': 109.2611, 'eval_samples_per_second': 91.524, 'eval_steps_per_second': 45.762, 'epoch': 1.2}\n",
      "{'loss': 1.9866, 'grad_norm': 3.938587188720703, 'learning_rate': 2.339451461581265e-05, 'epoch': 1.22}\n",
      "{'loss': 1.999, 'grad_norm': 3.57897686958313, 'learning_rate': 2.3280197036784977e-05, 'epoch': 1.24}\n",
      "{'loss': 2.0123, 'grad_norm': 7.133522987365723, 'learning_rate': 2.3164265756571965e-05, 'epoch': 1.26}\n",
      "{'loss': 2.0024, 'grad_norm': 9.554203033447266, 'learning_rate': 2.3046739682244843e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 03:42:36,932 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.1261348724365234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1261348724365234, 'eval_runtime': 109.041, 'eval_samples_per_second': 91.709, 'eval_steps_per_second': 45.854, 'epoch': 1.28}\n",
      "{'train_runtime': 7787.4205, 'train_samples_per_second': 51.365, 'train_steps_per_second': 1.605, 'train_loss': 2.2299020862579346, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 03:45:06,410 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.1252169609069824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1252169609069824, 'eval_runtime': 148.1575, 'eval_samples_per_second': 67.496, 'eval_steps_per_second': 33.748, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 03:45:06,740 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch121\\Lib\\site-packages\\optuna\\trial\\_trial.py:501: UserWarning: The reported value is ignored because this `step` 3200 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-07-23 03:45:07,150] Trial 4 finished with value: 2.1252169609069824 and parameters: {'learning_rate': 2.6771137242145903e-05, 'weight_decay': 0.013787764619353767, 'warmup_steps': 200, 'lr_scheduler_type': 'cosine'}. Best is trial 1 with value: 2.0840227603912354.\n",
      "2025-07-23 03:45:08,290 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 03:45:08,320 - INFO - Trial 5 parameters: learning_rate=3.538461259525519e-05, weight_decay=0.015837031559118753, warmup_steps=300, lr_scheduler_type=linear\n",
      "2025-07-23 03:45:08,320 - INFO - VRAM usage before training (trial 5): 1.53GB / 7.96GB\n",
      "2025-07-23 03:45:08,370 - INFO - Starting new training for trial 5\n",
      "2025-07-23 03:45:18,840 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7539, 'grad_norm': 7.405422210693359, 'learning_rate': 5.307691889288278e-06, 'epoch': 0.02}\n",
      "{'loss': 2.601, 'grad_norm': 6.280384063720703, 'learning_rate': 1.120512732183081e-05, 'epoch': 0.04}\n",
      "{'loss': 2.5, 'grad_norm': 5.811661243438721, 'learning_rate': 1.710256275437334e-05, 'epoch': 0.06}\n",
      "{'loss': 2.5089, 'grad_norm': 5.000411033630371, 'learning_rate': 2.2999998186915873e-05, 'epoch': 0.08}\n",
      "{'eval_loss': 2.3226265907287598, 'eval_runtime': 109.8387, 'eval_samples_per_second': 91.043, 'eval_steps_per_second': 45.521, 'epoch': 0.08}\n",
      "{'loss': 2.4902, 'grad_norm': 11.01403522491455, 'learning_rate': 2.8897433619458402e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4434, 'grad_norm': 7.720127105712891, 'learning_rate': 3.4794869052000935e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4482, 'grad_norm': 7.812938213348389, 'learning_rate': 3.525409558158417e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4263, 'grad_norm': 6.838233470916748, 'learning_rate': 3.510907667750525e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.2762153148651123, 'eval_runtime': 109.941, 'eval_samples_per_second': 90.958, 'eval_steps_per_second': 45.479, 'epoch': 0.16}\n",
      "{'loss': 2.4264, 'grad_norm': 6.294904708862305, 'learning_rate': 3.4964057773426335e-05, 'epoch': 0.18}\n",
      "{'loss': 2.3819, 'grad_norm': 8.940058708190918, 'learning_rate': 3.4821939247429e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3475, 'grad_norm': 7.038855075836182, 'learning_rate': 3.4676920343350085e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3229, 'grad_norm': 4.7238054275512695, 'learning_rate': 3.453190143927117e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.252586603164673, 'eval_runtime': 109.8287, 'eval_samples_per_second': 91.051, 'eval_steps_per_second': 45.525, 'epoch': 0.24}\n",
      "{'loss': 2.4156, 'grad_norm': 5.015871524810791, 'learning_rate': 3.438688253519226e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2575, 'grad_norm': 4.38116979598999, 'learning_rate': 3.424186363111334e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2951, 'grad_norm': 6.36939001083374, 'learning_rate': 3.409684472703442e-05, 'epoch': 0.3}\n",
      "{'loss': 2.378, 'grad_norm': 5.3247904777526855, 'learning_rate': 3.395182582295551e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.2059953212738037, 'eval_runtime': 109.9528, 'eval_samples_per_second': 90.948, 'eval_steps_per_second': 45.474, 'epoch': 0.32}\n",
      "{'loss': 2.2686, 'grad_norm': 4.391782283782959, 'learning_rate': 3.3806806918876594e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3561, 'grad_norm': 6.902624130249023, 'learning_rate': 3.366178801479768e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3124, 'grad_norm': 4.984175205230713, 'learning_rate': 3.3516769110718766e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2516, 'grad_norm': 5.195938587188721, 'learning_rate': 3.3371750206639855e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.1786422729492188, 'eval_runtime': 109.9801, 'eval_samples_per_second': 90.926, 'eval_steps_per_second': 45.463, 'epoch': 0.4}\n",
      "{'loss': 2.24, 'grad_norm': 8.69136905670166, 'learning_rate': 3.322673130256094e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2161, 'grad_norm': 8.452569007873535, 'learning_rate': 3.308171239848202e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2728, 'grad_norm': 4.40264368057251, 'learning_rate': 3.293669349440311e-05, 'epoch': 0.46}\n",
      "{'loss': 2.257, 'grad_norm': 4.504913806915283, 'learning_rate': 3.279167459032419e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.161428213119507, 'eval_runtime': 110.1528, 'eval_samples_per_second': 90.783, 'eval_steps_per_second': 45.391, 'epoch': 0.48}\n",
      "{'loss': 2.2567, 'grad_norm': 6.3221940994262695, 'learning_rate': 3.264955606432686e-05, 'epoch': 0.5}\n",
      "{'loss': 2.273, 'grad_norm': 4.835588455200195, 'learning_rate': 3.250453716024794e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2275, 'grad_norm': 10.276856422424316, 'learning_rate': 3.2359518256169025e-05, 'epoch': 0.54}\n",
      "{'loss': 2.279, 'grad_norm': 5.457676887512207, 'learning_rate': 3.2214499352090114e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1879842281341553, 'eval_runtime': 110.7208, 'eval_samples_per_second': 90.317, 'eval_steps_per_second': 45.159, 'epoch': 0.56}\n",
      "{'loss': 2.2588, 'grad_norm': 7.219326972961426, 'learning_rate': 3.2069480448011196e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2309, 'grad_norm': 6.3411455154418945, 'learning_rate': 3.1924461543932286e-05, 'epoch': 0.6}\n",
      "{'loss': 2.1953, 'grad_norm': 4.525317192077637, 'learning_rate': 3.177944263985337e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2837, 'grad_norm': 3.9003729820251465, 'learning_rate': 3.163442373577445e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.1549746990203857, 'eval_runtime': 110.341, 'eval_samples_per_second': 90.628, 'eval_steps_per_second': 45.314, 'epoch': 0.64}\n",
      "{'loss': 2.187, 'grad_norm': 6.0644636154174805, 'learning_rate': 3.148940483169554e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2353, 'grad_norm': 5.235890865325928, 'learning_rate': 3.134438592761662e-05, 'epoch': 0.68}\n",
      "{'loss': 2.1905, 'grad_norm': 3.5046443939208984, 'learning_rate': 3.119936702353771e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2072, 'grad_norm': 6.62838888168335, 'learning_rate': 3.1054348119458794e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.133469820022583, 'eval_runtime': 110.1501, 'eval_samples_per_second': 90.785, 'eval_steps_per_second': 45.393, 'epoch': 0.72}\n",
      "{'loss': 2.2002, 'grad_norm': 4.8639912605285645, 'learning_rate': 3.0909329215379884e-05, 'epoch': 0.74}\n",
      "{'loss': 2.1989, 'grad_norm': 4.571638107299805, 'learning_rate': 3.0764310311300966e-05, 'epoch': 0.76}\n",
      "{'loss': 2.228, 'grad_norm': 4.178912162780762, 'learning_rate': 3.061929140722205e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2329, 'grad_norm': 5.630261421203613, 'learning_rate': 3.0474272503143135e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1762125492095947, 'eval_runtime': 109.9237, 'eval_samples_per_second': 90.972, 'eval_steps_per_second': 45.486, 'epoch': 0.8}\n",
      "{'loss': 2.1965, 'grad_norm': 6.1873884201049805, 'learning_rate': 3.0329253599064224e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2102, 'grad_norm': 4.578584671020508, 'learning_rate': 3.0184234694985306e-05, 'epoch': 0.84}\n",
      "{'loss': 2.108, 'grad_norm': 7.423022270202637, 'learning_rate': 3.0039215790906392e-05, 'epoch': 0.86}\n",
      "{'loss': 2.1988, 'grad_norm': 3.931361198425293, 'learning_rate': 2.9894196886827475e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1123406887054443, 'eval_runtime': 109.9307, 'eval_samples_per_second': 90.966, 'eval_steps_per_second': 45.483, 'epoch': 0.88}\n",
      "{'loss': 2.1969, 'grad_norm': 6.116519451141357, 'learning_rate': 2.9749177982748564e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1418, 'grad_norm': 7.018927574157715, 'learning_rate': 2.960415907866965e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2028, 'grad_norm': 5.986027240753174, 'learning_rate': 2.9459140174590733e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1436, 'grad_norm': 4.95292854309082, 'learning_rate': 2.9314121270511822e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.100254535675049, 'eval_runtime': 109.9085, 'eval_samples_per_second': 90.985, 'eval_steps_per_second': 45.492, 'epoch': 0.96}\n",
      "{'loss': 2.1839, 'grad_norm': 6.142056941986084, 'learning_rate': 2.9169102366432904e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2057, 'grad_norm': 7.054244518280029, 'learning_rate': 2.902408346235399e-05, 'epoch': 1.0}\n",
      "{'loss': 1.9347, 'grad_norm': 4.291223049163818, 'learning_rate': 2.8879064558275073e-05, 'epoch': 1.02}\n",
      "{'loss': 2.0206, 'grad_norm': 5.400659561157227, 'learning_rate': 2.8734045654196162e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.1598408222198486, 'eval_runtime': 110.1202, 'eval_samples_per_second': 90.81, 'eval_steps_per_second': 45.405, 'epoch': 1.04}\n",
      "{'loss': 1.9872, 'grad_norm': 5.851222991943359, 'learning_rate': 2.8589026750117245e-05, 'epoch': 1.06}\n",
      "{'loss': 1.9837, 'grad_norm': 6.716696739196777, 'learning_rate': 2.844400784603833e-05, 'epoch': 1.08}\n",
      "{'loss': 1.9874, 'grad_norm': 4.306801795959473, 'learning_rate': 2.8298988941959413e-05, 'epoch': 1.1}\n",
      "{'loss': 1.9746, 'grad_norm': 8.557021141052246, 'learning_rate': 2.8153970037880502e-05, 'epoch': 1.12}\n",
      "{'eval_loss': 2.1049420833587646, 'eval_runtime': 109.9522, 'eval_samples_per_second': 90.949, 'eval_steps_per_second': 45.474, 'epoch': 1.12}\n",
      "{'loss': 1.984, 'grad_norm': 6.571026802062988, 'learning_rate': 2.8008951133801585e-05, 'epoch': 1.1400000000000001}\n",
      "{'loss': 1.9447, 'grad_norm': 4.67708158493042, 'learning_rate': 2.786393222972267e-05, 'epoch': 1.16}\n",
      "{'loss': 1.9769, 'grad_norm': 5.352391242980957, 'learning_rate': 2.771891332564376e-05, 'epoch': 1.18}\n",
      "{'loss': 1.9843, 'grad_norm': 4.116058826446533, 'learning_rate': 2.7573894421564843e-05, 'epoch': 1.2}\n",
      "{'eval_loss': 2.089772939682007, 'eval_runtime': 110.0188, 'eval_samples_per_second': 90.894, 'eval_steps_per_second': 45.447, 'epoch': 1.2}\n",
      "{'loss': 1.9594, 'grad_norm': 6.067636489868164, 'learning_rate': 2.742887551748593e-05, 'epoch': 1.22}\n",
      "{'loss': 1.9735, 'grad_norm': 3.4107472896575928, 'learning_rate': 2.728385661340701e-05, 'epoch': 1.24}\n",
      "{'loss': 1.9868, 'grad_norm': 5.076149940490723, 'learning_rate': 2.71388377093281e-05, 'epoch': 1.26}\n",
      "{'loss': 1.978, 'grad_norm': 8.970956802368164, 'learning_rate': 2.6993818805249183e-05, 'epoch': 1.28}\n",
      "{'eval_loss': 2.112290143966675, 'eval_runtime': 110.0101, 'eval_samples_per_second': 90.901, 'eval_steps_per_second': 45.45, 'epoch': 1.28}\n",
      "{'loss': 1.9433, 'grad_norm': 5.934335708618164, 'learning_rate': 2.684879990117027e-05, 'epoch': 1.3}\n",
      "{'loss': 1.9673, 'grad_norm': 5.219139575958252, 'learning_rate': 2.670378099709135e-05, 'epoch': 1.32}\n",
      "{'loss': 1.9771, 'grad_norm': 4.42616081237793, 'learning_rate': 2.655876209301244e-05, 'epoch': 1.34}\n",
      "{'loss': 1.9805, 'grad_norm': 5.333838939666748, 'learning_rate': 2.6413743188933523e-05, 'epoch': 1.3599999999999999}\n",
      "{'eval_loss': 2.0994648933410645, 'eval_runtime': 110.9185, 'eval_samples_per_second': 90.156, 'eval_steps_per_second': 45.078, 'epoch': 1.3599999999999999}\n",
      "{'loss': 2.0219, 'grad_norm': 5.556146144866943, 'learning_rate': 2.626872428485461e-05, 'epoch': 1.38}\n",
      "{'loss': 2.0085, 'grad_norm': 4.173588275909424, 'learning_rate': 2.612370538077569e-05, 'epoch': 1.4}\n",
      "{'loss': 1.9565, 'grad_norm': 12.125083923339844, 'learning_rate': 2.597868647669678e-05, 'epoch': 1.42}\n",
      "{'loss': 1.9707, 'grad_norm': 4.6143951416015625, 'learning_rate': 2.5833667572617867e-05, 'epoch': 1.44}\n",
      "{'eval_loss': 2.0781571865081787, 'eval_runtime': 111.1843, 'eval_samples_per_second': 89.941, 'eval_steps_per_second': 44.97, 'epoch': 1.44}\n",
      "{'loss': 1.9964, 'grad_norm': 3.6528682708740234, 'learning_rate': 2.568864866853895e-05, 'epoch': 1.46}\n",
      "{'loss': 2.0034, 'grad_norm': 3.821009635925293, 'learning_rate': 2.554362976446004e-05, 'epoch': 1.48}\n",
      "{'loss': 2.0185, 'grad_norm': 9.758502960205078, 'learning_rate': 2.539861086038112e-05, 'epoch': 1.5}\n",
      "{'loss': 1.9962, 'grad_norm': 5.6746134757995605, 'learning_rate': 2.5253591956302207e-05, 'epoch': 1.52}\n",
      "{'eval_loss': 2.119446039199829, 'eval_runtime': 109.9658, 'eval_samples_per_second': 90.937, 'eval_steps_per_second': 45.469, 'epoch': 1.52}\n",
      "{'loss': 2.0057, 'grad_norm': 8.097902297973633, 'learning_rate': 2.510857305222329e-05, 'epoch': 1.54}\n",
      "{'loss': 1.9352, 'grad_norm': 8.079656600952148, 'learning_rate': 2.496355414814438e-05, 'epoch': 1.56}\n",
      "{'loss': 1.9634, 'grad_norm': 3.9319546222686768, 'learning_rate': 2.481853524406546e-05, 'epoch': 1.58}\n",
      "{'loss': 1.9308, 'grad_norm': 4.507396697998047, 'learning_rate': 2.4673516339986547e-05, 'epoch': 1.6}\n",
      "{'eval_loss': 2.0695838928222656, 'eval_runtime': 109.8994, 'eval_samples_per_second': 90.992, 'eval_steps_per_second': 45.496, 'epoch': 1.6}\n",
      "{'loss': 1.9454, 'grad_norm': 7.8433732986450195, 'learning_rate': 2.4528497435907633e-05, 'epoch': 1.62}\n",
      "{'loss': 1.9263, 'grad_norm': 7.135405540466309, 'learning_rate': 2.438347853182872e-05, 'epoch': 1.6400000000000001}\n",
      "{'loss': 1.9639, 'grad_norm': 4.244819641113281, 'learning_rate': 2.4238459627749805e-05, 'epoch': 1.6600000000000001}\n",
      "{'loss': 1.9991, 'grad_norm': 5.112410068511963, 'learning_rate': 2.4093440723670887e-05, 'epoch': 1.6800000000000002}\n",
      "{'eval_loss': 2.0616865158081055, 'eval_runtime': 110.383, 'eval_samples_per_second': 90.594, 'eval_steps_per_second': 45.297, 'epoch': 1.6800000000000002}\n",
      "{'loss': 1.9588, 'grad_norm': 3.673830986022949, 'learning_rate': 2.3948421819591977e-05, 'epoch': 1.7}\n",
      "{'loss': 1.9836, 'grad_norm': 3.9114739894866943, 'learning_rate': 2.380340291551306e-05, 'epoch': 1.72}\n",
      "{'loss': 1.9542, 'grad_norm': 10.001306533813477, 'learning_rate': 2.3658384011434145e-05, 'epoch': 1.74}\n",
      "{'loss': 1.935, 'grad_norm': 5.57964563369751, 'learning_rate': 2.351336510735523e-05, 'epoch': 1.76}\n",
      "{'eval_loss': 2.0645835399627686, 'eval_runtime': 110.2356, 'eval_samples_per_second': 90.715, 'eval_steps_per_second': 45.357, 'epoch': 1.76}\n",
      "{'loss': 1.9479, 'grad_norm': 7.318079948425293, 'learning_rate': 2.3368346203276317e-05, 'epoch': 1.78}\n",
      "{'loss': 1.9554, 'grad_norm': 4.24690055847168, 'learning_rate': 2.32233272991974e-05, 'epoch': 1.8}\n",
      "{'loss': 1.986, 'grad_norm': 4.060410022735596, 'learning_rate': 2.3078308395118485e-05, 'epoch': 1.8199999999999998}\n",
      "{'loss': 1.9417, 'grad_norm': 12.247482299804688, 'learning_rate': 2.293328949103957e-05, 'epoch': 1.8399999999999999}\n",
      "{'eval_loss': 2.0534534454345703, 'eval_runtime': 109.4487, 'eval_samples_per_second': 91.367, 'eval_steps_per_second': 45.684, 'epoch': 1.8399999999999999}\n",
      "{'loss': 1.9675, 'grad_norm': 5.168550968170166, 'learning_rate': 2.2788270586960657e-05, 'epoch': 1.8599999999999999}\n",
      "{'loss': 1.9946, 'grad_norm': 6.988112449645996, 'learning_rate': 2.264325168288174e-05, 'epoch': 1.88}\n",
      "{'loss': 1.9549, 'grad_norm': 5.170078277587891, 'learning_rate': 2.249823277880283e-05, 'epoch': 1.9}\n",
      "{'loss': 1.9653, 'grad_norm': 4.364899635314941, 'learning_rate': 2.2353213874723915e-05, 'epoch': 1.92}\n",
      "{'eval_loss': 2.046395778656006, 'eval_runtime': 109.8595, 'eval_samples_per_second': 91.025, 'eval_steps_per_second': 45.513, 'epoch': 1.92}\n",
      "{'loss': 1.9282, 'grad_norm': 4.8381547927856445, 'learning_rate': 2.2208194970644997e-05, 'epoch': 1.94}\n",
      "{'loss': 1.9238, 'grad_norm': 9.500121116638184, 'learning_rate': 2.2063176066566083e-05, 'epoch': 1.96}\n",
      "{'loss': 1.9256, 'grad_norm': 12.588985443115234, 'learning_rate': 2.191815716248717e-05, 'epoch': 1.98}\n",
      "{'loss': 1.937, 'grad_norm': 7.3833699226379395, 'learning_rate': 2.1773138258408255e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 2.0686347484588623, 'eval_runtime': 109.8853, 'eval_samples_per_second': 91.004, 'eval_steps_per_second': 45.502, 'epoch': 2.0}\n",
      "{'loss': 1.7955, 'grad_norm': 4.494232177734375, 'learning_rate': 2.1628119354329338e-05, 'epoch': 2.02}\n",
      "{'loss': 1.7552, 'grad_norm': 4.795266151428223, 'learning_rate': 2.1483100450250427e-05, 'epoch': 2.04}\n",
      "{'loss': 1.7231, 'grad_norm': 5.074764251708984, 'learning_rate': 2.133808154617151e-05, 'epoch': 2.06}\n",
      "{'loss': 1.7543, 'grad_norm': 3.8528687953948975, 'learning_rate': 2.1193062642092595e-05, 'epoch': 2.08}\n",
      "{'eval_loss': 2.076946973800659, 'eval_runtime': 109.7538, 'eval_samples_per_second': 91.113, 'eval_steps_per_second': 45.557, 'epoch': 2.08}\n",
      "{'loss': 1.7589, 'grad_norm': 4.513274669647217, 'learning_rate': 2.1048043738013678e-05, 'epoch': 2.1}\n",
      "{'loss': 1.7484, 'grad_norm': 4.350643634796143, 'learning_rate': 2.0903024833934767e-05, 'epoch': 2.12}\n",
      "{'loss': 1.7747, 'grad_norm': 4.508938789367676, 'learning_rate': 2.0758005929855853e-05, 'epoch': 2.14}\n",
      "{'loss': 1.762, 'grad_norm': 3.965041399002075, 'learning_rate': 2.0612987025776936e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 07:32:22,962 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.060640811920166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.060640811920166, 'eval_runtime': 110.0397, 'eval_samples_per_second': 90.876, 'eval_steps_per_second': 45.438, 'epoch': 2.16}\n",
      "{'train_runtime': 13624.1323, 'train_samples_per_second': 29.36, 'train_steps_per_second': 0.917, 'train_loss': 2.103348623205114, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 07:34:36,636 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.059054374694824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.059054374694824, 'eval_runtime': 132.4042, 'eval_samples_per_second': 75.526, 'eval_steps_per_second': 37.763, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 07:34:36,986 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-23 07:34:37,307] Trial 5 pruned. \n",
      "2025-07-23 07:34:38,467 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 07:34:38,487 - INFO - Trial 6 parameters: learning_rate=2.658616083788978e-05, weight_decay=0.014808945119975192, warmup_steps=100, lr_scheduler_type=cosine\n",
      "2025-07-23 07:34:38,487 - INFO - VRAM usage before training (trial 6): 1.53GB / 7.96GB\n",
      "2025-07-23 07:34:38,537 - INFO - Starting new training for trial 6\n",
      "2025-07-23 07:34:49,009 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7117, 'grad_norm': 7.753420352935791, 'learning_rate': 1.1963772377050402e-05, 'epoch': 0.02}\n",
      "{'loss': 2.5561, 'grad_norm': 6.3083343505859375, 'learning_rate': 2.525685279599529e-05, 'epoch': 0.04}\n",
      "{'loss': 2.4823, 'grad_norm': 6.894830703735352, 'learning_rate': 2.658529692026355e-05, 'epoch': 0.06}\n",
      "{'loss': 2.4849, 'grad_norm': 7.655059814453125, 'learning_rate': 2.6582391307993265e-05, 'epoch': 0.08}\n",
      "{'eval_loss': 2.296781301498413, 'eval_runtime': 109.8006, 'eval_samples_per_second': 91.074, 'eval_steps_per_second': 45.537, 'epoch': 0.08}\n",
      "{'loss': 2.4665, 'grad_norm': 12.592074394226074, 'learning_rate': 2.657731520677366e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4071, 'grad_norm': 8.354461669921875, 'learning_rate': 2.657010740048031e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4092, 'grad_norm': 7.120022773742676, 'learning_rate': 2.6560769045741158e-05, 'epoch': 0.14}\n",
      "{'loss': 2.3885, 'grad_norm': 6.286619663238525, 'learning_rate': 2.6549301641070685e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.2560298442840576, 'eval_runtime': 109.5661, 'eval_samples_per_second': 91.269, 'eval_steps_per_second': 45.635, 'epoch': 0.16}\n",
      "{'loss': 2.3812, 'grad_norm': 5.849809646606445, 'learning_rate': 2.6535707026629442e-05, 'epoch': 0.18}\n",
      "{'loss': 2.3552, 'grad_norm': 10.636648178100586, 'learning_rate': 2.6519987383928737e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3247, 'grad_norm': 7.3558878898620605, 'learning_rate': 2.650214523548062e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3061, 'grad_norm': 4.452761173248291, 'learning_rate': 2.6482183444393036e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.2138853073120117, 'eval_runtime': 109.6368, 'eval_samples_per_second': 91.21, 'eval_steps_per_second': 45.605, 'epoch': 0.24}\n",
      "{'loss': 2.4015, 'grad_norm': 5.0319390296936035, 'learning_rate': 2.6460105213910444e-05, 'epoch': 0.26}\n",
      "{'loss': 2.237, 'grad_norm': 5.256190776824951, 'learning_rate': 2.6435914086899757e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2718, 'grad_norm': 9.91417407989502, 'learning_rate': 2.6409613945281835e-05, 'epoch': 0.3}\n",
      "{'loss': 2.353, 'grad_norm': 7.1667399406433105, 'learning_rate': 2.638120900940857e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.210125207901001, 'eval_runtime': 108.8509, 'eval_samples_per_second': 91.869, 'eval_steps_per_second': 45.934, 'epoch': 0.32}\n",
      "{'loss': 2.27, 'grad_norm': 4.520901203155518, 'learning_rate': 2.635070383738563e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3396, 'grad_norm': 9.454926490783691, 'learning_rate': 2.6318103324341038e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3017, 'grad_norm': 5.031344890594482, 'learning_rate': 2.6283412701639656e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2431, 'grad_norm': 4.73114013671875, 'learning_rate': 2.6246637536043708e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.1678848266601562, 'eval_runtime': 109.5489, 'eval_samples_per_second': 91.283, 'eval_steps_per_second': 45.642, 'epoch': 0.4}\n",
      "{'loss': 2.2274, 'grad_norm': 6.341284275054932, 'learning_rate': 2.6207783728819495e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2216, 'grad_norm': 8.391378402709961, 'learning_rate': 2.6166857514790424e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2846, 'grad_norm': 4.681857585906982, 'learning_rate': 2.612386546133651e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2551, 'grad_norm': 5.27730655670166, 'learning_rate': 2.6078814467340532e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.17028546333313, 'eval_runtime': 109.0057, 'eval_samples_per_second': 91.738, 'eval_steps_per_second': 45.869, 'epoch': 0.48}\n",
      "{'loss': 2.2589, 'grad_norm': 5.127031326293945, 'learning_rate': 2.6031711762080947e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2826, 'grad_norm': 6.919519901275635, 'learning_rate': 2.5982564904071843e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2455, 'grad_norm': 15.598259925842285, 'learning_rate': 2.593138177985003e-05, 'epoch': 0.54}\n",
      "{'loss': 2.3549, 'grad_norm': 7.892697811126709, 'learning_rate': 2.5879254646477794e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.2558984756469727, 'eval_runtime': 110.1227, 'eval_samples_per_second': 90.808, 'eval_steps_per_second': 45.404, 'epoch': 0.56}\n",
      "{'loss': 2.4584, 'grad_norm': 30.92559814453125, 'learning_rate': 2.5824064259657425e-05, 'epoch': 0.58}\n",
      "{'loss': 2.3647, 'grad_norm': 9.364143371582031, 'learning_rate': 2.576686304103086e-05, 'epoch': 0.6}\n",
      "{'loss': 2.2698, 'grad_norm': 9.022025108337402, 'learning_rate': 2.57076601696084e-05, 'epoch': 0.62}\n",
      "{'loss': 2.3451, 'grad_norm': 36.47928237915039, 'learning_rate': 2.564646514560315e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 08:41:50,807 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.270174503326416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.270174503326416, 'eval_runtime': 109.7792, 'eval_samples_per_second': 91.092, 'eval_steps_per_second': 45.546, 'epoch': 0.64}\n",
      "{'train_runtime': 4021.7987, 'train_samples_per_second': 99.458, 'train_steps_per_second': 3.108, 'train_loss': 2.35186598777771, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 08:45:17,880 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.2632651329040527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2632651329040527, 'eval_runtime': 205.8023, 'eval_samples_per_second': 48.59, 'eval_steps_per_second': 24.295, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 08:45:18,220 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-23 08:45:18,540] Trial 6 pruned. \n",
      "2025-07-23 08:45:19,740 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 08:45:19,760 - INFO - Trial 7 parameters: learning_rate=3.67320780046205e-05, weight_decay=0.020165721691808594, warmup_steps=100, lr_scheduler_type=linear\n",
      "2025-07-23 08:45:19,760 - INFO - VRAM usage before training (trial 7): 1.53GB / 7.96GB\n",
      "2025-07-23 08:45:19,810 - INFO - Starting new training for trial 7\n",
      "2025-07-23 08:45:30,221 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6955, 'grad_norm': 8.338322639465332, 'learning_rate': 1.6529435102079224e-05, 'epoch': 0.02}\n",
      "{'loss': 2.5488, 'grad_norm': 7.6975998878479, 'learning_rate': 3.489547410438947e-05, 'epoch': 0.04}\n",
      "{'loss': 2.508, 'grad_norm': 103.20072937011719, 'learning_rate': 3.66017383729912e-05, 'epoch': 0.06}\n",
      "{'loss': 2.6326, 'grad_norm': 30.514934539794922, 'learning_rate': 3.6453625155230634e-05, 'epoch': 0.08}\n",
      "{'eval_loss': 2.4320242404937744, 'eval_runtime': 109.7171, 'eval_samples_per_second': 91.144, 'eval_steps_per_second': 45.572, 'epoch': 0.08}\n",
      "{'loss': 2.539, 'grad_norm': 26.09661293029785, 'learning_rate': 3.630551193747007e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4611, 'grad_norm': 21.795413970947266, 'learning_rate': 3.61573987197095e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4481, 'grad_norm': 7.859288215637207, 'learning_rate': 3.6012247766304144e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4409, 'grad_norm': 5.966324806213379, 'learning_rate': 3.586413454854357e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.3034112453460693, 'eval_runtime': 109.8308, 'eval_samples_per_second': 91.049, 'eval_steps_per_second': 45.525, 'epoch': 0.16}\n",
      "{'loss': 2.4383, 'grad_norm': 6.519250869750977, 'learning_rate': 3.571602133078301e-05, 'epoch': 0.18}\n",
      "{'loss': 2.405, 'grad_norm': 5.761479377746582, 'learning_rate': 3.556790811302244e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3694, 'grad_norm': 6.102183818817139, 'learning_rate': 3.541979489526188e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3517, 'grad_norm': 4.303834438323975, 'learning_rate': 3.527168167750131e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.264108180999756, 'eval_runtime': 109.8019, 'eval_samples_per_second': 91.073, 'eval_steps_per_second': 45.537, 'epoch': 0.24}\n",
      "{'loss': 2.4349, 'grad_norm': 5.036109447479248, 'learning_rate': 3.512356845974074e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2704, 'grad_norm': 5.926336765289307, 'learning_rate': 3.497545524198018e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3047, 'grad_norm': 6.622907638549805, 'learning_rate': 3.482734202421961e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3739, 'grad_norm': 5.056336879730225, 'learning_rate': 3.467922880645905e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.233736991882324, 'eval_runtime': 109.9, 'eval_samples_per_second': 90.992, 'eval_steps_per_second': 45.496, 'epoch': 0.32}\n",
      "{'loss': 2.2737, 'grad_norm': 4.468966484069824, 'learning_rate': 3.4531115588698476e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3592, 'grad_norm': 11.973714828491211, 'learning_rate': 3.438300237093791e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3158, 'grad_norm': 5.2349371910095215, 'learning_rate': 3.4234889153177346e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2568, 'grad_norm': 4.3758864402771, 'learning_rate': 3.408677593541678e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.1838583946228027, 'eval_runtime': 109.3676, 'eval_samples_per_second': 91.435, 'eval_steps_per_second': 45.717, 'epoch': 0.4}\n",
      "{'loss': 2.2469, 'grad_norm': 5.84393835067749, 'learning_rate': 3.393866271765621e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2168, 'grad_norm': 10.299115180969238, 'learning_rate': 3.3790549499895645e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2795, 'grad_norm': 4.55794095993042, 'learning_rate': 3.364243628213508e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2524, 'grad_norm': 5.289872169494629, 'learning_rate': 3.3494323064374516e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.165757894515991, 'eval_runtime': 109.8687, 'eval_samples_per_second': 91.018, 'eval_steps_per_second': 45.509, 'epoch': 0.48}\n",
      "{'loss': 2.2498, 'grad_norm': 4.585819244384766, 'learning_rate': 3.334917211096916e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2739, 'grad_norm': 5.460033893585205, 'learning_rate': 3.320105889320859e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2258, 'grad_norm': 5.591747760772705, 'learning_rate': 3.3052945675448026e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2825, 'grad_norm': 4.901122093200684, 'learning_rate': 3.2904832457687454e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1552653312683105, 'eval_runtime': 109.5085, 'eval_samples_per_second': 91.317, 'eval_steps_per_second': 45.659, 'epoch': 0.56}\n",
      "{'loss': 2.2595, 'grad_norm': 10.511123657226562, 'learning_rate': 3.275671923992689e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2311, 'grad_norm': 4.847690582275391, 'learning_rate': 3.2608606022166325e-05, 'epoch': 0.6}\n",
      "{'loss': 2.1949, 'grad_norm': 7.568512439727783, 'learning_rate': 3.246049280440576e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2844, 'grad_norm': 25.785863876342773, 'learning_rate': 3.231237958664519e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.169166326522827, 'eval_runtime': 109.4282, 'eval_samples_per_second': 91.384, 'eval_steps_per_second': 45.692, 'epoch': 0.64}\n",
      "{'loss': 2.2139, 'grad_norm': 9.574191093444824, 'learning_rate': 3.2164266368884624e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2521, 'grad_norm': 5.19830322265625, 'learning_rate': 3.201615315112406e-05, 'epoch': 0.68}\n",
      "{'loss': 2.2031, 'grad_norm': 3.716928482055664, 'learning_rate': 3.1868039933363494e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2203, 'grad_norm': 8.621885299682617, 'learning_rate': 3.171992671560292e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.1460819244384766, 'eval_runtime': 109.6894, 'eval_samples_per_second': 91.167, 'eval_steps_per_second': 45.583, 'epoch': 0.72}\n",
      "{'loss': 2.2243, 'grad_norm': 11.79092788696289, 'learning_rate': 3.157181349784236e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2369, 'grad_norm': 6.891149044036865, 'learning_rate': 3.1423700280081793e-05, 'epoch': 0.76}\n",
      "{'loss': 2.249, 'grad_norm': 4.19635534286499, 'learning_rate': 3.127558706232123e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2503, 'grad_norm': 7.701475143432617, 'learning_rate': 3.112747384456066e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.137174606323242, 'eval_runtime': 110.1556, 'eval_samples_per_second': 90.781, 'eval_steps_per_second': 45.39, 'epoch': 0.8}\n",
      "{'loss': 2.2091, 'grad_norm': 4.096388816833496, 'learning_rate': 3.097936062680009e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2213, 'grad_norm': 16.257640838623047, 'learning_rate': 3.083124740903953e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1242, 'grad_norm': 33.019466400146484, 'learning_rate': 3.068313419127896e-05, 'epoch': 0.86}\n",
      "{'loss': 2.202, 'grad_norm': 13.69804573059082, 'learning_rate': 3.053502097351839e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1178393363952637, 'eval_runtime': 110.6411, 'eval_samples_per_second': 90.382, 'eval_steps_per_second': 45.191, 'epoch': 0.88}\n",
      "{'loss': 2.209, 'grad_norm': 13.663969993591309, 'learning_rate': 3.0386907755757827e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1555, 'grad_norm': 7.5517354011535645, 'learning_rate': 3.0238794537997262e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2115, 'grad_norm': 9.020533561706543, 'learning_rate': 3.0090681320236694e-05, 'epoch': 0.94}\n",
      "{'loss': 2.161, 'grad_norm': 4.790965557098389, 'learning_rate': 2.994256810247613e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.168057918548584, 'eval_runtime': 109.769, 'eval_samples_per_second': 91.1, 'eval_steps_per_second': 45.55, 'epoch': 0.96}\n",
      "{'loss': 2.1962, 'grad_norm': 4.9997429847717285, 'learning_rate': 2.979445488471556e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2208, 'grad_norm': 5.3549370765686035, 'learning_rate': 2.9646341666954996e-05, 'epoch': 1.0}\n",
      "{'loss': 1.9522, 'grad_norm': 4.48359489440918, 'learning_rate': 2.9498228449194428e-05, 'epoch': 1.02}\n",
      "{'loss': 2.0365, 'grad_norm': 8.024306297302246, 'learning_rate': 2.9350115231433863e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.112964153289795, 'eval_runtime': 110.1242, 'eval_samples_per_second': 90.807, 'eval_steps_per_second': 45.403, 'epoch': 1.04}\n",
      "{'loss': 1.9968, 'grad_norm': 4.316217422485352, 'learning_rate': 2.9202002013673295e-05, 'epoch': 1.06}\n",
      "{'loss': 1.9925, 'grad_norm': 6.739846229553223, 'learning_rate': 2.905388879591273e-05, 'epoch': 1.08}\n",
      "{'loss': 1.9995, 'grad_norm': 5.084281921386719, 'learning_rate': 2.8905775578152162e-05, 'epoch': 1.1}\n",
      "{'loss': 1.9826, 'grad_norm': 10.021835327148438, 'learning_rate': 2.8757662360391598e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 10:41:49,729 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.11639404296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.11639404296875, 'eval_runtime': 110.2734, 'eval_samples_per_second': 90.684, 'eval_steps_per_second': 45.342, 'epoch': 1.12}\n",
      "{'train_runtime': 6979.5075, 'train_samples_per_second': 57.311, 'train_steps_per_second': 1.791, 'train_loss': 2.2704624230521064, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 10:44:04,197 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.118257761001587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.118257761001587, 'eval_runtime': 133.171, 'eval_samples_per_second': 75.091, 'eval_steps_per_second': 37.546, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 10:44:04,547 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-23 10:44:04,845] Trial 7 pruned. \n",
      "2025-07-23 10:44:06,075 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 10:44:06,104 - INFO - Trial 8 parameters: learning_rate=1.2170293883738781e-05, weight_decay=0.03127353036780371, warmup_steps=100, lr_scheduler_type=linear\n",
      "2025-07-23 10:44:06,104 - INFO - VRAM usage before training (trial 8): 1.53GB / 7.96GB\n",
      "2025-07-23 10:44:06,162 - INFO - Starting new training for trial 8\n",
      "2025-07-23 10:44:17,034 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7523, 'grad_norm': 7.356632709503174, 'learning_rate': 5.476632247682452e-06, 'epoch': 0.02}\n",
      "{'loss': 2.5992, 'grad_norm': 6.268283367156982, 'learning_rate': 1.1561779189551843e-05, 'epoch': 0.04}\n",
      "{'loss': 2.4985, 'grad_norm': 5.718626976013184, 'learning_rate': 1.2126127494644567e-05, 'epoch': 0.06}\n",
      "{'loss': 2.5066, 'grad_norm': 5.504232883453369, 'learning_rate': 1.2078035204297536e-05, 'epoch': 0.08}\n",
      "{'eval_loss': 2.3337972164154053, 'eval_runtime': 109.6069, 'eval_samples_per_second': 91.235, 'eval_steps_per_second': 45.618, 'epoch': 0.08}\n",
      "{'loss': 2.4791, 'grad_norm': 11.100144386291504, 'learning_rate': 1.2028961438637299e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4267, 'grad_norm': 7.685903072357178, 'learning_rate': 1.1979887672977061e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4212, 'grad_norm': 11.275552749633789, 'learning_rate': 1.1930813907316826e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4013, 'grad_norm': 5.556790828704834, 'learning_rate': 1.1881740141656589e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.254991292953491, 'eval_runtime': 109.9896, 'eval_samples_per_second': 90.918, 'eval_steps_per_second': 45.459, 'epoch': 0.16}\n",
      "{'loss': 2.3985, 'grad_norm': 5.909174919128418, 'learning_rate': 1.1832666375996351e-05, 'epoch': 0.18}\n",
      "{'loss': 2.3769, 'grad_norm': 10.125983238220215, 'learning_rate': 1.1783592610336114e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3375, 'grad_norm': 9.3129243850708, 'learning_rate': 1.1734518844675877e-05, 'epoch': 0.22}\n",
      "{'loss': 2.321, 'grad_norm': 5.198636531829834, 'learning_rate': 1.168544507901564e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.2205498218536377, 'eval_runtime': 109.3493, 'eval_samples_per_second': 91.45, 'eval_steps_per_second': 45.725, 'epoch': 0.24}\n",
      "{'loss': 2.4127, 'grad_norm': 5.137270927429199, 'learning_rate': 1.1636371313355404e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2577, 'grad_norm': 4.658564567565918, 'learning_rate': 1.158827902300837e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2956, 'grad_norm': 5.270770072937012, 'learning_rate': 1.1539205257348133e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3731, 'grad_norm': 6.7872419357299805, 'learning_rate': 1.1490131491687896e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.1903793811798096, 'eval_runtime': 108.8999, 'eval_samples_per_second': 91.827, 'eval_steps_per_second': 45.914, 'epoch': 0.32}\n",
      "{'loss': 2.2762, 'grad_norm': 6.6248698234558105, 'learning_rate': 1.1441057726027659e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3606, 'grad_norm': 8.157780647277832, 'learning_rate': 1.1391983960367421e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3121, 'grad_norm': 5.853687763214111, 'learning_rate': 1.1342910194707186e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2545, 'grad_norm': 6.11503267288208, 'learning_rate': 1.1293836429046949e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.172450304031372, 'eval_runtime': 109.5297, 'eval_samples_per_second': 91.299, 'eval_steps_per_second': 45.65, 'epoch': 0.4}\n",
      "{'loss': 2.2404, 'grad_norm': 6.405015468597412, 'learning_rate': 1.1244762663386711e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2241, 'grad_norm': 15.213547706604004, 'learning_rate': 1.1195688897726474e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2824, 'grad_norm': 5.116330146789551, 'learning_rate': 1.1146615132066237e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2548, 'grad_norm': 5.950965881347656, 'learning_rate': 1.1097541366406001e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.1565983295440674, 'eval_runtime': 109.3674, 'eval_samples_per_second': 91.435, 'eval_steps_per_second': 45.717, 'epoch': 0.48}\n",
      "{'loss': 2.2579, 'grad_norm': 5.74659538269043, 'learning_rate': 1.1048467600745764e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2753, 'grad_norm': 7.9512434005737305, 'learning_rate': 1.0999393835085526e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2373, 'grad_norm': 7.212625980377197, 'learning_rate': 1.095032006942529e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2905, 'grad_norm': 5.477686405181885, 'learning_rate': 1.0901246303765052e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1634604930877686, 'eval_runtime': 109.0115, 'eval_samples_per_second': 91.733, 'eval_steps_per_second': 45.867, 'epoch': 0.56}\n",
      "{'loss': 2.27, 'grad_norm': 4.9194488525390625, 'learning_rate': 1.0852172538104815e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2399, 'grad_norm': 12.614535331726074, 'learning_rate': 1.0803098772444579e-05, 'epoch': 0.6}\n",
      "{'loss': 2.1982, 'grad_norm': 5.156961441040039, 'learning_rate': 1.0754025006784342e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2886, 'grad_norm': 4.464747905731201, 'learning_rate': 1.0704951241124104e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.13669753074646, 'eval_runtime': 109.2202, 'eval_samples_per_second': 91.558, 'eval_steps_per_second': 45.779, 'epoch': 0.64}\n",
      "{'loss': 2.1963, 'grad_norm': 5.933489799499512, 'learning_rate': 1.0655877475463867e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2449, 'grad_norm': 5.767060279846191, 'learning_rate': 1.060680370980363e-05, 'epoch': 0.68}\n",
      "{'loss': 2.2026, 'grad_norm': 4.385237216949463, 'learning_rate': 1.0557729944143394e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2219, 'grad_norm': 8.042745590209961, 'learning_rate': 1.0508656178483157e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.1270031929016113, 'eval_runtime': 108.7498, 'eval_samples_per_second': 91.954, 'eval_steps_per_second': 45.977, 'epoch': 0.72}\n",
      "{'loss': 2.2154, 'grad_norm': 5.07328987121582, 'learning_rate': 1.0459582412822918e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2133, 'grad_norm': 5.977156162261963, 'learning_rate': 1.041050864716268e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2484, 'grad_norm': 4.675126552581787, 'learning_rate': 1.0361434881502445e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2492, 'grad_norm': 4.569653034210205, 'learning_rate': 1.0312361115842208e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1416683197021484, 'eval_runtime': 109.3462, 'eval_samples_per_second': 91.453, 'eval_steps_per_second': 45.726, 'epoch': 0.8}\n",
      "{'loss': 2.2086, 'grad_norm': 5.013563632965088, 'learning_rate': 1.026328735018197e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2241, 'grad_norm': 7.836714744567871, 'learning_rate': 1.0214213584521733e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1329, 'grad_norm': 6.268350601196289, 'learning_rate': 1.0165139818861496e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2157, 'grad_norm': 4.801375865936279, 'learning_rate': 1.011606605320126e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1034903526306152, 'eval_runtime': 109.3521, 'eval_samples_per_second': 91.448, 'eval_steps_per_second': 45.724, 'epoch': 0.88}\n",
      "{'loss': 2.2158, 'grad_norm': 6.477152347564697, 'learning_rate': 1.0066992287541023e-05, 'epoch': 0.9}\n",
      "{'loss': 2.169, 'grad_norm': 6.623560428619385, 'learning_rate': 1.0017918521880786e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2203, 'grad_norm': 6.860359191894531, 'learning_rate': 9.968844756220548e-06, 'epoch': 0.94}\n",
      "{'loss': 2.1646, 'grad_norm': 4.709562301635742, 'learning_rate': 9.919770990560311e-06, 'epoch': 0.96}\n",
      "{'eval_loss': 2.1013684272766113, 'eval_runtime': 109.5699, 'eval_samples_per_second': 91.266, 'eval_steps_per_second': 45.633, 'epoch': 0.96}\n",
      "{'loss': 2.203, 'grad_norm': 6.203965187072754, 'learning_rate': 9.870697224900074e-06, 'epoch': 0.98}\n",
      "{'loss': 2.2291, 'grad_norm': 7.96435546875, 'learning_rate': 9.821623459239838e-06, 'epoch': 1.0}\n",
      "{'loss': 2.0498, 'grad_norm': 6.803384780883789, 'learning_rate': 9.7725496935796e-06, 'epoch': 1.02}\n",
      "{'loss': 2.1447, 'grad_norm': 9.156281471252441, 'learning_rate': 9.723475927919363e-06, 'epoch': 1.04}\n",
      "{'eval_loss': 2.104670763015747, 'eval_runtime': 108.9721, 'eval_samples_per_second': 91.767, 'eval_steps_per_second': 45.883, 'epoch': 1.04}\n",
      "{'loss': 2.107, 'grad_norm': 6.509189128875732, 'learning_rate': 9.674402162259126e-06, 'epoch': 1.06}\n",
      "{'loss': 2.0959, 'grad_norm': 11.887057304382324, 'learning_rate': 9.625328396598889e-06, 'epoch': 1.08}\n",
      "{'loss': 2.1068, 'grad_norm': 12.896177291870117, 'learning_rate': 9.576254630938653e-06, 'epoch': 1.1}\n",
      "{'loss': 2.0948, 'grad_norm': 8.332576751708984, 'learning_rate': 9.527180865278416e-06, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 12:44:40,977 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.095647096633911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.095647096633911, 'eval_runtime': 109.4003, 'eval_samples_per_second': 91.407, 'eval_steps_per_second': 45.704, 'epoch': 1.12}\n",
      "{'train_runtime': 7223.9438, 'train_samples_per_second': 55.371, 'train_steps_per_second': 1.73, 'train_loss': 2.276690583910261, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 12:46:57,240 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.0971078872680664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0971078872680664, 'eval_runtime': 134.9924, 'eval_samples_per_second': 74.078, 'eval_steps_per_second': 37.039, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 12:46:57,590 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-23 12:46:58,010] Trial 8 finished with value: 2.0971078872680664 and parameters: {'learning_rate': 1.2170293883738781e-05, 'weight_decay': 0.03127353036780371, 'warmup_steps': 100, 'lr_scheduler_type': 'linear'}. Best is trial 1 with value: 2.0840227603912354.\n",
      "2025-07-23 12:46:59,200 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 12:46:59,220 - INFO - Trial 9 parameters: learning_rate=2.9045790726652738e-05, weight_decay=0.020497980520950188, warmup_steps=300, lr_scheduler_type=linear\n",
      "2025-07-23 12:46:59,220 - INFO - VRAM usage before training (trial 9): 1.53GB / 7.96GB\n",
      "2025-07-23 12:46:59,270 - INFO - Starting new training for trial 9\n",
      "2025-07-23 12:47:09,690 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7638, 'grad_norm': 7.634951114654541, 'learning_rate': 4.356868608997911e-06, 'epoch': 0.02}\n",
      "{'loss': 2.6142, 'grad_norm': 6.317503929138184, 'learning_rate': 9.1978337301067e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5087, 'grad_norm': 5.960926055908203, 'learning_rate': 1.403879885121549e-05, 'epoch': 0.06}\n",
      "{'loss': 2.5146, 'grad_norm': 5.8559489250183105, 'learning_rate': 1.887976397232428e-05, 'epoch': 0.08}\n",
      "{'eval_loss': 2.323354959487915, 'eval_runtime': 108.8195, 'eval_samples_per_second': 91.895, 'eval_steps_per_second': 45.948, 'epoch': 0.08}\n",
      "{'loss': 2.4934, 'grad_norm': 8.447162628173828, 'learning_rate': 2.3720729093433068e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4375, 'grad_norm': 6.878093242645264, 'learning_rate': 2.8561694214541856e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4361, 'grad_norm': 7.386343002319336, 'learning_rate': 2.8938654613316725e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4112, 'grad_norm': 5.265451431274414, 'learning_rate': 2.881961448738782e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.2530264854431152, 'eval_runtime': 108.9223, 'eval_samples_per_second': 91.809, 'eval_steps_per_second': 45.904, 'epoch': 0.16}\n",
      "{'loss': 2.4072, 'grad_norm': 5.751410484313965, 'learning_rate': 2.8700574361458916e-05, 'epoch': 0.18}\n",
      "{'loss': 2.3773, 'grad_norm': 9.748391151428223, 'learning_rate': 2.858153423553001e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3379, 'grad_norm': 6.518992900848389, 'learning_rate': 2.8462494109601107e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3197, 'grad_norm': 4.816561698913574, 'learning_rate': 2.83434539836722e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.222109317779541, 'eval_runtime': 108.8928, 'eval_samples_per_second': 91.833, 'eval_steps_per_second': 45.917, 'epoch': 0.24}\n",
      "{'loss': 2.4077, 'grad_norm': 5.003452777862549, 'learning_rate': 2.8224413857743295e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2545, 'grad_norm': 7.36928653717041, 'learning_rate': 2.810537373181439e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2835, 'grad_norm': 4.746506214141846, 'learning_rate': 2.7986333605885486e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3636, 'grad_norm': 8.979955673217773, 'learning_rate': 2.786729347995658e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.1984739303588867, 'eval_runtime': 109.3103, 'eval_samples_per_second': 91.483, 'eval_steps_per_second': 45.741, 'epoch': 0.32}\n",
      "{'loss': 2.2631, 'grad_norm': 4.9359331130981445, 'learning_rate': 2.7748253354027678e-05, 'epoch': 0.34}\n",
      "{'loss': 2.345, 'grad_norm': 4.945918083190918, 'learning_rate': 2.7629213228098775e-05, 'epoch': 0.36}\n",
      "{'loss': 2.306, 'grad_norm': 5.7899041175842285, 'learning_rate': 2.751017310216987e-05, 'epoch': 0.38}\n",
      "{'loss': 2.248, 'grad_norm': 4.402830600738525, 'learning_rate': 2.7391132976240966e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.172959327697754, 'eval_runtime': 108.9583, 'eval_samples_per_second': 91.778, 'eval_steps_per_second': 45.889, 'epoch': 0.4}\n",
      "{'loss': 2.2411, 'grad_norm': 13.924452781677246, 'learning_rate': 2.7272092850312057e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2144, 'grad_norm': 8.003327369689941, 'learning_rate': 2.7153052724383154e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2731, 'grad_norm': 4.384853839874268, 'learning_rate': 2.7034012598454248e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2447, 'grad_norm': 4.708034992218018, 'learning_rate': 2.6917353275043923e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.152892589569092, 'eval_runtime': 109.1643, 'eval_samples_per_second': 91.605, 'eval_steps_per_second': 45.803, 'epoch': 0.48}\n",
      "{'loss': 2.2464, 'grad_norm': 6.941710948944092, 'learning_rate': 2.679831314911502e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2661, 'grad_norm': 4.2465901374816895, 'learning_rate': 2.6679273023186114e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2258, 'grad_norm': 17.1278133392334, 'learning_rate': 2.6560232897257208e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2755, 'grad_norm': 4.790701389312744, 'learning_rate': 2.6441192771328302e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1392009258270264, 'eval_runtime': 108.8537, 'eval_samples_per_second': 91.866, 'eval_steps_per_second': 45.933, 'epoch': 0.56}\n",
      "{'loss': 2.2542, 'grad_norm': 5.304348468780518, 'learning_rate': 2.63221526453994e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2278, 'grad_norm': 4.864871501922607, 'learning_rate': 2.6203112519470493e-05, 'epoch': 0.6}\n",
      "{'loss': 2.2224, 'grad_norm': 4.588733196258545, 'learning_rate': 2.608407239354159e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2912, 'grad_norm': 19.361772537231445, 'learning_rate': 2.5965032267612688e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.1456611156463623, 'eval_runtime': 108.8474, 'eval_samples_per_second': 91.872, 'eval_steps_per_second': 45.936, 'epoch': 0.64}\n",
      "{'loss': 2.1854, 'grad_norm': 4.566535949707031, 'learning_rate': 2.584599214168378e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2321, 'grad_norm': 5.758570671081543, 'learning_rate': 2.572695201575488e-05, 'epoch': 0.68}\n",
      "{'loss': 2.1909, 'grad_norm': 6.831507682800293, 'learning_rate': 2.5607911889825973e-05, 'epoch': 0.7}\n",
      "{'loss': 2.209, 'grad_norm': 4.811925411224365, 'learning_rate': 2.5488871763897067e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.198228597640991, 'eval_runtime': 110.2472, 'eval_samples_per_second': 90.705, 'eval_steps_per_second': 45.353, 'epoch': 0.72}\n",
      "{'loss': 2.1994, 'grad_norm': 5.477832317352295, 'learning_rate': 2.536983163796816e-05, 'epoch': 0.74}\n",
      "{'loss': 2.1936, 'grad_norm': 8.070441246032715, 'learning_rate': 2.5250791512039258e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2301, 'grad_norm': 4.121774673461914, 'learning_rate': 2.513175138611035e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2326, 'grad_norm': 4.388975143432617, 'learning_rate': 2.501271126018145e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1135950088500977, 'eval_runtime': 110.4102, 'eval_samples_per_second': 90.571, 'eval_steps_per_second': 45.286, 'epoch': 0.8}\n",
      "{'loss': 2.1903, 'grad_norm': 4.507108688354492, 'learning_rate': 2.4893671134252543e-05, 'epoch': 0.82}\n",
      "{'loss': 2.211, 'grad_norm': 4.952201843261719, 'learning_rate': 2.477463100832364e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1126, 'grad_norm': 7.51054573059082, 'learning_rate': 2.4655590882394734e-05, 'epoch': 0.86}\n",
      "{'loss': 2.1978, 'grad_norm': 4.002133846282959, 'learning_rate': 2.4536550756465828e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.098674774169922, 'eval_runtime': 109.7063, 'eval_samples_per_second': 91.152, 'eval_steps_per_second': 45.576, 'epoch': 0.88}\n",
      "{'loss': 2.1937, 'grad_norm': 7.319103240966797, 'learning_rate': 2.4417510630536925e-05, 'epoch': 0.9}\n",
      "{'loss': 2.145, 'grad_norm': 6.14622688293457, 'learning_rate': 2.429847050460802e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2135, 'grad_norm': 3.9505786895751953, 'learning_rate': 2.4179430378679116e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1445, 'grad_norm': 3.943094253540039, 'learning_rate': 2.406039025275021e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.0956830978393555, 'eval_runtime': 109.6329, 'eval_samples_per_second': 91.213, 'eval_steps_per_second': 45.607, 'epoch': 0.96}\n",
      "{'loss': 2.1809, 'grad_norm': 5.34553861618042, 'learning_rate': 2.3941350126821307e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2041, 'grad_norm': 5.555008411407471, 'learning_rate': 2.38223100008924e-05, 'epoch': 1.0}\n",
      "{'loss': 1.9538, 'grad_norm': 5.020672798156738, 'learning_rate': 2.37032698749635e-05, 'epoch': 1.02}\n",
      "{'loss': 2.0418, 'grad_norm': 6.118808746337891, 'learning_rate': 2.3584229749034592e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.11572003364563, 'eval_runtime': 110.0465, 'eval_samples_per_second': 90.871, 'eval_steps_per_second': 45.435, 'epoch': 1.04}\n",
      "{'loss': 2.0059, 'grad_norm': 5.331401348114014, 'learning_rate': 2.3465189623105686e-05, 'epoch': 1.06}\n",
      "{'loss': 2.0014, 'grad_norm': 7.563430309295654, 'learning_rate': 2.334614949717678e-05, 'epoch': 1.08}\n",
      "{'loss': 2.0097, 'grad_norm': 8.927521705627441, 'learning_rate': 2.3227109371247878e-05, 'epoch': 1.1}\n",
      "{'loss': 1.9942, 'grad_norm': 7.112519264221191, 'learning_rate': 2.3108069245318975e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 14:43:58,211 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.1152210235595703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1152210235595703, 'eval_runtime': 110.3425, 'eval_samples_per_second': 90.627, 'eval_steps_per_second': 45.313, 'epoch': 1.12}\n",
      "{'train_runtime': 7008.5215, 'train_samples_per_second': 57.073, 'train_steps_per_second': 1.784, 'train_loss': 2.2598079817635672, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 14:46:12,376 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.116762399673462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.116762399673462, 'eval_runtime': 132.8909, 'eval_samples_per_second': 75.25, 'eval_steps_per_second': 37.625, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 14:46:12,723 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-23 14:46:13,097] Trial 9 pruned. \n",
      "2025-07-23 14:46:14,267 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 14:46:14,316 - INFO - Trial 10 parameters: learning_rate=1.777063571721834e-05, weight_decay=0.010139048090380883, warmup_steps=500, lr_scheduler_type=cosine\n",
      "2025-07-23 14:46:14,317 - INFO - VRAM usage before training (trial 10): 1.53GB / 7.96GB\n",
      "2025-07-23 14:46:14,376 - INFO - Starting new training for trial 10\n",
      "2025-07-23 14:46:24,949 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.805, 'grad_norm': 7.441847801208496, 'learning_rate': 1.5993572145496504e-06, 'epoch': 0.02}\n",
      "{'loss': 2.6883, 'grad_norm': 6.414226055145264, 'learning_rate': 3.3764207862714844e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5711, 'grad_norm': 5.537060737609863, 'learning_rate': 5.153484357993318e-06, 'epoch': 0.06}\n",
      "{'loss': 2.5739, 'grad_norm': 5.890913963317871, 'learning_rate': 6.8950066582807155e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 2.3837172985076904, 'eval_runtime': 114.0084, 'eval_samples_per_second': 87.713, 'eval_steps_per_second': 43.856, 'epoch': 0.08}\n",
      "{'loss': 2.5295, 'grad_norm': 9.38420295715332, 'learning_rate': 8.67207023000255e-06, 'epoch': 0.1}\n",
      "{'loss': 2.4724, 'grad_norm': 9.816500663757324, 'learning_rate': 1.0449133801724382e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4626, 'grad_norm': 10.634675025939941, 'learning_rate': 1.2226197373446217e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4386, 'grad_norm': 5.767257213592529, 'learning_rate': 1.4003260945168053e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.2836618423461914, 'eval_runtime': 113.9343, 'eval_samples_per_second': 87.77, 'eval_steps_per_second': 43.885, 'epoch': 0.16}\n",
      "{'loss': 2.4395, 'grad_norm': 6.005497455596924, 'learning_rate': 1.5780324516889887e-05, 'epoch': 0.18}\n",
      "{'loss': 2.4098, 'grad_norm': 5.421727180480957, 'learning_rate': 1.7557388088611718e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3704, 'grad_norm': 8.028831481933594, 'learning_rate': 1.7770046221334503e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3486, 'grad_norm': 5.722340106964111, 'learning_rate': 1.7767945334803697e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.2304012775421143, 'eval_runtime': 116.0128, 'eval_samples_per_second': 86.197, 'eval_steps_per_second': 43.099, 'epoch': 0.24}\n",
      "{'loss': 2.4321, 'grad_norm': 5.361494541168213, 'learning_rate': 1.7764322455782736e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2801, 'grad_norm': 4.690104007720947, 'learning_rate': 1.775917820503329e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3075, 'grad_norm': 4.927943229675293, 'learning_rate': 1.7752513463996245e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3846, 'grad_norm': 6.000975131988525, 'learning_rate': 1.774432937464066e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.199007272720337, 'eval_runtime': 119.2239, 'eval_samples_per_second': 83.876, 'eval_steps_per_second': 41.938, 'epoch': 0.32}\n",
      "{'loss': 2.2855, 'grad_norm': 5.480721950531006, 'learning_rate': 1.773462733926812e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3619, 'grad_norm': 5.104996681213379, 'learning_rate': 1.772340902027244e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3219, 'grad_norm': 8.70316219329834, 'learning_rate': 1.7710676339854827e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2644, 'grad_norm': 5.875063419342041, 'learning_rate': 1.7696431479694518e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.183689832687378, 'eval_runtime': 119.19, 'eval_samples_per_second': 83.9, 'eval_steps_per_second': 41.95, 'epoch': 0.4}\n",
      "{'loss': 2.2504, 'grad_norm': 10.011981964111328, 'learning_rate': 1.7680676880574977e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2278, 'grad_norm': 5.099201679229736, 'learning_rate': 1.7663415241965656e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2843, 'grad_norm': 4.747812747955322, 'learning_rate': 1.764464952155946e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2645, 'grad_norm': 8.254047393798828, 'learning_rate': 1.7624802952955417e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.1617000102996826, 'eval_runtime': 118.9396, 'eval_samples_per_second': 84.076, 'eval_steps_per_second': 42.038, 'epoch': 0.48}\n",
      "{'loss': 2.2594, 'grad_norm': 5.292422771453857, 'learning_rate': 1.7603068884534447e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2781, 'grad_norm': 5.735444068908691, 'learning_rate': 1.757984107435429e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2367, 'grad_norm': 20.622802734375, 'learning_rate': 1.7555123502380642e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2907, 'grad_norm': 5.566272258758545, 'learning_rate': 1.7528920403842218e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.142228603363037, 'eval_runtime': 118.7775, 'eval_samples_per_second': 84.191, 'eval_steps_per_second': 42.096, 'epoch': 0.56}\n",
      "{'loss': 2.2691, 'grad_norm': 7.661024570465088, 'learning_rate': 1.7501236268505053e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2425, 'grad_norm': 6.9664411544799805, 'learning_rate': 1.7472075839903224e-05, 'epoch': 0.6}\n",
      "{'loss': 2.1993, 'grad_norm': 7.195289134979248, 'learning_rate': 1.7441444114526047e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2866, 'grad_norm': 6.908767223358154, 'learning_rate': 1.7409346340961978e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.143855333328247, 'eval_runtime': 114.6887, 'eval_samples_per_second': 87.193, 'eval_steps_per_second': 43.596, 'epoch': 0.64}\n",
      "{'loss': 2.192, 'grad_norm': 4.755415916442871, 'learning_rate': 1.737578801899928e-05, 'epoch': 0.66}\n",
      "{'loss': 2.249, 'grad_norm': 5.62083625793457, 'learning_rate': 1.7340774898683655e-05, 'epoch': 0.68}\n",
      "{'loss': 2.1995, 'grad_norm': 4.0383710861206055, 'learning_rate': 1.7304312979333028e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2169, 'grad_norm': 6.003652095794678, 'learning_rate': 1.726640850850957e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.131293773651123, 'eval_runtime': 114.5521, 'eval_samples_per_second': 87.296, 'eval_steps_per_second': 43.648, 'epoch': 0.72}\n",
      "{'loss': 2.2081, 'grad_norm': 5.041559219360352, 'learning_rate': 1.722706798094922e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2105, 'grad_norm': 5.795551300048828, 'learning_rate': 1.718629813744886e-05, 'epoch': 0.76}\n",
      "{'loss': 2.244, 'grad_norm': 4.323760986328125, 'learning_rate': 1.7144105963711283e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2443, 'grad_norm': 5.840534687042236, 'learning_rate': 1.710049868914825e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1544415950775146, 'eval_runtime': 114.7488, 'eval_samples_per_second': 87.147, 'eval_steps_per_second': 43.573, 'epoch': 0.8}\n",
      "{'loss': 2.2039, 'grad_norm': 4.78949499130249, 'learning_rate': 1.7055483785641774e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2199, 'grad_norm': 7.896005630493164, 'learning_rate': 1.7009068966263828e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1259, 'grad_norm': 6.82505989074707, 'learning_rate': 1.6961262183954772e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2105, 'grad_norm': 4.846521377563477, 'learning_rate': 1.691207163016064e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1079447269439697, 'eval_runtime': 114.193, 'eval_samples_per_second': 87.571, 'eval_steps_per_second': 43.786, 'epoch': 0.88}\n",
      "{'loss': 2.2098, 'grad_norm': 5.034959316253662, 'learning_rate': 1.6861505733429598e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1577, 'grad_norm': 9.392492294311523, 'learning_rate': 1.6809573157967733e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2129, 'grad_norm': 4.447093963623047, 'learning_rate': 1.6756282802154497e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1546, 'grad_norm': 4.662688732147217, 'learning_rate': 1.6701643797018027e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.102996826171875, 'eval_runtime': 114.6252, 'eval_samples_per_second': 87.241, 'eval_steps_per_second': 43.62, 'epoch': 0.96}\n",
      "{'loss': 2.1969, 'grad_norm': 6.220912933349609, 'learning_rate': 1.664566550467056e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2205, 'grad_norm': 6.181173801422119, 'learning_rate': 1.6588357516704326e-05, 'epoch': 1.0}\n",
      "{'loss': 2.0131, 'grad_norm': 4.971529483795166, 'learning_rate': 1.652972965254803e-05, 'epoch': 1.02}\n",
      "{'loss': 2.1064, 'grad_norm': 5.443559646606445, 'learning_rate': 1.6469791957784384e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.103592872619629, 'eval_runtime': 114.8191, 'eval_samples_per_second': 87.094, 'eval_steps_per_second': 43.547, 'epoch': 1.04}\n",
      "{'loss': 2.0683, 'grad_norm': 6.23076057434082, 'learning_rate': 1.6408554702428822e-05, 'epoch': 1.06}\n",
      "{'loss': 2.0606, 'grad_norm': 11.302292823791504, 'learning_rate': 1.6346028379169802e-05, 'epoch': 1.08}\n",
      "{'loss': 2.0726, 'grad_norm': 15.272772789001465, 'learning_rate': 1.6282223701570923e-05, 'epoch': 1.1}\n",
      "{'loss': 2.0562, 'grad_norm': 6.716594696044922, 'learning_rate': 1.6217151602235232e-05, 'epoch': 1.12}\n",
      "{'eval_loss': 2.09787917137146, 'eval_runtime': 114.6734, 'eval_samples_per_second': 87.204, 'eval_steps_per_second': 43.602, 'epoch': 1.12}\n",
      "{'loss': 2.0631, 'grad_norm': 6.813826560974121, 'learning_rate': 1.6150823230931953e-05, 'epoch': 1.1400000000000001}\n",
      "{'loss': 2.0261, 'grad_norm': 6.215900421142578, 'learning_rate': 1.608324995268605e-05, 'epoch': 1.16}\n",
      "{'loss': 2.06, 'grad_norm': 7.122341632843018, 'learning_rate': 1.6014443345830885e-05, 'epoch': 1.18}\n",
      "{'loss': 2.0677, 'grad_norm': 9.37777042388916, 'learning_rate': 1.594441520002432e-05, 'epoch': 1.2}\n",
      "{'eval_loss': 2.0899643898010254, 'eval_runtime': 113.7082, 'eval_samples_per_second': 87.944, 'eval_steps_per_second': 43.972, 'epoch': 1.2}\n",
      "{'loss': 2.0346, 'grad_norm': 4.228667259216309, 'learning_rate': 1.5873177514228637e-05, 'epoch': 1.22}\n",
      "{'loss': 2.046, 'grad_norm': 3.884331703186035, 'learning_rate': 1.5800742494654564e-05, 'epoch': 1.24}\n",
      "{'loss': 2.0683, 'grad_norm': 4.507047653198242, 'learning_rate': 1.5727122552669805e-05, 'epoch': 1.26}\n",
      "{'loss': 2.0541, 'grad_norm': 10.87187385559082, 'learning_rate': 1.5652330302672426e-05, 'epoch': 1.28}\n",
      "{'eval_loss': 2.0895299911499023, 'eval_runtime': 113.5778, 'eval_samples_per_second': 88.045, 'eval_steps_per_second': 44.023, 'epoch': 1.28}\n",
      "{'loss': 2.0166, 'grad_norm': 4.958965301513672, 'learning_rate': 1.5576378559929434e-05, 'epoch': 1.3}\n",
      "{'loss': 2.0399, 'grad_norm': 5.893945693969727, 'learning_rate': 1.5499280338380946e-05, 'epoch': 1.32}\n",
      "{'loss': 2.053, 'grad_norm': 4.1669392585754395, 'learning_rate': 1.542104884841032e-05, 'epoch': 1.34}\n",
      "{'loss': 2.0523, 'grad_norm': 5.070234775543213, 'learning_rate': 1.5341697494580625e-05, 'epoch': 1.3599999999999999}\n",
      "{'eval_loss': 2.079895496368408, 'eval_runtime': 113.412, 'eval_samples_per_second': 88.174, 'eval_steps_per_second': 44.087, 'epoch': 1.3599999999999999}\n",
      "{'loss': 2.0889, 'grad_norm': 7.010958194732666, 'learning_rate': 1.526123987333784e-05, 'epoch': 1.38}\n",
      "{'loss': 2.0833, 'grad_norm': 9.384490966796875, 'learning_rate': 1.5179689770681167e-05, 'epoch': 1.4}\n",
      "{'loss': 2.0271, 'grad_norm': 5.0942792892456055, 'learning_rate': 1.5097061159800878e-05, 'epoch': 1.42}\n",
      "{'loss': 2.0472, 'grad_norm': 8.126705169677734, 'learning_rate': 1.501336819868407e-05, 'epoch': 1.44}\n",
      "{'eval_loss': 2.0937275886535645, 'eval_runtime': 115.3511, 'eval_samples_per_second': 86.692, 'eval_steps_per_second': 43.346, 'epoch': 1.44}\n",
      "{'loss': 2.0715, 'grad_norm': 6.744627475738525, 'learning_rate': 1.4928625227688778e-05, 'epoch': 1.46}\n",
      "{'loss': 2.074, 'grad_norm': 6.4202728271484375, 'learning_rate': 1.4842846767086829e-05, 'epoch': 1.48}\n",
      "{'loss': 2.0851, 'grad_norm': 7.575818061828613, 'learning_rate': 1.4756047514575855e-05, 'epoch': 1.5}\n",
      "{'loss': 2.0737, 'grad_norm': 6.880013942718506, 'learning_rate': 1.4668242342760928e-05, 'epoch': 1.52}\n",
      "{'eval_loss': 2.0812933444976807, 'eval_runtime': 117.0286, 'eval_samples_per_second': 85.449, 'eval_steps_per_second': 42.725, 'epoch': 1.52}\n",
      "{'loss': 2.0767, 'grad_norm': 7.286349296569824, 'learning_rate': 1.4579446296606215e-05, 'epoch': 1.54}\n",
      "{'loss': 2.0041, 'grad_norm': 11.406563758850098, 'learning_rate': 1.4489674590857109e-05, 'epoch': 1.56}\n",
      "{'loss': 2.0387, 'grad_norm': 4.2306437492370605, 'learning_rate': 1.4398942607433239e-05, 'epoch': 1.58}\n",
      "{'loss': 2.0082, 'grad_norm': 5.026838302612305, 'learning_rate': 1.4307265892792876e-05, 'epoch': 1.6}\n",
      "{'eval_loss': 2.0671987533569336, 'eval_runtime': 114.7057, 'eval_samples_per_second': 87.18, 'eval_steps_per_second': 43.59, 'epoch': 1.6}\n",
      "{'loss': 2.017, 'grad_norm': 4.853960037231445, 'learning_rate': 1.421466015526912e-05, 'epoch': 1.62}\n",
      "{'loss': 1.9917, 'grad_norm': 4.186732769012451, 'learning_rate': 1.4121141262378359e-05, 'epoch': 1.6400000000000001}\n",
      "{'loss': 2.0394, 'grad_norm': 4.68364953994751, 'learning_rate': 1.4026725238101457e-05, 'epoch': 1.6600000000000001}\n",
      "{'loss': 2.0666, 'grad_norm': 4.846363067626953, 'learning_rate': 1.3931428260138116e-05, 'epoch': 1.6800000000000002}\n",
      "{'eval_loss': 2.0660722255706787, 'eval_runtime': 118.1343, 'eval_samples_per_second': 84.649, 'eval_steps_per_second': 42.325, 'epoch': 1.6800000000000002}\n",
      "{'loss': 2.0284, 'grad_norm': 4.361277103424072, 'learning_rate': 1.383526665713492e-05, 'epoch': 1.7}\n",
      "{'loss': 2.0568, 'grad_norm': 4.379988670349121, 'learning_rate': 1.3738256905887502e-05, 'epoch': 1.72}\n",
      "{'loss': 2.0242, 'grad_norm': 17.081113815307617, 'learning_rate': 1.364041562851733e-05, 'epoch': 1.74}\n",
      "{'loss': 2.0098, 'grad_norm': 6.560647964477539, 'learning_rate': 1.3541759589623598e-05, 'epoch': 1.76}\n",
      "{'eval_loss': 2.0865209102630615, 'eval_runtime': 117.5852, 'eval_samples_per_second': 85.045, 'eval_steps_per_second': 42.522, 'epoch': 1.76}\n",
      "{'loss': 2.0159, 'grad_norm': 4.693203449249268, 'learning_rate': 1.3442305693410682e-05, 'epoch': 1.78}\n",
      "{'loss': 2.0298, 'grad_norm': 4.494261741638184, 'learning_rate': 1.334207098079171e-05, 'epoch': 1.8}\n",
      "{'loss': 2.0582, 'grad_norm': 4.4398980140686035, 'learning_rate': 1.3241072626468681e-05, 'epoch': 1.8199999999999998}\n",
      "{'loss': 2.0149, 'grad_norm': 5.304102897644043, 'learning_rate': 1.3139327935989664e-05, 'epoch': 1.8399999999999999}\n",
      "{'eval_loss': 2.0530242919921875, 'eval_runtime': 114.4088, 'eval_samples_per_second': 87.406, 'eval_steps_per_second': 43.703, 'epoch': 1.8399999999999999}\n",
      "{'loss': 2.0392, 'grad_norm': 4.957027435302734, 'learning_rate': 1.3036854342783593e-05, 'epoch': 1.8599999999999999}\n",
      "{'loss': 2.071, 'grad_norm': 6.1745381355285645, 'learning_rate': 1.293366940517313e-05, 'epoch': 1.88}\n",
      "{'loss': 2.0284, 'grad_norm': 6.249142169952393, 'learning_rate': 1.2829790803366127e-05, 'epoch': 1.9}\n",
      "{'loss': 2.0409, 'grad_norm': 7.445846080780029, 'learning_rate': 1.2725236336426247e-05, 'epoch': 1.92}\n",
      "{'eval_loss': 2.046264171600342, 'eval_runtime': 114.5505, 'eval_samples_per_second': 87.298, 'eval_steps_per_second': 43.649, 'epoch': 1.92}\n",
      "{'loss': 1.9973, 'grad_norm': 4.714721202850342, 'learning_rate': 1.2620023919223147e-05, 'epoch': 1.94}\n",
      "{'loss': 1.9961, 'grad_norm': 10.714550971984863, 'learning_rate': 1.2514171579362879e-05, 'epoch': 1.96}\n",
      "{'loss': 1.9948, 'grad_norm': 4.140952110290527, 'learning_rate': 1.240769745409895e-05, 'epoch': 1.98}\n",
      "{'loss': 2.0057, 'grad_norm': 5.590991973876953, 'learning_rate': 1.2300619787224594e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 2.0466771125793457, 'eval_runtime': 114.7698, 'eval_samples_per_second': 87.131, 'eval_steps_per_second': 43.565, 'epoch': 2.0}\n",
      "{'loss': 1.9325, 'grad_norm': 5.221226692199707, 'learning_rate': 1.219295692594679e-05, 'epoch': 2.02}\n",
      "{'loss': 1.8853, 'grad_norm': 4.392711639404297, 'learning_rate': 1.2084727317742574e-05, 'epoch': 2.04}\n",
      "{'loss': 1.8502, 'grad_norm': 5.099645614624023, 'learning_rate': 1.1975949507198137e-05, 'epoch': 2.06}\n",
      "{'loss': 1.8823, 'grad_norm': 4.235644340515137, 'learning_rate': 1.1866642132831333e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:24:12,837 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.0440101623535156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0440101623535156, 'eval_runtime': 117.5975, 'eval_samples_per_second': 85.036, 'eval_steps_per_second': 42.518, 'epoch': 2.08}\n",
      "{'train_runtime': 13067.8884, 'train_samples_per_second': 30.609, 'train_steps_per_second': 0.957, 'train_loss': 2.165654651935284, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:27:41,070 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.044877290725708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.044877290725708, 'eval_runtime': 206.9434, 'eval_samples_per_second': 48.322, 'eval_steps_per_second': 24.161, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:27:41,428 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-23 18:27:41,453] Trial 10 pruned. \n",
      "2025-07-23 18:27:47,859 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 18:27:47,899 - INFO - Trial 11 parameters: learning_rate=1.8217753575067095e-05, weight_decay=0.08084716496554312, warmup_steps=400, lr_scheduler_type=cosine\n",
      "2025-07-23 18:27:47,899 - INFO - VRAM usage before training (trial 11): 1.53GB / 7.96GB\n",
      "2025-07-23 18:27:47,960 - INFO - Starting new training for trial 11\n",
      "2025-07-23 18:27:58,650 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7965, 'grad_norm': 8.028253555297852, 'learning_rate': 2.049497277195048e-06, 'epoch': 0.02}\n",
      "{'loss': 2.6687, 'grad_norm': 6.364377498626709, 'learning_rate': 4.326716474078435e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5534, 'grad_norm': 5.582976818084717, 'learning_rate': 6.603935670961822e-06, 'epoch': 0.06}\n",
      "{'loss': 2.5553, 'grad_norm': 5.4468464851379395, 'learning_rate': 8.83561048390754e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 2.369004249572754, 'eval_runtime': 111.9859, 'eval_samples_per_second': 89.297, 'eval_steps_per_second': 44.648, 'epoch': 0.08}\n",
      "{'loss': 2.5191, 'grad_norm': 18.223304748535156, 'learning_rate': 1.1112829680790928e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4607, 'grad_norm': 8.88865852355957, 'learning_rate': 1.3390048877674315e-05, 'epoch': 0.12}\n",
      "{'loss': 2.452, 'grad_norm': 10.940591812133789, 'learning_rate': 1.56672680745577e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4312, 'grad_norm': 5.937512397766113, 'learning_rate': 1.7944487271441088e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.2659919261932373, 'eval_runtime': 111.5425, 'eval_samples_per_second': 89.652, 'eval_steps_per_second': 44.826, 'epoch': 0.16}\n",
      "{'loss': 2.4312, 'grad_norm': 6.42170524597168, 'learning_rate': 1.8217159194685553e-05, 'epoch': 0.18}\n",
      "{'loss': 2.3978, 'grad_norm': 12.9385986328125, 'learning_rate': 1.8215040898681507e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3574, 'grad_norm': 8.304386138916016, 'learning_rate': 1.8211387991393895e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3398, 'grad_norm': 4.730884075164795, 'learning_rate': 1.8206319854416345e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.2482950687408447, 'eval_runtime': 111.6957, 'eval_samples_per_second': 89.529, 'eval_steps_per_second': 44.764, 'epoch': 0.24}\n",
      "{'loss': 2.4259, 'grad_norm': 9.934473991394043, 'learning_rate': 1.8199630482083343e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2659, 'grad_norm': 4.806059837341309, 'learning_rate': 1.819140909549873e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3001, 'grad_norm': 6.442413330078125, 'learning_rate': 1.818165708016715e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3758, 'grad_norm': 6.227944374084473, 'learning_rate': 1.8170376079541607e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.19930100440979, 'eval_runtime': 111.7624, 'eval_samples_per_second': 89.476, 'eval_steps_per_second': 44.738, 'epoch': 0.32}\n",
      "{'loss': 2.2812, 'grad_norm': 10.627360343933105, 'learning_rate': 1.8157567994746508e-05, 'epoch': 0.34}\n",
      "{'loss': 2.354, 'grad_norm': 4.912478446960449, 'learning_rate': 1.8143234984257274e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3195, 'grad_norm': 6.642824172973633, 'learning_rate': 1.8127379463536596e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2566, 'grad_norm': 4.770817756652832, 'learning_rate': 1.8110004104627358e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.186053991317749, 'eval_runtime': 111.514, 'eval_samples_per_second': 89.675, 'eval_steps_per_second': 44.837, 'epoch': 0.4}\n",
      "{'loss': 2.2431, 'grad_norm': 8.09323501586914, 'learning_rate': 1.809111183570233e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2221, 'grad_norm': 7.204280853271484, 'learning_rate': 1.8070705840570716e-05, 'epoch': 0.44}\n",
      "{'loss': 2.282, 'grad_norm': 5.201468467712402, 'learning_rate': 1.8048789558141585e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2642, 'grad_norm': 7.973456859588623, 'learning_rate': 1.8025366681844344e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.166156053543091, 'eval_runtime': 110.4052, 'eval_samples_per_second': 90.575, 'eval_steps_per_second': 45.288, 'epoch': 0.48}\n",
      "{'loss': 2.2626, 'grad_norm': 6.814130783081055, 'learning_rate': 1.8000441159006302e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2909, 'grad_norm': 5.712926864624023, 'learning_rate': 1.7974017190187446e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2366, 'grad_norm': 18.169950485229492, 'learning_rate': 1.7946099228472546e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2855, 'grad_norm': 5.452426910400391, 'learning_rate': 1.7916691978720695e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.14408016204834, 'eval_runtime': 110.6899, 'eval_samples_per_second': 90.343, 'eval_steps_per_second': 45.171, 'epoch': 0.56}\n",
      "{'loss': 2.2665, 'grad_norm': 5.10791015625, 'learning_rate': 1.788580039677244e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2416, 'grad_norm': 5.674109935760498, 'learning_rate': 1.785342968861458e-05, 'epoch': 0.6}\n",
      "{'loss': 2.198, 'grad_norm': 4.65097713470459, 'learning_rate': 1.781958530950286e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2875, 'grad_norm': 4.165392875671387, 'learning_rate': 1.7784272963042595e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.1428451538085938, 'eval_runtime': 110.8788, 'eval_samples_per_second': 90.189, 'eval_steps_per_second': 45.094, 'epoch': 0.64}\n",
      "{'loss': 2.1944, 'grad_norm': 5.246941089630127, 'learning_rate': 1.77474986002275e-05, 'epoch': 0.66}\n",
      "{'loss': 2.242, 'grad_norm': 5.009769439697266, 'learning_rate': 1.7709268418436772e-05, 'epoch': 0.68}\n",
      "{'loss': 2.1992, 'grad_norm': 4.152364730834961, 'learning_rate': 1.766958886039071e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2166, 'grad_norm': 7.411594867706299, 'learning_rate': 1.762846661306495e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.1305716037750244, 'eval_runtime': 124.3864, 'eval_samples_per_second': 80.395, 'eval_steps_per_second': 40.197, 'epoch': 0.72}\n",
      "{'loss': 2.2107, 'grad_norm': 9.0450439453125, 'learning_rate': 1.758590860656354e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2066, 'grad_norm': 6.341097831726074, 'learning_rate': 1.754192201295104e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2386, 'grad_norm': 4.3017168045043945, 'learning_rate': 1.749651424504388e-05, 'epoch': 0.78}\n",
      "{'loss': 2.244, 'grad_norm': 5.2873101234436035, 'learning_rate': 1.74496929551611e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1393074989318848, 'eval_runtime': 117.2019, 'eval_samples_per_second': 85.323, 'eval_steps_per_second': 42.661, 'epoch': 0.8}\n",
      "{'loss': 2.2024, 'grad_norm': 8.123128890991211, 'learning_rate': 1.7401466033834744e-05, 'epoch': 0.82}\n",
      "{'loss': 2.217, 'grad_norm': 4.289263725280762, 'learning_rate': 1.735184160848013e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1238, 'grad_norm': 6.717594146728516, 'learning_rate': 1.7300828042026162e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2063, 'grad_norm': 4.461568832397461, 'learning_rate': 1.7248433931505987e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1014225482940674, 'eval_runtime': 111.5769, 'eval_samples_per_second': 89.624, 'eval_steps_per_second': 44.812, 'epoch': 0.88}\n",
      "{'loss': 2.2068, 'grad_norm': 7.500065803527832, 'learning_rate': 1.719466810660818e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1568, 'grad_norm': 6.077490329742432, 'learning_rate': 1.713953962818872e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2115, 'grad_norm': 4.1853461265563965, 'learning_rate': 1.7083057786744027e-05, 'epoch': 0.94}\n",
      "{'loss': 2.154, 'grad_norm': 6.151220321655273, 'learning_rate': 1.7025232100845268e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.099949359893799, 'eval_runtime': 111.5831, 'eval_samples_per_second': 89.619, 'eval_steps_per_second': 44.81, 'epoch': 0.96}\n",
      "{'loss': 2.1926, 'grad_norm': 6.448945999145508, 'learning_rate': 1.696607231553426e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2151, 'grad_norm': 5.368005275726318, 'learning_rate': 1.6905588400681194e-05, 'epoch': 1.0}\n",
      "{'loss': 2.0053, 'grad_norm': 5.076147079467773, 'learning_rate': 1.6843790549304453e-05, 'epoch': 1.02}\n",
      "{'loss': 2.1, 'grad_norm': 5.738207817077637, 'learning_rate': 1.6780689175852846e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.0985231399536133, 'eval_runtime': 111.6162, 'eval_samples_per_second': 89.593, 'eval_steps_per_second': 44.796, 'epoch': 1.04}\n",
      "{'loss': 2.063, 'grad_norm': 5.712852954864502, 'learning_rate': 1.671629491445053e-05, 'epoch': 1.06}\n",
      "{'loss': 2.0541, 'grad_norm': 9.697442054748535, 'learning_rate': 1.6650618617104883e-05, 'epoch': 1.08}\n",
      "{'loss': 2.0663, 'grad_norm': 6.802971363067627, 'learning_rate': 1.658367135187769e-05, 'epoch': 1.1}\n",
      "{'loss': 2.0499, 'grad_norm': 6.525656700134277, 'learning_rate': 1.6515464401019885e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 20:26:30,131 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.0929675102233887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0929675102233887, 'eval_runtime': 111.6971, 'eval_samples_per_second': 89.528, 'eval_steps_per_second': 44.764, 'epoch': 1.12}\n",
      "{'train_runtime': 7111.4809, 'train_samples_per_second': 56.247, 'train_steps_per_second': 1.758, 'train_loss': 2.278565913609096, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 20:29:17,183 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.0943987369537354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0943987369537354, 'eval_runtime': 165.7739, 'eval_samples_per_second': 60.323, 'eval_steps_per_second': 30.162, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 20:29:17,530 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-23 20:29:17,566] Trial 11 pruned. \n",
      "2025-07-23 20:29:18,742 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 20:29:18,785 - INFO - Trial 12 parameters: learning_rate=1.7067425832878712e-05, weight_decay=0.055640433189825755, warmup_steps=400, lr_scheduler_type=cosine\n",
      "2025-07-23 20:29:18,786 - INFO - VRAM usage before training (trial 12): 1.53GB / 7.96GB\n",
      "2025-07-23 20:29:18,829 - INFO - Starting new training for trial 12\n",
      "2025-07-23 20:29:29,396 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7988, 'grad_norm': 7.836755275726318, 'learning_rate': 1.9200854061988552e-06, 'epoch': 0.02}\n",
      "{'loss': 2.6739, 'grad_norm': 6.441662788391113, 'learning_rate': 4.053513635308694e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5583, 'grad_norm': 6.105821132659912, 'learning_rate': 6.186941864418533e-06, 'epoch': 0.06}\n",
      "{'loss': 2.5603, 'grad_norm': 5.595389366149902, 'learning_rate': 8.277701528946175e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 2.374535083770752, 'eval_runtime': 110.3739, 'eval_samples_per_second': 90.601, 'eval_steps_per_second': 45.301, 'epoch': 0.08}\n",
      "{'loss': 2.5204, 'grad_norm': 9.701536178588867, 'learning_rate': 1.0411129758056014e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4616, 'grad_norm': 10.57320499420166, 'learning_rate': 1.2544557987165854e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4532, 'grad_norm': 11.511216163635254, 'learning_rate': 1.4677986216275691e-05, 'epoch': 0.14}\n",
      "{'loss': 2.433, 'grad_norm': 5.691565036773682, 'learning_rate': 1.681141444538553e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.270559310913086, 'eval_runtime': 110.8954, 'eval_samples_per_second': 90.175, 'eval_steps_per_second': 45.088, 'epoch': 0.16}\n",
      "{'loss': 2.4343, 'grad_norm': 6.482486724853516, 'learning_rate': 1.706686898359229e-05, 'epoch': 0.18}\n",
      "{'loss': 2.4009, 'grad_norm': 20.422447204589844, 'learning_rate': 1.7064884443632838e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3593, 'grad_norm': 7.72370719909668, 'learning_rate': 1.706146219269785e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3532, 'grad_norm': 14.520565032958984, 'learning_rate': 1.7056602807520274e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.2213821411132812, 'eval_runtime': 110.83, 'eval_samples_per_second': 90.228, 'eval_steps_per_second': 45.114, 'epoch': 0.24}\n",
      "{'loss': 2.4279, 'grad_norm': 5.102128028869629, 'learning_rate': 1.7050307107025317e-05, 'epoch': 0.26}\n",
      "{'loss': 2.277, 'grad_norm': 4.588016986846924, 'learning_rate': 1.704274482875347e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3111, 'grad_norm': 6.938531398773193, 'learning_rate': 1.703360858713581e-05, 'epoch': 0.3}\n",
      "{'loss': 2.38, 'grad_norm': 12.193086624145508, 'learning_rate': 1.702303990528907e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.2009811401367188, 'eval_runtime': 112.0142, 'eval_samples_per_second': 89.274, 'eval_steps_per_second': 44.637, 'epoch': 0.32}\n",
      "{'loss': 2.3041, 'grad_norm': 10.962749481201172, 'learning_rate': 1.7011040564294542e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3624, 'grad_norm': 7.606380939483643, 'learning_rate': 1.6997612586334533e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3241, 'grad_norm': 5.551365852355957, 'learning_rate': 1.698275823435163e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2736, 'grad_norm': 6.755358695983887, 'learning_rate': 1.6966480011667306e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.1856067180633545, 'eval_runtime': 118.5245, 'eval_samples_per_second': 84.371, 'eval_steps_per_second': 42.185, 'epoch': 0.4}\n",
      "{'loss': 2.2463, 'grad_norm': 7.529117584228516, 'learning_rate': 1.6948780661560055e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2264, 'grad_norm': 7.705745220184326, 'learning_rate': 1.6929663166803095e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2814, 'grad_norm': 4.9635186195373535, 'learning_rate': 1.6909130749161684e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2613, 'grad_norm': 5.130146503448486, 'learning_rate': 1.6887186868850176e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.1652793884277344, 'eval_runtime': 117.3455, 'eval_samples_per_second': 85.218, 'eval_steps_per_second': 42.609, 'epoch': 0.48}\n",
      "{'loss': 2.2571, 'grad_norm': 5.029837608337402, 'learning_rate': 1.6863835223948896e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2768, 'grad_norm': 5.475252151489258, 'learning_rate': 1.683907974978092e-05, 'epoch': 0.52}\n",
      "{'loss': 2.24, 'grad_norm': 10.23216724395752, 'learning_rate': 1.681292461824888e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2907, 'grad_norm': 5.126299858093262, 'learning_rate': 1.6785374237131877e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1556642055511475, 'eval_runtime': 118.8159, 'eval_samples_per_second': 84.164, 'eval_steps_per_second': 42.082, 'epoch': 0.56}\n",
      "{'loss': 2.2663, 'grad_norm': 8.057474136352539, 'learning_rate': 1.6756433249342706e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2401, 'grad_norm': 10.999911308288574, 'learning_rate': 1.672610653214536e-05, 'epoch': 0.6}\n",
      "{'loss': 2.2031, 'grad_norm': 6.38888692855835, 'learning_rate': 1.669439919633313e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2847, 'grad_norm': 6.935705184936523, 'learning_rate': 1.666131658536729e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.1442270278930664, 'eval_runtime': 120.7011, 'eval_samples_per_second': 82.849, 'eval_steps_per_second': 41.425, 'epoch': 0.64}\n",
      "{'loss': 2.194, 'grad_norm': 5.1140875816345215, 'learning_rate': 1.6626864274476608e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2417, 'grad_norm': 5.2143025398254395, 'learning_rate': 1.6591048069717766e-05, 'epoch': 0.68}\n",
      "{'loss': 2.1987, 'grad_norm': 4.0699334144592285, 'learning_rate': 1.6553874006996915e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2197, 'grad_norm': 8.156790733337402, 'learning_rate': 1.6515348351052472e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.1376354694366455, 'eval_runtime': 120.7523, 'eval_samples_per_second': 82.814, 'eval_steps_per_second': 41.407, 'epoch': 0.72}\n",
      "{'loss': 2.2094, 'grad_norm': 5.293460369110107, 'learning_rate': 1.647547759439935e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2111, 'grad_norm': 7.721986293792725, 'learning_rate': 1.643426845623482e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2412, 'grad_norm': 4.596120357513428, 'learning_rate': 1.6391727881306158e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2415, 'grad_norm': 4.515810966491699, 'learning_rate': 1.6347863038740294e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.124767541885376, 'eval_runtime': 120.0817, 'eval_samples_per_second': 83.277, 'eval_steps_per_second': 41.638, 'epoch': 0.8}\n",
      "{'loss': 2.201, 'grad_norm': 4.802924156188965, 'learning_rate': 1.6302681320835613e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2196, 'grad_norm': 5.181562423706055, 'learning_rate': 1.625619034181621e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1257, 'grad_norm': 11.287801742553711, 'learning_rate': 1.6208397936548674e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2061, 'grad_norm': 4.4985575675964355, 'learning_rate': 1.615931215922174e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1007113456726074, 'eval_runtime': 109.889, 'eval_samples_per_second': 91.001, 'eval_steps_per_second': 45.5, 'epoch': 0.88}\n",
      "{'loss': 2.2072, 'grad_norm': 5.349140644073486, 'learning_rate': 1.610894128198895e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1577, 'grad_norm': 7.051796913146973, 'learning_rate': 1.605729379357461e-05, 'epoch': 0.92}\n",
      "{'loss': 2.212, 'grad_norm': 6.611800193786621, 'learning_rate': 1.6004378397843215e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1563, 'grad_norm': 4.451477527618408, 'learning_rate': 1.5950204012332637e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.1086955070495605, 'eval_runtime': 109.8368, 'eval_samples_per_second': 91.044, 'eval_steps_per_second': 45.522, 'epoch': 0.96}\n",
      "{'loss': 2.1943, 'grad_norm': 6.811552047729492, 'learning_rate': 1.589477976675131e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2175, 'grad_norm': 6.80448579788208, 'learning_rate': 1.583811500143965e-05, 'epoch': 1.0}\n",
      "{'loss': 2.0111, 'grad_norm': 7.2360944747924805, 'learning_rate': 1.5780219265795968e-05, 'epoch': 1.02}\n",
      "{'loss': 2.1091, 'grad_norm': 4.547407150268555, 'learning_rate': 1.5721102316667177e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.099149227142334, 'eval_runtime': 110.1467, 'eval_samples_per_second': 90.788, 'eval_steps_per_second': 45.394, 'epoch': 1.04}\n",
      "{'loss': 2.072, 'grad_norm': 4.801021575927734, 'learning_rate': 1.566077411670452e-05, 'epoch': 1.06}\n",
      "{'loss': 2.0593, 'grad_norm': 8.826299667358398, 'learning_rate': 1.5599244832684617e-05, 'epoch': 1.08}\n",
      "{'loss': 2.0712, 'grad_norm': 11.945070266723633, 'learning_rate': 1.5536524833796114e-05, 'epoch': 1.1}\n",
      "{'loss': 2.0563, 'grad_norm': 6.556596279144287, 'learning_rate': 1.547262468989223e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 22:27:49,276 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.1045470237731934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1045470237731934, 'eval_runtime': 109.9518, 'eval_samples_per_second': 90.949, 'eval_steps_per_second': 45.474, 'epoch': 1.12}\n",
      "{'train_runtime': 7099.8801, 'train_samples_per_second': 56.339, 'train_steps_per_second': 1.761, 'train_loss': 2.281704698290144, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 22:30:33,961 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.1059975624084473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1059975624084473, 'eval_runtime': 163.3983, 'eval_samples_per_second': 61.2, 'eval_steps_per_second': 30.6, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 22:30:34,295 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-23 22:30:34,597] Trial 12 pruned. \n",
      "2025-07-23 22:30:35,804 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-23 22:30:35,851 - INFO - Trial 13 parameters: learning_rate=1.4123623126220836e-05, weight_decay=0.036053084954851045, warmup_steps=400, lr_scheduler_type=cosine\n",
      "2025-07-23 22:30:35,851 - INFO - VRAM usage before training (trial 13): 1.53GB / 7.96GB\n",
      "2025-07-23 22:30:35,898 - INFO - Starting new training for trial 13\n",
      "2025-07-23 22:30:46,550 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8052, 'grad_norm': 7.4107985496521, 'learning_rate': 1.5889076016998441e-06, 'epoch': 0.02}\n",
      "{'loss': 2.6888, 'grad_norm': 6.402698040008545, 'learning_rate': 3.354360492477448e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5716, 'grad_norm': 5.511441230773926, 'learning_rate': 5.119813383255053e-06, 'epoch': 0.06}\n",
      "{'loss': 2.573, 'grad_norm': 5.848320484161377, 'learning_rate': 6.849957216217105e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 2.3838706016540527, 'eval_runtime': 109.8067, 'eval_samples_per_second': 91.069, 'eval_steps_per_second': 45.535, 'epoch': 0.08}\n",
      "{'loss': 2.5301, 'grad_norm': 9.32917594909668, 'learning_rate': 8.61541010699471e-06, 'epoch': 0.1}\n",
      "{'loss': 2.4713, 'grad_norm': 10.759674072265625, 'learning_rate': 1.0380862997772315e-05, 'epoch': 0.12}\n",
      "{'loss': 2.4609, 'grad_norm': 13.287156105041504, 'learning_rate': 1.2146315888549918e-05, 'epoch': 0.14}\n",
      "{'loss': 2.4395, 'grad_norm': 5.726579666137695, 'learning_rate': 1.3911768779327524e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.275125741958618, 'eval_runtime': 110.2383, 'eval_samples_per_second': 90.713, 'eval_steps_per_second': 45.356, 'epoch': 0.16}\n",
      "{'loss': 2.4398, 'grad_norm': 6.0798659324646, 'learning_rate': 1.412316232272671e-05, 'epoch': 0.18}\n",
      "{'loss': 2.408, 'grad_norm': 8.959513664245605, 'learning_rate': 1.4121520077742571e-05, 'epoch': 0.2}\n",
      "{'loss': 2.362, 'grad_norm': 9.534605979919434, 'learning_rate': 1.4118688099275376e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3452, 'grad_norm': 5.317530632019043, 'learning_rate': 1.4114666864582737e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.226886034011841, 'eval_runtime': 110.1347, 'eval_samples_per_second': 90.798, 'eval_steps_per_second': 45.399, 'epoch': 0.24}\n",
      "{'loss': 2.4295, 'grad_norm': 5.13590145111084, 'learning_rate': 1.410945705134101e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2741, 'grad_norm': 4.692035675048828, 'learning_rate': 1.4103059537531098e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3013, 'grad_norm': 6.691564559936523, 'learning_rate': 1.4095475401290487e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3832, 'grad_norm': 8.7994966506958, 'learning_rate': 1.4086705920731557e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.210982084274292, 'eval_runtime': 110.0567, 'eval_samples_per_second': 90.862, 'eval_steps_per_second': 45.431, 'epoch': 0.32}\n",
      "{'loss': 2.286, 'grad_norm': 7.388589382171631, 'learning_rate': 1.4076752573726175e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3641, 'grad_norm': 8.381394386291504, 'learning_rate': 1.4065617037656655e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3238, 'grad_norm': 6.028702735900879, 'learning_rate': 1.4053301189133078e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2647, 'grad_norm': 4.958746433258057, 'learning_rate': 1.4039807103677014e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.1905362606048584, 'eval_runtime': 110.3419, 'eval_samples_per_second': 90.627, 'eval_steps_per_second': 45.314, 'epoch': 0.4}\n",
      "{'loss': 2.2516, 'grad_norm': 7.645157337188721, 'learning_rate': 1.4025137055371776e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2306, 'grad_norm': 6.372917175292969, 'learning_rate': 1.4009293516479159e-05, 'epoch': 0.44}\n",
      "{'loss': 2.29, 'grad_norm': 7.602461338043213, 'learning_rate': 1.3992279157022818e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2642, 'grad_norm': 5.532950401306152, 'learning_rate': 1.3974096844338293e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.1641037464141846, 'eval_runtime': 109.4703, 'eval_samples_per_second': 91.349, 'eval_steps_per_second': 45.674, 'epoch': 0.48}\n",
      "{'loss': 2.2633, 'grad_norm': 6.101658344268799, 'learning_rate': 1.3954749642589797e-05, 'epoch': 0.5}\n",
      "{'loss': 2.282, 'grad_norm': 6.584266185760498, 'learning_rate': 1.3934240812253834e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2411, 'grad_norm': 8.41810131072998, 'learning_rate': 1.3912573809569728e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2951, 'grad_norm': 5.396157741546631, 'learning_rate': 1.3889752285957145e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.1699795722961426, 'eval_runtime': 109.8623, 'eval_samples_per_second': 91.023, 'eval_steps_per_second': 45.512, 'epoch': 0.56}\n",
      "{'loss': 2.2723, 'grad_norm': 4.783050537109375, 'learning_rate': 1.3865780087400768e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2451, 'grad_norm': 7.915177822113037, 'learning_rate': 1.3840661253802136e-05, 'epoch': 0.6}\n",
      "{'loss': 2.2105, 'grad_norm': 5.72649621963501, 'learning_rate': 1.3814400018298821e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2904, 'grad_norm': 4.31445837020874, 'learning_rate': 1.3787000806551053e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.139968156814575, 'eval_runtime': 110.5915, 'eval_samples_per_second': 90.423, 'eval_steps_per_second': 45.211, 'epoch': 0.64}\n",
      "{'loss': 2.1994, 'grad_norm': 6.774606227874756, 'learning_rate': 1.3758468235995874e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2471, 'grad_norm': 5.526528835296631, 'learning_rate': 1.3728807115069e-05, 'epoch': 0.68}\n",
      "{'loss': 2.2054, 'grad_norm': 6.93577766418457, 'learning_rate': 1.3698022442394472e-05, 'epoch': 0.7}\n",
      "{'loss': 2.224, 'grad_norm': 5.550780296325684, 'learning_rate': 1.366611940594227e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.1313111782073975, 'eval_runtime': 110.9481, 'eval_samples_per_second': 90.132, 'eval_steps_per_second': 45.066, 'epoch': 0.72}\n",
      "{'loss': 2.2141, 'grad_norm': 7.003043174743652, 'learning_rate': 1.3633103382154013e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2146, 'grad_norm': 5.030886650085449, 'learning_rate': 1.3598979935036901e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2481, 'grad_norm': 4.504693508148193, 'learning_rate': 1.3563754815226033e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2483, 'grad_norm': 4.4624342918396, 'learning_rate': 1.3527433959015293e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1262433528900146, 'eval_runtime': 110.5597, 'eval_samples_per_second': 90.449, 'eval_steps_per_second': 45.224, 'epoch': 0.8}\n",
      "{'loss': 2.2058, 'grad_norm': 6.276127338409424, 'learning_rate': 1.349002348735694e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2254, 'grad_norm': 8.650708198547363, 'learning_rate': 1.3451529704830072e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1307, 'grad_norm': 6.1069135665893555, 'learning_rate': 1.3411959098578154e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2115, 'grad_norm': 4.624178886413574, 'learning_rate': 1.3371318337215778e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1135830879211426, 'eval_runtime': 110.9426, 'eval_samples_per_second': 90.137, 'eval_steps_per_second': 45.068, 'epoch': 0.88}\n",
      "{'loss': 2.2155, 'grad_norm': 4.695592403411865, 'learning_rate': 1.3329614269704833e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1658, 'grad_norm': 6.428223609924316, 'learning_rate': 1.3287719436317666e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2193, 'grad_norm': 5.838422775268555, 'learning_rate': 1.3243930928653822e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1629, 'grad_norm': 4.398703098297119, 'learning_rate': 1.3199100582734169e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.1077141761779785, 'eval_runtime': 110.7699, 'eval_samples_per_second': 90.277, 'eval_steps_per_second': 45.139, 'epoch': 0.96}\n",
      "{'loss': 2.2014, 'grad_norm': 6.91035270690918, 'learning_rate': 1.3153235953568017e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2265, 'grad_norm': 6.2947187423706055, 'learning_rate': 1.3106344770466699e-05, 'epoch': 1.0}\n",
      "{'loss': 2.0341, 'grad_norm': 5.382735252380371, 'learning_rate': 1.3058434935740981e-05, 'epoch': 1.02}\n",
      "{'loss': 2.1296, 'grad_norm': 7.1824774742126465, 'learning_rate': 1.300951452336933e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.1077191829681396, 'eval_runtime': 110.4569, 'eval_samples_per_second': 90.533, 'eval_steps_per_second': 45.267, 'epoch': 1.04}\n",
      "{'loss': 2.0939, 'grad_norm': 6.521404266357422, 'learning_rate': 1.2959591777637256e-05, 'epoch': 1.06}\n",
      "{'loss': 2.0796, 'grad_norm': 11.981817245483398, 'learning_rate': 1.2908675111747944e-05, 'epoch': 1.08}\n",
      "{'loss': 2.0939, 'grad_norm': 14.286925315856934, 'learning_rate': 1.2856773106404424e-05, 'epoch': 1.1}\n",
      "{'loss': 2.0782, 'grad_norm': 7.526533126831055, 'learning_rate': 1.2803894508363516e-05, 'epoch': 1.12}\n",
      "{'eval_loss': 2.096670627593994, 'eval_runtime': 110.9222, 'eval_samples_per_second': 90.153, 'eval_steps_per_second': 45.077, 'epoch': 1.12}\n",
      "{'loss': 2.0875, 'grad_norm': 7.096742630004883, 'learning_rate': 1.275004822896179e-05, 'epoch': 1.1400000000000001}\n",
      "{'loss': 2.0486, 'grad_norm': 8.982150077819824, 'learning_rate': 1.2695243342613774e-05, 'epoch': 1.16}\n",
      "{'loss': 2.08, 'grad_norm': 6.737297534942627, 'learning_rate': 1.2639489085282706e-05, 'epoch': 1.18}\n",
      "{'loss': 2.0879, 'grad_norm': 6.687857151031494, 'learning_rate': 1.258279485292405e-05, 'epoch': 1.2}\n",
      "{'eval_loss': 2.0874621868133545, 'eval_runtime': 110.933, 'eval_samples_per_second': 90.145, 'eval_steps_per_second': 45.072, 'epoch': 1.2}\n",
      "{'loss': 2.0578, 'grad_norm': 4.318362236022949, 'learning_rate': 1.2525170199902042e-05, 'epoch': 1.22}\n",
      "{'loss': 2.0664, 'grad_norm': 4.00345516204834, 'learning_rate': 1.2466624837379539e-05, 'epoch': 1.24}\n",
      "{'loss': 2.0905, 'grad_norm': 9.275022506713867, 'learning_rate': 1.2407168631681469e-05, 'epoch': 1.26}\n",
      "{'loss': 2.075, 'grad_norm': 7.47322940826416, 'learning_rate': 1.2346811602632095e-05, 'epoch': 1.28}\n",
      "{'eval_loss': 2.0836071968078613, 'eval_runtime': 110.6167, 'eval_samples_per_second': 90.402, 'eval_steps_per_second': 45.201, 'epoch': 1.28}\n",
      "{'loss': 2.0404, 'grad_norm': 5.814143657684326, 'learning_rate': 1.2285563921866448e-05, 'epoch': 1.3}\n",
      "{'loss': 2.0627, 'grad_norm': 7.859636306762695, 'learning_rate': 1.2223435911116141e-05, 'epoch': 1.32}\n",
      "{'loss': 2.0728, 'grad_norm': 4.310009956359863, 'learning_rate': 1.2160438040469911e-05, 'epoch': 1.34}\n",
      "{'loss': 2.0686, 'grad_norm': 5.099787712097168, 'learning_rate': 1.2096580926609159e-05, 'epoch': 1.3599999999999999}\n",
      "{'eval_loss': 2.085432767868042, 'eval_runtime': 110.6656, 'eval_samples_per_second': 90.362, 'eval_steps_per_second': 45.181, 'epoch': 1.3599999999999999}\n",
      "{'loss': 2.1084, 'grad_norm': 7.571351051330566, 'learning_rate': 1.2031875331018772e-05, 'epoch': 1.38}\n",
      "{'loss': 2.1087, 'grad_norm': 10.99466323852539, 'learning_rate': 1.1966332158173554e-05, 'epoch': 1.4}\n",
      "{'loss': 2.0468, 'grad_norm': 4.90540885925293, 'learning_rate': 1.189996245370055e-05, 'epoch': 1.42}\n",
      "{'loss': 2.069, 'grad_norm': 11.256261825561523, 'learning_rate': 1.1832777402517599e-05, 'epoch': 1.44}\n",
      "{'eval_loss': 2.1024482250213623, 'eval_runtime': 110.7725, 'eval_samples_per_second': 90.275, 'eval_steps_per_second': 45.138, 'epoch': 1.44}\n",
      "{'loss': 2.0897, 'grad_norm': 5.884911060333252, 'learning_rate': 1.1764788326948389e-05, 'epoch': 1.46}\n",
      "{'loss': 2.0967, 'grad_norm': 5.474812984466553, 'learning_rate': 1.1696006684814372e-05, 'epoch': 1.48}\n",
      "{'loss': 2.1083, 'grad_norm': 6.952495098114014, 'learning_rate': 1.1626444067503848e-05, 'epoch': 1.5}\n",
      "{'loss': 2.0944, 'grad_norm': 6.421207427978516, 'learning_rate': 1.155611219801853e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 01:11:36,273 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.075291872024536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.075291872024536, 'eval_runtime': 110.6569, 'eval_samples_per_second': 90.369, 'eval_steps_per_second': 45.185, 'epoch': 1.52}\n",
      "{'train_runtime': 9649.7327, 'train_samples_per_second': 41.452, 'train_steps_per_second': 1.295, 'train_loss': 2.2328046738473994, 'epoch': 1.52}\n",
      "{'eval_loss': 2.07149338722229, 'eval_runtime': 133.5055, 'eval_samples_per_second': 74.903, 'eval_steps_per_second': 37.452, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 01:13:51,392 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-24 01:13:51,694] Trial 13 pruned. \n",
      "2025-07-24 01:13:52,922 - INFO - Gradient checkpointing enabled: True\n",
      "2025-07-24 01:13:52,964 - INFO - Trial 14 parameters: learning_rate=2.06855389280456e-05, weight_decay=0.050876264401976144, warmup_steps=500, lr_scheduler_type=linear\n",
      "2025-07-24 01:13:52,965 - INFO - VRAM usage before training (trial 14): 1.53GB / 7.96GB\n",
      "2025-07-24 01:13:53,013 - INFO - Starting new training for trial 14\n",
      "2025-07-24 01:14:03,552 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7999, 'grad_norm': 7.756847858428955, 'learning_rate': 1.8616985035241038e-06, 'epoch': 0.02}\n",
      "{'loss': 2.6764, 'grad_norm': 6.367199420928955, 'learning_rate': 3.930252396328664e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5601, 'grad_norm': 5.619592666625977, 'learning_rate': 5.998806289133223e-06, 'epoch': 0.06}\n",
      "{'loss': 2.567, 'grad_norm': 5.711325645446777, 'learning_rate': 8.025989104081693e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 2.372591018676758, 'eval_runtime': 110.1708, 'eval_samples_per_second': 90.768, 'eval_steps_per_second': 45.384, 'epoch': 0.08}\n",
      "{'loss': 2.5197, 'grad_norm': 11.38612174987793, 'learning_rate': 1.0094542996886252e-05, 'epoch': 0.1}\n",
      "{'loss': 2.4635, 'grad_norm': 9.956722259521484, 'learning_rate': 1.2163096889690812e-05, 'epoch': 0.12}\n",
      "{'loss': 2.457, 'grad_norm': 13.943159103393555, 'learning_rate': 1.423165078249537e-05, 'epoch': 0.14}\n",
      "{'loss': 2.433, 'grad_norm': 5.7905473709106445, 'learning_rate': 1.6300204675299932e-05, 'epoch': 0.16}\n",
      "{'eval_loss': 2.276730537414551, 'eval_runtime': 109.8081, 'eval_samples_per_second': 91.068, 'eval_steps_per_second': 45.534, 'epoch': 0.16}\n",
      "{'loss': 2.4294, 'grad_norm': 5.888808250427246, 'learning_rate': 1.836875856810449e-05, 'epoch': 0.18}\n",
      "{'loss': 2.4037, 'grad_norm': 7.102105617523193, 'learning_rate': 2.0437312460909053e-05, 'epoch': 0.2}\n",
      "{'loss': 2.3657, 'grad_norm': 7.641732215881348, 'learning_rate': 2.0609691951976097e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3621, 'grad_norm': 64.36516571044922, 'learning_rate': 2.0523502206442573e-05, 'epoch': 0.24}\n",
      "{'eval_loss': 2.2412643432617188, 'eval_runtime': 110.5864, 'eval_samples_per_second': 90.427, 'eval_steps_per_second': 45.214, 'epoch': 0.24}\n",
      "{'loss': 2.4317, 'grad_norm': 5.294484615325928, 'learning_rate': 2.0437312460909053e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2787, 'grad_norm': 4.464152812957764, 'learning_rate': 2.035112271537553e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3057, 'grad_norm': 5.853271961212158, 'learning_rate': 2.0264932969842005e-05, 'epoch': 0.3}\n",
      "{'loss': 2.3816, 'grad_norm': 6.891451358795166, 'learning_rate': 2.017874322430848e-05, 'epoch': 0.32}\n",
      "{'eval_loss': 2.197169065475464, 'eval_runtime': 111.0206, 'eval_samples_per_second': 90.073, 'eval_steps_per_second': 45.037, 'epoch': 0.32}\n",
      "{'loss': 2.2818, 'grad_norm': 5.75018310546875, 'learning_rate': 2.0092553478774958e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3653, 'grad_norm': 7.080895900726318, 'learning_rate': 2.0006363733241434e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3171, 'grad_norm': 5.668292999267578, 'learning_rate': 1.992017398770791e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2585, 'grad_norm': 4.805822849273682, 'learning_rate': 1.9833984242174387e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.1770002841949463, 'eval_runtime': 119.489, 'eval_samples_per_second': 83.69, 'eval_steps_per_second': 41.845, 'epoch': 0.4}\n",
      "{'loss': 2.2465, 'grad_norm': 6.3632330894470215, 'learning_rate': 1.9747794496640863e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2244, 'grad_norm': 15.298856735229492, 'learning_rate': 1.9661604751107343e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2908, 'grad_norm': 4.785898208618164, 'learning_rate': 1.957541500557382e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2555, 'grad_norm': 5.192431926727295, 'learning_rate': 1.9489225260040295e-05, 'epoch': 0.48}\n",
      "{'eval_loss': 2.1558926105499268, 'eval_runtime': 111.1499, 'eval_samples_per_second': 89.969, 'eval_steps_per_second': 44.984, 'epoch': 0.48}\n",
      "{'loss': 2.2562, 'grad_norm': 5.293258190155029, 'learning_rate': 1.940303551450677e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2752, 'grad_norm': 4.768008232116699, 'learning_rate': 1.9316845768973248e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2363, 'grad_norm': 11.629851341247559, 'learning_rate': 1.9230656023439724e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2861, 'grad_norm': 5.350802421569824, 'learning_rate': 1.91444662779062e-05, 'epoch': 0.56}\n",
      "{'eval_loss': 2.146129846572876, 'eval_runtime': 111.2692, 'eval_samples_per_second': 89.872, 'eval_steps_per_second': 44.936, 'epoch': 0.56}\n",
      "{'loss': 2.2646, 'grad_norm': 4.808839321136475, 'learning_rate': 1.9058276532372677e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2384, 'grad_norm': 5.037514686584473, 'learning_rate': 1.8972086786839157e-05, 'epoch': 0.6}\n",
      "{'loss': 2.2007, 'grad_norm': 7.924898624420166, 'learning_rate': 1.8885897041305633e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2843, 'grad_norm': 7.922774791717529, 'learning_rate': 1.879970729577211e-05, 'epoch': 0.64}\n",
      "{'eval_loss': 2.1338980197906494, 'eval_runtime': 109.6954, 'eval_samples_per_second': 91.162, 'eval_steps_per_second': 45.581, 'epoch': 0.64}\n",
      "{'loss': 2.19, 'grad_norm': 5.114330291748047, 'learning_rate': 1.8713517550238585e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2354, 'grad_norm': 6.647650718688965, 'learning_rate': 1.8627327804705062e-05, 'epoch': 0.68}\n",
      "{'loss': 2.195, 'grad_norm': 3.9789257049560547, 'learning_rate': 1.8541138059171538e-05, 'epoch': 0.7}\n",
      "{'loss': 2.216, 'grad_norm': 7.780385494232178, 'learning_rate': 1.8454948313638014e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 2.1289126873016357, 'eval_runtime': 109.8836, 'eval_samples_per_second': 91.005, 'eval_steps_per_second': 45.503, 'epoch': 0.72}\n",
      "{'loss': 2.2077, 'grad_norm': 5.648091793060303, 'learning_rate': 1.836875856810449e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2041, 'grad_norm': 6.464745998382568, 'learning_rate': 1.828256882257097e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2357, 'grad_norm': 4.357262134552002, 'learning_rate': 1.8196379077037447e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2403, 'grad_norm': 4.714500904083252, 'learning_rate': 1.811018933150392e-05, 'epoch': 0.8}\n",
      "{'eval_loss': 2.1262526512145996, 'eval_runtime': 118.9357, 'eval_samples_per_second': 84.079, 'eval_steps_per_second': 42.04, 'epoch': 0.8}\n",
      "{'loss': 2.1966, 'grad_norm': 4.810766696929932, 'learning_rate': 1.8023999585970396e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2132, 'grad_norm': 6.137394905090332, 'learning_rate': 1.7937809840436876e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1196, 'grad_norm': 9.413806915283203, 'learning_rate': 1.7851620094903352e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2009, 'grad_norm': 4.312778472900391, 'learning_rate': 1.7765430349369828e-05, 'epoch': 0.88}\n",
      "{'eval_loss': 2.1051647663116455, 'eval_runtime': 118.2841, 'eval_samples_per_second': 84.542, 'eval_steps_per_second': 42.271, 'epoch': 0.88}\n",
      "{'loss': 2.2034, 'grad_norm': 4.578089714050293, 'learning_rate': 1.7679240603836305e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1484, 'grad_norm': 6.575521469116211, 'learning_rate': 1.759305085830278e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2103, 'grad_norm': 11.780494689941406, 'learning_rate': 1.750686111276926e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1559, 'grad_norm': 4.9369354248046875, 'learning_rate': 1.7420671367235733e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 2.099015235900879, 'eval_runtime': 118.8021, 'eval_samples_per_second': 84.174, 'eval_steps_per_second': 42.087, 'epoch': 0.96}\n",
      "{'loss': 2.1942, 'grad_norm': 5.735925674438477, 'learning_rate': 1.733448162170221e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2167, 'grad_norm': 7.27532434463501, 'learning_rate': 1.724829187616869e-05, 'epoch': 1.0}\n",
      "{'loss': 1.9992, 'grad_norm': 5.042924880981445, 'learning_rate': 1.7162102130635166e-05, 'epoch': 1.02}\n",
      "{'loss': 2.0934, 'grad_norm': 5.216263771057129, 'learning_rate': 1.7075912385101642e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 2.0955164432525635, 'eval_runtime': 118.9069, 'eval_samples_per_second': 84.099, 'eval_steps_per_second': 42.05, 'epoch': 1.04}\n",
      "{'loss': 2.0552, 'grad_norm': 5.104605197906494, 'learning_rate': 1.698972263956812e-05, 'epoch': 1.06}\n",
      "{'loss': 2.0454, 'grad_norm': 6.83073091506958, 'learning_rate': 1.6903532894034595e-05, 'epoch': 1.08}\n",
      "{'loss': 2.0602, 'grad_norm': 12.565290451049805, 'learning_rate': 1.681734314850107e-05, 'epoch': 1.1}\n",
      "{'loss': 2.0407, 'grad_norm': 5.8190531730651855, 'learning_rate': 1.6731153402967547e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 03:12:12,208 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.100008249282837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.100008249282837, 'eval_runtime': 118.5215, 'eval_samples_per_second': 84.373, 'eval_steps_per_second': 42.186, 'epoch': 1.12}\n",
      "{'train_runtime': 7088.6564, 'train_samples_per_second': 56.428, 'train_steps_per_second': 1.763, 'train_loss': 2.2784713472638813, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 03:15:31,921 - INFO - Early stopping triggered after 3 evaluations with eval_loss=2.101543426513672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.101543426513672, 'eval_runtime': 198.3846, 'eval_samples_per_second': 50.407, 'eval_steps_per_second': 25.204, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 03:15:32,287 - INFO - VRAM usage after training: 4.43GB / 7.96GB\n",
      "[I 2025-07-24 03:15:32,311] Trial 14 pruned. \n",
      "2025-07-24 03:15:32,345 - INFO - Saved trial results to optuna_trials.csv\n",
      "2025-07-24 03:15:32,350 - INFO - Best hyperparameters: {'learning_rate': 1.2853916978930139e-05, 'weight_decay': 0.011430983876313222, 'warmup_steps': 450, 'lr_scheduler_type': 'cosine'}\n",
      "2025-07-24 03:15:32,355 - INFO - Best objective value (eval_loss): 2.0840227603912354\n",
      "2025-07-24 03:15:32,360 - INFO - Deleted non-best trial directory: trial_0\n",
      "2025-07-24 03:15:32,360 - INFO - Deleted non-best trial directory: trial_10\n",
      "2025-07-24 03:15:32,361 - INFO - Deleted non-best trial directory: trial_11\n",
      "2025-07-24 03:15:32,362 - INFO - Deleted non-best trial directory: trial_12\n",
      "2025-07-24 03:15:32,363 - INFO - Deleted non-best trial directory: trial_13\n",
      "2025-07-24 03:15:32,363 - INFO - Deleted non-best trial directory: trial_14\n",
      "2025-07-24 03:15:32,364 - INFO - Deleted non-best trial directory: trial_2\n",
      "2025-07-24 03:15:32,364 - INFO - Deleted non-best trial directory: trial_3\n",
      "2025-07-24 03:15:32,365 - INFO - Deleted non-best trial directory: trial_4\n",
      "2025-07-24 03:15:32,366 - INFO - Deleted non-best trial directory: trial_5\n",
      "2025-07-24 03:15:32,366 - INFO - Deleted non-best trial directory: trial_6\n",
      "2025-07-24 03:15:32,367 - INFO - Deleted non-best trial directory: trial_7\n",
      "2025-07-24 03:15:32,367 - INFO - Deleted non-best trial directory: trial_8\n",
      "2025-07-24 03:15:32,368 - INFO - Deleted non-best trial directory: trial_9\n",
      "2025-07-24 03:15:32,368 - INFO - Closed Optuna storage connections.\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna hyperparameter search with Bayesian optimization\n",
    "study_name = \"bart_question_generation\"\n",
    "storage_url = f\"sqlite:///{os.path.join(OUTPUT_DIR, 'optuna_study.db')}\"\n",
    "db_path = os.path.join(OUTPUT_DIR, \"optuna_study.db\")\n",
    "if os.path.exists(db_path):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        conn.close()\n",
    "        os.remove(db_path)\n",
    "        logger.info(\"Deleted existing Optuna database to start fresh.\")\n",
    "    except (PermissionError, sqlite3.OperationalError) as e:\n",
    "        logger.warning(f\"Could not delete existing database {db_path}: {e}. Reusing existing database.\")\n",
    "\n",
    "try:\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_url,\n",
    "        direction=\"minimize\",\n",
    "        sampler=TPESampler(seed=42),  # Bayesian optimization\n",
    "        pruner=MedianPruner(n_warmup_steps=2),\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(objective, n_trials=15)\n",
    "    save_trial_results(study, OUTPUT_DIR)\n",
    "    best_params = study.best_params\n",
    "    with open(os.path.join(OUTPUT_DIR, \"best_params.json\"), \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    logger.info(f\"Best hyperparameters: {best_params}\")\n",
    "    logger.info(f\"Best objective value (eval_loss): {study.best_value}\")\n",
    "\n",
    "    # Clean up non-best trials\n",
    "    best_trial_dir = os.path.join(OUTPUT_DIR, f\"trial_{study.best_trial.number}\")\n",
    "    for trial_dir in os.listdir(OUTPUT_DIR):\n",
    "        if trial_dir.startswith(\"trial_\") and trial_dir != os.path.basename(best_trial_dir):\n",
    "            try:\n",
    "                shutil.rmtree(os.path.join(OUTPUT_DIR, trial_dir))\n",
    "                logger.info(f\"Deleted non-best trial directory: {trial_dir}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to delete trial directory {trial_dir}: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Optuna optimization or file saving failed: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    if 'storage' in locals():\n",
    "        del storage\n",
    "    logger.info(\"Closed Optuna storage connections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ceaae71-5ce5-4bf0-a382-9cdcb24646c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 03:18:04,084 - INFO - Training final model with best hyperparameters\n",
      "2025-07-24 03:18:05,551 - INFO - VRAM usage before final training: 5.95GB / 7.96GB\n",
      "2025-07-24 03:18:05,734 - INFO - Starting final training\n",
      "2025-07-24 03:18:16,375 - INFO - Initialized CustomEarlyStoppingCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8115, 'grad_norm': 7.2612762451171875, 'learning_rate': 1.285391697893014e-06, 'epoch': 0.02}\n",
      "{'loss': 2.7058, 'grad_norm': 6.481954574584961, 'learning_rate': 2.713604695551918e-06, 'epoch': 0.04}\n",
      "{'loss': 2.5877, 'grad_norm': 5.4879150390625, 'learning_rate': 4.141817693210823e-06, 'epoch': 0.06}\n",
      "{'loss': 2.5881, 'grad_norm': 6.2297821044921875, 'learning_rate': 5.541466430916549e-06, 'epoch': 0.08}\n",
      "{'loss': 2.5418, 'grad_norm': 8.759868621826172, 'learning_rate': 6.969679428575454e-06, 'epoch': 0.1}\n",
      "{'loss': 2.4848, 'grad_norm': 9.387632369995117, 'learning_rate': 8.397892426234358e-06, 'epoch': 0.12}\n",
      "{'loss': 2.4736, 'grad_norm': 13.297496795654297, 'learning_rate': 9.826105423893262e-06, 'epoch': 0.14}\n",
      "{'loss': 2.4506, 'grad_norm': 5.881244659423828, 'learning_rate': 1.1254318421552166e-05, 'epoch': 0.16}\n",
      "{'loss': 2.4455, 'grad_norm': 6.311375141143799, 'learning_rate': 1.268253141921107e-05, 'epoch': 0.18}\n",
      "{'loss': 2.4191, 'grad_norm': 12.817657470703125, 'learning_rate': 1.2853494113929514e-05, 'epoch': 0.2}\n",
      "{'loss': 2.373, 'grad_norm': 9.956896781921387, 'learning_rate': 1.2851987077528022e-05, 'epoch': 0.22}\n",
      "{'loss': 2.3542, 'grad_norm': 6.108640670776367, 'learning_rate': 1.2849388262533352e-05, 'epoch': 0.24}\n",
      "{'loss': 2.4399, 'grad_norm': 5.043007850646973, 'learning_rate': 1.2845698110551412e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2855, 'grad_norm': 4.838390350341797, 'learning_rate': 1.2841023549881212e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3102, 'grad_norm': 5.435155391693115, 'learning_rate': 1.2835174579212733e-05, 'epoch': 0.3}\n",
      "{'loss': 2.395, 'grad_norm': 7.450186252593994, 'learning_rate': 1.2828236686829534e-05, 'epoch': 0.32}\n",
      "{'loss': 2.2941, 'grad_norm': 5.286353588104248, 'learning_rate': 1.2820211051659035e-05, 'epoch': 0.34}\n",
      "{'loss': 2.3726, 'grad_norm': 10.252151489257812, 'learning_rate': 1.2811099037464302e-05, 'epoch': 0.36}\n",
      "{'loss': 2.3288, 'grad_norm': 6.4493584632873535, 'learning_rate': 1.28009021926123e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2724, 'grad_norm': 5.9947686195373535, 'learning_rate': 1.2789622249810801e-05, 'epoch': 0.4}\n",
      "{'loss': 2.2558, 'grad_norm': 8.435148239135742, 'learning_rate': 1.277726112581393e-05, 'epoch': 0.42}\n",
      "{'loss': 2.2388, 'grad_norm': 6.419126510620117, 'learning_rate': 1.2763820921096478e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2941, 'grad_norm': 5.194875240325928, 'learning_rate': 1.2749303919496965e-05, 'epoch': 0.46}\n",
      "{'loss': 2.2731, 'grad_norm': 8.195486068725586, 'learning_rate': 1.2733712587829569e-05, 'epoch': 0.48}\n",
      "{'loss': 2.2699, 'grad_norm': 6.974205017089844, 'learning_rate': 1.2717049575464934e-05, 'epoch': 0.5}\n",
      "{'loss': 2.2914, 'grad_norm': 5.493058204650879, 'learning_rate': 1.2699317713879987e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2521, 'grad_norm': 10.898320198059082, 'learning_rate': 1.2680520016176792e-05, 'epoch': 0.54}\n",
      "{'loss': 2.3028, 'grad_norm': 5.339782238006592, 'learning_rate': 1.2660659676570547e-05, 'epoch': 0.56}\n",
      "{'loss': 2.2817, 'grad_norm': 4.838479518890381, 'learning_rate': 1.2639740069846796e-05, 'epoch': 0.58}\n",
      "{'loss': 2.2526, 'grad_norm': 13.413585662841797, 'learning_rate': 1.2617764750787985e-05, 'epoch': 0.6}\n",
      "{'loss': 2.208, 'grad_norm': 4.994421482086182, 'learning_rate': 1.259473745356939e-05, 'epoch': 0.62}\n",
      "{'loss': 2.2995, 'grad_norm': 11.320837020874023, 'learning_rate': 1.25706620911246e-05, 'epoch': 0.64}\n",
      "{'loss': 2.2058, 'grad_norm': 6.924022674560547, 'learning_rate': 1.2545542754480602e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2571, 'grad_norm': 5.6239447593688965, 'learning_rate': 1.2519383712062605e-05, 'epoch': 0.68}\n",
      "{'loss': 2.2128, 'grad_norm': 5.3938727378845215, 'learning_rate': 1.249218940896874e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2326, 'grad_norm': 8.596445083618164, 'learning_rate': 1.2463964466214704e-05, 'epoch': 0.72}\n",
      "{'loss': 2.2239, 'grad_norm': 5.729229927062988, 'learning_rate': 1.2434713679948541e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2211, 'grad_norm': 5.334582805633545, 'learning_rate': 1.2404442020635646e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2542, 'grad_norm': 4.600104331970215, 'learning_rate': 1.2373154632214165e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2531, 'grad_norm': 4.688175201416016, 'learning_rate': 1.2340856831220894e-05, 'epoch': 0.8}\n",
      "{'loss': 2.2139, 'grad_norm': 6.584193706512451, 'learning_rate': 1.2307554105887868e-05, 'epoch': 0.82}\n",
      "{'loss': 2.2324, 'grad_norm': 7.128045558929443, 'learning_rate': 1.2273252115209771e-05, 'epoch': 0.84}\n",
      "{'loss': 2.1372, 'grad_norm': 6.097860813140869, 'learning_rate': 1.2237956687982323e-05, 'epoch': 0.86}\n",
      "{'loss': 2.2209, 'grad_norm': 4.593571662902832, 'learning_rate': 1.2201673821811813e-05, 'epoch': 0.88}\n",
      "{'loss': 2.2201, 'grad_norm': 4.847883701324463, 'learning_rate': 1.2164409682095957e-05, 'epoch': 0.9}\n",
      "{'loss': 2.1753, 'grad_norm': 6.747812271118164, 'learning_rate': 1.2126170600976238e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2257, 'grad_norm': 7.311673641204834, 'learning_rate': 1.2086963076261903e-05, 'epoch': 0.94}\n",
      "{'loss': 2.1691, 'grad_norm': 6.1137237548828125, 'learning_rate': 1.2046793770325829e-05, 'epoch': 0.96}\n",
      "{'loss': 2.2072, 'grad_norm': 5.248306751251221, 'learning_rate': 1.2005669508972399e-05, 'epoch': 0.98}\n",
      "{'loss': 2.2308, 'grad_norm': 6.310542106628418, 'learning_rate': 1.1963597280277634e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 05:12:17,566 - INFO - Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1029350757598877, 'eval_bleu-1': 0.2700138367266258, 'eval_bleu-2': 0.148682834700573, 'eval_bleu-3': 0.10192163977284457, 'eval_bleu-4': 0.07242951093638948, 'eval_rouge-l': 0.2205406619320127, 'eval_meteor': 0.23111950149719418, 'eval_bertscore': 0.8767882585525513, 'eval_runtime': 2078.7477, 'eval_samples_per_second': 4.811, 'eval_steps_per_second': 2.405, 'epoch': 1.0}\n",
      "{'loss': 2.1202, 'grad_norm': 5.572562217712402, 'learning_rate': 1.1920584233401736e-05, 'epoch': 1.02}\n",
      "{'loss': 2.1231, 'grad_norm': 8.109247207641602, 'learning_rate': 1.187663767737425e-05, 'epoch': 1.04}\n",
      "{'loss': 2.0864, 'grad_norm': 5.420586585998535, 'learning_rate': 1.1831765079852086e-05, 'epoch': 1.06}\n",
      "{'loss': 2.1186, 'grad_norm': 7.683208465576172, 'learning_rate': 1.1785974065850561e-05, 'epoch': 1.08}\n",
      "{'loss': 2.1085, 'grad_norm': 9.568700790405273, 'learning_rate': 1.173927241644771e-05, 'epoch': 1.1}\n",
      "{'loss': 2.1213, 'grad_norm': 4.8004841804504395, 'learning_rate': 1.1691668067462093e-05, 'epoch': 1.12}\n",
      "{'loss': 2.0933, 'grad_norm': 7.000199794769287, 'learning_rate': 1.1643169108104276e-05, 'epoch': 1.1400000000000001}\n",
      "{'loss': 2.1189, 'grad_norm': 8.061059951782227, 'learning_rate': 1.159378377960228e-05, 'epoch': 1.16}\n",
      "{'loss': 2.0702, 'grad_norm': 5.7520012855529785, 'learning_rate': 1.1543520473801173e-05, 'epoch': 1.18}\n",
      "{'loss': 2.0924, 'grad_norm': 8.172025680541992, 'learning_rate': 1.1492387731737084e-05, 'epoch': 1.2}\n",
      "{'loss': 2.0845, 'grad_norm': 4.941314220428467, 'learning_rate': 1.1440394242185854e-05, 'epoch': 1.22}\n",
      "{'loss': 2.1271, 'grad_norm': 4.861856460571289, 'learning_rate': 1.1387548840186599e-05, 'epoch': 1.24}\n",
      "{'loss': 2.0576, 'grad_norm': 5.0287017822265625, 'learning_rate': 1.1333860505540399e-05, 'epoch': 1.26}\n",
      "{'loss': 2.1066, 'grad_norm': 13.858171463012695, 'learning_rate': 1.1279338361284397e-05, 'epoch': 1.28}\n",
      "{'loss': 2.091, 'grad_norm': 5.868900775909424, 'learning_rate': 1.1223991672141567e-05, 'epoch': 1.3}\n",
      "{'loss': 2.069, 'grad_norm': 5.671584606170654, 'learning_rate': 1.116782984294639e-05, 'epoch': 1.32}\n",
      "{'loss': 2.0844, 'grad_norm': 4.917626857757568, 'learning_rate': 1.1110862417046731e-05, 'epoch': 1.34}\n",
      "{'loss': 2.0713, 'grad_norm': 7.5014142990112305, 'learning_rate': 1.105309907468218e-05, 'epoch': 1.3599999999999999}\n",
      "{'loss': 2.1215, 'grad_norm': 6.863321781158447, 'learning_rate': 1.0994549631339131e-05, 'epoch': 1.38}\n",
      "{'loss': 2.0783, 'grad_norm': 6.851878643035889, 'learning_rate': 1.0935224036082867e-05, 'epoch': 1.4}\n",
      "{'loss': 2.0745, 'grad_norm': 5.1966872215271, 'learning_rate': 1.0875132369866963e-05, 'epoch': 1.42}\n",
      "{'loss': 2.0836, 'grad_norm': 12.75436782836914, 'learning_rate': 1.0814284843820279e-05, 'epoch': 1.44}\n",
      "{'loss': 2.1289, 'grad_norm': 5.1791462898254395, 'learning_rate': 1.0752691797511801e-05, 'epoch': 1.46}\n",
      "{'loss': 2.0762, 'grad_norm': 5.00025749206543, 'learning_rate': 1.0690363697193717e-05, 'epoch': 1.48}\n",
      "{'loss': 2.0526, 'grad_norm': 6.368342876434326, 'learning_rate': 1.0627311134022893e-05, 'epoch': 1.5}\n",
      "{'loss': 2.0945, 'grad_norm': 7.121289253234863, 'learning_rate': 1.0563544822261184e-05, 'epoch': 1.52}\n",
      "{'loss': 2.0764, 'grad_norm': 10.72463607788086, 'learning_rate': 1.0499075597454797e-05, 'epoch': 1.54}\n",
      "{'loss': 2.0537, 'grad_norm': 12.160786628723145, 'learning_rate': 1.0433914414593053e-05, 'epoch': 1.56}\n",
      "{'loss': 2.1067, 'grad_norm': 4.173401355743408, 'learning_rate': 1.0368072346246845e-05, 'epoch': 1.58}\n",
      "{'loss': 2.0485, 'grad_norm': 5.028210639953613, 'learning_rate': 1.0301560580687135e-05, 'epoch': 1.6}\n",
      "{'loss': 2.0919, 'grad_norm': 5.0792412757873535, 'learning_rate': 1.0234390419983763e-05, 'epoch': 1.62}\n",
      "{'loss': 2.1072, 'grad_norm': 5.340366363525391, 'learning_rate': 1.0166573278084936e-05, 'epoch': 1.6400000000000001}\n",
      "{'loss': 2.0612, 'grad_norm': 5.262008190155029, 'learning_rate': 1.0098120678877714e-05, 'epoch': 1.6600000000000001}\n",
      "{'loss': 2.0679, 'grad_norm': 6.3126068115234375, 'learning_rate': 1.0029044254229787e-05, 'epoch': 1.6800000000000002}\n",
      "{'loss': 2.0744, 'grad_norm': 4.743760585784912, 'learning_rate': 9.959355742012932e-06, 'epoch': 1.7}\n",
      "{'loss': 2.0589, 'grad_norm': 7.613552093505859, 'learning_rate': 9.88906698410844e-06, 'epoch': 1.72}\n",
      "{'loss': 2.0561, 'grad_norm': 13.438197135925293, 'learning_rate': 9.818189924394875e-06, 'epoch': 1.74}\n",
      "{'loss': 2.0816, 'grad_norm': 7.182173728942871, 'learning_rate': 9.746736606718498e-06, 'epoch': 1.76}\n",
      "{'loss': 2.058, 'grad_norm': 5.439145088195801, 'learning_rate': 9.674719172846706e-06, 'epoch': 1.78}\n",
      "{'loss': 2.0463, 'grad_norm': 4.549468040466309, 'learning_rate': 9.60214986040484e-06, 'epoch': 1.8}\n",
      "{'loss': 2.0704, 'grad_norm': 4.825024604797363, 'learning_rate': 9.52904100079668e-06, 'epoch': 1.8199999999999998}\n",
      "{'loss': 2.0455, 'grad_norm': 11.937095642089844, 'learning_rate': 9.455405017109037e-06, 'epoch': 1.8399999999999999}\n",
      "{'loss': 2.0714, 'grad_norm': 7.596925735473633, 'learning_rate': 9.381254422000739e-06, 'epoch': 1.8599999999999999}\n",
      "{'loss': 2.0527, 'grad_norm': 11.38196086883545, 'learning_rate': 9.306601815576403e-06, 'epoch': 1.88}\n",
      "{'loss': 2.0582, 'grad_norm': 10.52678108215332, 'learning_rate': 9.231459883245368e-06, 'epoch': 1.9}\n",
      "{'loss': 2.0132, 'grad_norm': 4.96956205368042, 'learning_rate': 9.155841393566095e-06, 'epoch': 1.92}\n",
      "{'loss': 2.018, 'grad_norm': 8.828947067260742, 'learning_rate': 9.079759196076477e-06, 'epoch': 1.94}\n",
      "{'loss': 2.0915, 'grad_norm': 4.909588813781738, 'learning_rate': 9.003226219110357e-06, 'epoch': 1.96}\n",
      "{'loss': 2.0857, 'grad_norm': 5.057737350463867, 'learning_rate': 8.926255467600672e-06, 'epoch': 1.98}\n",
      "{'loss': 2.0325, 'grad_norm': 5.427199840545654, 'learning_rate': 8.848860020869587e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 07:10:10,170 - INFO - Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.057978630065918, 'eval_bleu-1': 0.2645873183010486, 'eval_bleu-2': 0.14799917622265413, 'eval_bleu-3': 0.10225881430572212, 'eval_bleu-4': 0.07299661793154594, 'eval_rouge-l': 0.22071261938481257, 'eval_meteor': 0.2357890755636357, 'eval_bertscore': 0.8763830065727234, 'eval_runtime': 2268.5744, 'eval_samples_per_second': 4.408, 'eval_steps_per_second': 2.204, 'epoch': 2.0}\n",
      "{'loss': 1.9341, 'grad_norm': 4.783074378967285, 'learning_rate': 8.771053030405965e-06, 'epoch': 2.02}\n",
      "{'loss': 1.9291, 'grad_norm': 5.118368148803711, 'learning_rate': 8.692847717630605e-06, 'epoch': 2.04}\n",
      "{'loss': 1.9138, 'grad_norm': 5.140848159790039, 'learning_rate': 8.61425737164957e-06, 'epoch': 2.06}\n",
      "{'loss': 1.9574, 'grad_norm': 4.8419084548950195, 'learning_rate': 8.535295346996024e-06, 'epoch': 2.08}\n",
      "{'loss': 1.9885, 'grad_norm': 5.008954048156738, 'learning_rate': 8.455975061360957e-06, 'epoch': 2.1}\n",
      "{'loss': 1.9541, 'grad_norm': 4.736142158508301, 'learning_rate': 8.37630999331317e-06, 'epoch': 2.12}\n",
      "{'loss': 1.9953, 'grad_norm': 7.382446765899658, 'learning_rate': 8.297916764646689e-06, 'epoch': 2.14}\n",
      "{'loss': 1.9961, 'grad_norm': 8.971339225769043, 'learning_rate': 8.217609018996925e-06, 'epoch': 2.16}\n",
      "{'loss': 1.9368, 'grad_norm': 6.007615566253662, 'learning_rate': 8.136996995492326e-06, 'epoch': 2.18}\n",
      "{'loss': 1.9396, 'grad_norm': 6.129716396331787, 'learning_rate': 8.056094392201385e-06, 'epoch': 2.2}\n",
      "{'loss': 1.9896, 'grad_norm': 4.819878578186035, 'learning_rate': 7.974914956569605e-06, 'epoch': 2.22}\n",
      "{'loss': 1.9537, 'grad_norm': 5.2877116203308105, 'learning_rate': 7.893472483083474e-06, 'epoch': 2.24}\n",
      "{'loss': 1.9283, 'grad_norm': 10.658591270446777, 'learning_rate': 7.811780810926412e-06, 'epoch': 2.26}\n",
      "{'loss': 1.9766, 'grad_norm': 7.0802693367004395, 'learning_rate': 7.729853821627146e-06, 'epoch': 2.2800000000000002}\n",
      "{'loss': 1.9827, 'grad_norm': 4.256796360015869, 'learning_rate': 7.647705436700872e-06, 'epoch': 2.3}\n",
      "{'loss': 1.9556, 'grad_norm': 5.065797328948975, 'learning_rate': 7.565349615283629e-06, 'epoch': 2.32}\n",
      "{'loss': 1.9523, 'grad_norm': 4.755655765533447, 'learning_rate': 7.482800351760285e-06, 'epoch': 2.34}\n",
      "{'loss': 1.979, 'grad_norm': 7.017025947570801, 'learning_rate': 7.4000716733865246e-06, 'epoch': 2.36}\n",
      "{'loss': 1.981, 'grad_norm': 5.551936149597168, 'learning_rate': 7.317177637905265e-06, 'epoch': 2.38}\n",
      "{'loss': 1.9568, 'grad_norm': 4.942154884338379, 'learning_rate': 7.234132331157863e-06, 'epoch': 2.4}\n",
      "{'loss': 1.9161, 'grad_norm': 10.037521362304688, 'learning_rate': 7.150949864690585e-06, 'epoch': 2.42}\n",
      "{'loss': 1.9729, 'grad_norm': 4.912211894989014, 'learning_rate': 7.067644373356682e-06, 'epoch': 2.44}\n",
      "{'loss': 1.9894, 'grad_norm': 5.152975559234619, 'learning_rate': 6.98423001291451e-06, 'epoch': 2.46}\n",
      "{'loss': 1.9367, 'grad_norm': 4.2645697593688965, 'learning_rate': 6.900720957622105e-06, 'epoch': 2.48}\n",
      "{'loss': 2.0276, 'grad_norm': 5.193452835083008, 'learning_rate': 6.817131397828607e-06, 'epoch': 2.5}\n",
      "{'loss': 1.9517, 'grad_norm': 5.989157199859619, 'learning_rate': 6.733475537562958e-06, 'epoch': 2.52}\n",
      "{'loss': 1.9536, 'grad_norm': 6.4379119873046875, 'learning_rate': 6.649767592120257e-06, 'epoch': 2.54}\n",
      "{'loss': 1.9684, 'grad_norm': 4.908402919769287, 'learning_rate': 6.566021785646231e-06, 'epoch': 2.56}\n",
      "{'loss': 1.8896, 'grad_norm': 5.3702192306518555, 'learning_rate': 6.482252348720169e-06, 'epoch': 2.58}\n",
      "{'loss': 1.9803, 'grad_norm': 4.874999523162842, 'learning_rate': 6.398473515936794e-06, 'epoch': 2.6}\n",
      "{'loss': 1.913, 'grad_norm': 8.976483345031738, 'learning_rate': 6.314699523487414e-06, 'epoch': 2.62}\n",
      "{'loss': 1.9192, 'grad_norm': 4.253270626068115, 'learning_rate': 6.230944606740856e-06, 'epoch': 2.64}\n",
      "{'loss': 1.9259, 'grad_norm': 6.086986541748047, 'learning_rate': 6.147222997824479e-06, 'epoch': 2.66}\n",
      "{'loss': 1.9239, 'grad_norm': 7.29707145690918, 'learning_rate': 6.063548923205786e-06, 'epoch': 2.68}\n",
      "{'loss': 2.0225, 'grad_norm': 4.4819207191467285, 'learning_rate': 5.9799366012749675e-06, 'epoch': 2.7}\n",
      "{'loss': 1.9532, 'grad_norm': 8.44150161743164, 'learning_rate': 5.896400239928842e-06, 'epoch': 2.7199999999999998}\n",
      "{'loss': 1.9218, 'grad_norm': 6.634314060211182, 'learning_rate': 5.812954034156553e-06, 'epoch': 2.74}\n",
      "{'loss': 1.8924, 'grad_norm': 16.60879898071289, 'learning_rate': 5.7296121636274815e-06, 'epoch': 2.76}\n",
      "{'loss': 1.9589, 'grad_norm': 5.421474933624268, 'learning_rate': 5.646388790281752e-06, 'epoch': 2.7800000000000002}\n",
      "{'loss': 1.9559, 'grad_norm': 4.213469505310059, 'learning_rate': 5.563298055923755e-06, 'epoch': 2.8}\n",
      "{'loss': 1.9393, 'grad_norm': 4.926420211791992, 'learning_rate': 5.480354079819076e-06, 'epoch': 2.82}\n",
      "{'loss': 1.9707, 'grad_norm': 7.7674689292907715, 'learning_rate': 5.397570956295282e-06, 'epoch': 2.84}\n",
      "{'loss': 1.9562, 'grad_norm': 4.8324384689331055, 'learning_rate': 5.314962752346929e-06, 'epoch': 2.86}\n",
      "{'loss': 1.9982, 'grad_norm': 4.372940540313721, 'learning_rate': 5.2325435052452025e-06, 'epoch': 2.88}\n",
      "{'loss': 1.9153, 'grad_norm': 7.672372341156006, 'learning_rate': 5.150327220152633e-06, 'epoch': 2.9}\n",
      "{'loss': 1.9438, 'grad_norm': 4.241641998291016, 'learning_rate': 5.06832786774326e-06, 'epoch': 2.92}\n",
      "{'loss': 1.9433, 'grad_norm': 8.508514404296875, 'learning_rate': 4.986559381828643e-06, 'epoch': 2.94}\n",
      "{'loss': 1.936, 'grad_norm': 7.191079139709473, 'learning_rate': 4.905035656990152e-06, 'epoch': 2.96}\n",
      "{'loss': 1.9699, 'grad_norm': 6.647617340087891, 'learning_rate': 4.825393224623818e-06, 'epoch': 2.98}\n",
      "{'loss': 1.9073, 'grad_norm': 4.288381576538086, 'learning_rate': 4.744394953467436e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 09:05:56,368 - INFO - Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0274946689605713, 'eval_bleu-1': 0.27264447494418337, 'eval_bleu-2': 0.1524797718063744, 'eval_bleu-3': 0.10511095432569854, 'eval_bleu-4': 0.07490457733100264, 'eval_rouge-l': 0.23465419073840293, 'eval_meteor': 0.24675927683619292, 'eval_bertscore': 0.879502534866333, 'eval_runtime': 2173.4878, 'eval_samples_per_second': 4.601, 'eval_steps_per_second': 2.3, 'epoch': 3.0}\n",
      "{'loss': 1.8691, 'grad_norm': 8.324563026428223, 'learning_rate': 4.663682593389126e-06, 'epoch': 3.02}\n",
      "{'loss': 1.8803, 'grad_norm': 4.338119983673096, 'learning_rate': 4.583269859507152e-06, 'epoch': 3.04}\n",
      "{'loss': 1.8481, 'grad_norm': 4.917848587036133, 'learning_rate': 4.503170416025539e-06, 'epoch': 3.06}\n",
      "{'loss': 1.865, 'grad_norm': 4.235669136047363, 'learning_rate': 4.423397873912161e-06, 'epoch': 3.08}\n",
      "{'loss': 1.8531, 'grad_norm': 5.017637729644775, 'learning_rate': 4.343965788585893e-06, 'epoch': 3.1}\n",
      "{'loss': 1.8491, 'grad_norm': 5.7430548667907715, 'learning_rate': 4.264887657613198e-06, 'epoch': 3.12}\n",
      "{'loss': 1.911, 'grad_norm': 5.1263203620910645, 'learning_rate': 4.186176918414531e-06, 'epoch': 3.14}\n",
      "{'loss': 1.873, 'grad_norm': 14.997989654541016, 'learning_rate': 4.107846945980981e-06, 'epoch': 3.16}\n",
      "{'loss': 1.9054, 'grad_norm': 6.56271505355835, 'learning_rate': 4.029911050601511e-06, 'epoch': 3.18}\n",
      "{'loss': 1.8889, 'grad_norm': 5.52522611618042, 'learning_rate': 3.952382475601199e-06, 'epoch': 3.2}\n",
      "{'loss': 1.9085, 'grad_norm': 6.441898345947266, 'learning_rate': 3.875274395090849e-06, 'epoch': 3.22}\n",
      "{'loss': 1.8925, 'grad_norm': 5.987137794494629, 'learning_rate': 3.798599911728371e-06, 'epoch': 3.24}\n",
      "{'loss': 1.9209, 'grad_norm': 4.339778900146484, 'learning_rate': 3.722372054492303e-06, 'epoch': 3.26}\n",
      "{'loss': 1.872, 'grad_norm': 6.132890701293945, 'learning_rate': 3.646603776467851e-06, 'epoch': 3.2800000000000002}\n",
      "{'loss': 1.8535, 'grad_norm': 4.553279399871826, 'learning_rate': 3.571307952645816e-06, 'epoch': 3.3}\n",
      "{'loss': 1.8839, 'grad_norm': 5.812704563140869, 'learning_rate': 3.4964973777348125e-06, 'epoch': 3.32}\n",
      "{'loss': 1.8847, 'grad_norm': 6.345953941345215, 'learning_rate': 3.4221847639871067e-06, 'epoch': 3.34}\n",
      "{'loss': 1.8696, 'grad_norm': 4.636763095855713, 'learning_rate': 3.348382739038478e-06, 'epoch': 3.36}\n",
      "{'loss': 1.9022, 'grad_norm': 5.133237838745117, 'learning_rate': 3.27510384376246e-06, 'epoch': 3.38}\n",
      "{'loss': 1.9007, 'grad_norm': 5.466887950897217, 'learning_rate': 3.202360530139319e-06, 'epoch': 3.4}\n",
      "{'loss': 1.8739, 'grad_norm': 4.21937894821167, 'learning_rate': 3.1301651591401455e-06, 'epoch': 3.42}\n",
      "{'loss': 1.853, 'grad_norm': 8.35957145690918, 'learning_rate': 3.058529998626393e-06, 'epoch': 3.44}\n",
      "{'loss': 1.8551, 'grad_norm': 5.101169586181641, 'learning_rate': 2.9888827891914384e-06, 'epoch': 3.46}\n",
      "{'loss': 1.8971, 'grad_norm': 5.280575752258301, 'learning_rate': 2.9183926635163255e-06, 'epoch': 3.48}\n",
      "{'loss': 1.8795, 'grad_norm': 4.715389728546143, 'learning_rate': 2.8484987339528776e-06, 'epoch': 3.5}\n",
      "{'loss': 1.878, 'grad_norm': 4.797547340393066, 'learning_rate': 2.7792128772881056e-06, 'epoch': 3.52}\n",
      "{'loss': 1.876, 'grad_norm': 5.4920334815979, 'learning_rate': 2.710546866981703e-06, 'epoch': 3.54}\n",
      "{'loss': 1.8707, 'grad_norm': 4.800484657287598, 'learning_rate': 2.6425123711654304e-06, 'epoch': 3.56}\n",
      "{'loss': 1.871, 'grad_norm': 6.065979480743408, 'learning_rate': 2.5751209506603953e-06, 'epoch': 3.58}\n",
      "{'loss': 1.845, 'grad_norm': 4.805615425109863, 'learning_rate': 2.508384057012578e-06, 'epoch': 3.6}\n",
      "{'loss': 1.8375, 'grad_norm': 11.507800102233887, 'learning_rate': 2.4423130305469045e-06, 'epoch': 3.62}\n",
      "{'loss': 1.8549, 'grad_norm': 4.973620414733887, 'learning_rate': 2.3769190984402366e-06, 'epoch': 3.64}\n",
      "{'loss': 1.8912, 'grad_norm': 7.298279285430908, 'learning_rate': 2.312213372813596e-06, 'epoch': 3.66}\n",
      "{'loss': 1.8737, 'grad_norm': 4.231501579284668, 'learning_rate': 2.248206848843909e-06, 'epoch': 3.68}\n",
      "{'loss': 1.8517, 'grad_norm': 15.373030662536621, 'learning_rate': 2.1849104028956495e-06, 'epoch': 3.7}\n",
      "{'loss': 1.9089, 'grad_norm': 4.6144914627075195, 'learning_rate': 2.122334790672667e-06, 'epoch': 3.7199999999999998}\n",
      "{'loss': 1.8805, 'grad_norm': 4.191294193267822, 'learning_rate': 2.060490645390509e-06, 'epoch': 3.74}\n",
      "{'loss': 1.9244, 'grad_norm': 4.841843605041504, 'learning_rate': 1.999388475969568e-06, 'epoch': 3.76}\n",
      "{'loss': 1.8435, 'grad_norm': 4.6040520668029785, 'learning_rate': 1.939038665249332e-06, 'epoch': 3.7800000000000002}\n",
      "{'loss': 1.8876, 'grad_norm': 7.603415489196777, 'learning_rate': 1.8794514682240916e-06, 'epoch': 3.8}\n",
      "{'loss': 1.8729, 'grad_norm': 8.839838981628418, 'learning_rate': 1.820637010300329e-06, 'epoch': 3.82}\n",
      "{'loss': 1.8391, 'grad_norm': 4.345241546630859, 'learning_rate': 1.7626052855761654e-06, 'epoch': 3.84}\n",
      "{'loss': 1.8436, 'grad_norm': 7.6414666175842285, 'learning_rate': 1.7053661551430977e-06, 'epoch': 3.86}\n",
      "{'loss': 1.892, 'grad_norm': 4.058588981628418, 'learning_rate': 1.6489293454103435e-06, 'epoch': 3.88}\n",
      "{'loss': 1.9296, 'grad_norm': 5.076733112335205, 'learning_rate': 1.5933044464520815e-06, 'epoch': 3.9}\n",
      "{'loss': 1.8517, 'grad_norm': 5.214784622192383, 'learning_rate': 1.5385009103778359e-06, 'epoch': 3.92}\n",
      "{'loss': 1.8702, 'grad_norm': 7.3695068359375, 'learning_rate': 1.484528049726331e-06, 'epoch': 3.94}\n",
      "{'loss': 1.8591, 'grad_norm': 4.7635602951049805, 'learning_rate': 1.4313950358830416e-06, 'epoch': 3.96}\n",
      "{'loss': 1.8754, 'grad_norm': 4.597871780395508, 'learning_rate': 1.3791108975217406e-06, 'epoch': 3.98}\n",
      "{'loss': 1.8654, 'grad_norm': 6.297711372375488, 'learning_rate': 1.327684519070293e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 11:00:17,091 - INFO - Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0270698070526123, 'eval_bleu-1': 0.2761500317205484, 'eval_bleu-2': 0.15425701872424658, 'eval_bleu-3': 0.10619294363686237, 'eval_bleu-4': 0.07559452968756013, 'eval_rouge-l': 0.2341221712719213, 'eval_meteor': 0.24466703876821397, 'eval_bertscore': 0.8797650337219238, 'eval_runtime': 2073.9605, 'eval_samples_per_second': 4.822, 'eval_steps_per_second': 2.411, 'epoch': 4.0}\n",
      "{'loss': 1.8125, 'grad_norm': 6.050045967102051, 'learning_rate': 1.2771246392009642e-06, 'epoch': 4.02}\n",
      "{'loss': 1.8176, 'grad_norm': 5.4402899742126465, 'learning_rate': 1.2274398493454873e-06, 'epoch': 4.04}\n",
      "{'loss': 1.8312, 'grad_norm': 4.517246723175049, 'learning_rate': 1.178638592235167e-06, 'epoch': 4.06}\n",
      "{'loss': 1.7958, 'grad_norm': 6.796860218048096, 'learning_rate': 1.1307291604662274e-06, 'epoch': 4.08}\n",
      "{'loss': 1.8404, 'grad_norm': 4.849963188171387, 'learning_rate': 1.0837196950907004e-06, 'epoch': 4.1}\n",
      "{'loss': 1.8371, 'grad_norm': 4.885039329528809, 'learning_rate': 1.0376181842330367e-06, 'epoch': 4.12}\n",
      "{'loss': 1.8635, 'grad_norm': 4.763210296630859, 'learning_rate': 9.924324617327247e-07, 'epoch': 4.14}\n",
      "{'loss': 1.8479, 'grad_norm': 5.295217037200928, 'learning_rate': 9.481702058131168e-07, 'epoch': 4.16}\n",
      "{'loss': 1.853, 'grad_norm': 4.56179141998291, 'learning_rate': 9.048389377766986e-07, 'epoch': 4.18}\n",
      "{'loss': 1.8354, 'grad_norm': 6.209224700927734, 'learning_rate': 8.624460207270305e-07, 'epoch': 4.2}\n",
      "{'loss': 1.8057, 'grad_norm': 10.437525749206543, 'learning_rate': 8.20998658317558e-07, 'epoch': 4.22}\n",
      "{'loss': 1.861, 'grad_norm': 5.839334487915039, 'learning_rate': 7.805038935275322e-07, 'epoch': 4.24}\n",
      "{'loss': 1.8267, 'grad_norm': 4.179052829742432, 'learning_rate': 7.409686074652196e-07, 'epoch': 4.26}\n",
      "{'loss': 1.877, 'grad_norm': 9.359457015991211, 'learning_rate': 7.023995181986259e-07, 'epoch': 4.28}\n",
      "{'loss': 1.8188, 'grad_norm': 4.937790870666504, 'learning_rate': 6.648031796139246e-07, 'epoch': 4.3}\n",
      "{'loss': 1.851, 'grad_norm': 5.011951923370361, 'learning_rate': 6.281859803017771e-07, 'epoch': 4.32}\n",
      "{'loss': 1.8189, 'grad_norm': 4.805720329284668, 'learning_rate': 5.925541424717506e-07, 'epoch': 4.34}\n",
      "{'loss': 1.8429, 'grad_norm': 5.0485005378723145, 'learning_rate': 5.579137208950034e-07, 'epoch': 4.36}\n",
      "{'loss': 1.8398, 'grad_norm': 7.11697244644165, 'learning_rate': 5.242706018754214e-07, 'epoch': 4.38}\n",
      "{'loss': 1.8521, 'grad_norm': 6.884678840637207, 'learning_rate': 4.916305022493793e-07, 'epoch': 4.4}\n",
      "{'loss': 1.855, 'grad_norm': 4.868533134460449, 'learning_rate': 4.5999896841430817e-07, 'epoch': 4.42}\n",
      "{'loss': 1.8776, 'grad_norm': 4.593333721160889, 'learning_rate': 4.2938137538621543e-07, 'epoch': 4.44}\n",
      "{'loss': 1.8249, 'grad_norm': 5.262930870056152, 'learning_rate': 3.997829258863315e-07, 'epoch': 4.46}\n",
      "{'loss': 1.807, 'grad_norm': 5.358160495758057, 'learning_rate': 3.712086494570322e-07, 'epoch': 4.48}\n",
      "{'loss': 1.8481, 'grad_norm': 4.664737224578857, 'learning_rate': 3.4366340160719334e-07, 'epoch': 4.5}\n",
      "{'loss': 1.8547, 'grad_norm': 4.601387023925781, 'learning_rate': 3.171518629871057e-07, 'epoch': 4.52}\n",
      "{'loss': 1.8382, 'grad_norm': 4.512726306915283, 'learning_rate': 2.9167853859312203e-07, 'epoch': 4.54}\n",
      "{'loss': 1.8435, 'grad_norm': 4.549055576324463, 'learning_rate': 2.6724775700213207e-07, 'epoch': 4.5600000000000005}\n",
      "{'loss': 1.8884, 'grad_norm': 6.938117980957031, 'learning_rate': 2.4386366963603014e-07, 'epoch': 4.58}\n",
      "{'loss': 1.8169, 'grad_norm': 5.2090301513671875, 'learning_rate': 2.2153025005628156e-07, 'epoch': 4.6}\n",
      "{'loss': 1.8453, 'grad_norm': 6.3415398597717285, 'learning_rate': 2.0025129328870947e-07, 'epoch': 4.62}\n",
      "{'loss': 1.8378, 'grad_norm': 5.430281162261963, 'learning_rate': 1.800304151786234e-07, 'epoch': 4.64}\n",
      "{'loss': 1.8202, 'grad_norm': 5.1313982009887695, 'learning_rate': 1.608710517763936e-07, 'epoch': 4.66}\n",
      "{'loss': 1.8626, 'grad_norm': 4.735479354858398, 'learning_rate': 1.4277645875357623e-07, 'epoch': 4.68}\n",
      "{'loss': 1.8293, 'grad_norm': 4.682387828826904, 'learning_rate': 1.2574971084969126e-07, 'epoch': 4.7}\n",
      "{'loss': 1.8002, 'grad_norm': 7.287736415863037, 'learning_rate': 1.0979370134974369e-07, 'epoch': 4.72}\n",
      "{'loss': 1.8278, 'grad_norm': 4.731290340423584, 'learning_rate': 9.491114159258046e-08, 'epoch': 4.74}\n",
      "{'loss': 1.8291, 'grad_norm': 4.88568115234375, 'learning_rate': 8.110456051016387e-08, 'epoch': 4.76}\n",
      "{'loss': 1.8412, 'grad_norm': 6.085586071014404, 'learning_rate': 6.837630419783976e-08, 'epoch': 4.78}\n",
      "{'loss': 1.827, 'grad_norm': 5.86466646194458, 'learning_rate': 5.67285355156765e-08, 'epoch': 4.8}\n",
      "{'loss': 1.8337, 'grad_norm': 7.291390419006348, 'learning_rate': 4.616323372093991e-08, 'epoch': 4.82}\n",
      "{'loss': 1.8328, 'grad_norm': 9.995380401611328, 'learning_rate': 3.668219413176169e-08, 'epoch': 4.84}\n",
      "{'loss': 1.8519, 'grad_norm': 4.697607517242432, 'learning_rate': 2.8287027822073013e-08, 'epoch': 4.86}\n",
      "{'loss': 1.8335, 'grad_norm': 13.510293006896973, 'learning_rate': 2.097916134783936e-08, 'epoch': 4.88}\n",
      "{'loss': 1.8283, 'grad_norm': 4.827958583831787, 'learning_rate': 1.4759836504653206e-08, 'epoch': 4.9}\n",
      "{'loss': 1.8363, 'grad_norm': 4.760256767272949, 'learning_rate': 9.630110116719348e-09, 'epoch': 4.92}\n",
      "{'loss': 1.8535, 'grad_norm': 6.679068565368652, 'learning_rate': 5.590853857272974e-09, 'epoch': 4.9399999999999995}\n",
      "{'loss': 1.8458, 'grad_norm': 17.19512367248535, 'learning_rate': 2.642754100461053e-09, 'epoch': 4.96}\n",
      "{'loss': 1.8631, 'grad_norm': 4.306008815765381, 'learning_rate': 7.863118047099247e-10, 'epoch': 4.98}\n",
      "{'loss': 1.854, 'grad_norm': 5.317963600158691, 'learning_rate': 2.1842427597633444e-11, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 12:55:46,301 - INFO - Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0252857208251953, 'eval_bleu-1': 0.27332187196680857, 'eval_bleu-2': 0.15301182716600392, 'eval_bleu-3': 0.10548378318198912, 'eval_bleu-4': 0.07516052867158196, 'eval_rouge-l': 0.23357904550707687, 'eval_meteor': 0.2461827601471226, 'eval_bertscore': 0.8796655535697937, 'eval_runtime': 2139.2749, 'eval_samples_per_second': 4.674, 'eval_steps_per_second': 2.337, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "2025-07-24 12:56:23,212 - INFO - Final training completed successfully\n",
      "2025-07-24 12:56:23,222 - INFO - VRAM usage after final training: 4.43GB / 7.96GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 34686.8464, 'train_samples_per_second': 11.532, 'train_steps_per_second': 0.36, 'train_loss': 2.013610836791992, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1615"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train final model with best hyperparameters\n",
    "logger.info(\"Training final model with best hyperparameters\")\n",
    "best_params = study.best_params\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_PATH, trust_remote_code=True, use_safetensors=True)\n",
    "model.to(device)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.generation_config.no_repeat_ngram_size = 3\n",
    "model.generation_config.min_length = 5\n",
    "\n",
    "# Log VRAM usage before final training\n",
    "if torch.cuda.is_available():\n",
    "    vram_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    vram_total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
    "    logger.info(f\"VRAM usage before final training: {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "\n",
    "# Define training arguments with best parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(OUTPUT_DIR, \"final_model\"),\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=torch.cuda.is_available(),\n",
    "    lr_scheduler_type=best_params[\"lr_scheduler_type\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    warmup_steps=best_params[\"warmup_steps\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    generation_num_beams=3,\n",
    "    group_by_length=True,\n",
    "    skip_memory_metrics=True,\n",
    "    disable_tqdm=True,\n",
    ")\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    logger.info(\"Starting final training\")\n",
    "    with torch.amp.autocast('cuda'):  # Updated to use correct autocast API\n",
    "        trainer.train()\n",
    "    logger.info(\"Final training completed successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Final training failed: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Log VRAM usage after training\n",
    "if torch.cuda.is_available():\n",
    "    vram_used = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    logger.info(f\"VRAM usage after final training: {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "\n",
    "# Clean up memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8eb3b74-bd64-4028-9591-c267a0bb8667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 16:09:04,593 - INFO - Saved model and tokenizer to D:\\A_CSE499\\outputLarge_B_phase2\\final_model\\reload_model\n"
     ]
    }
   ],
   "source": [
    "# Save final model and tokenizer\n",
    "reload_model_dir = os.path.join(final_model_dir, \"reload_model\")\n",
    "os.makedirs(reload_model_dir, exist_ok=True)\n",
    "try:\n",
    "    model.save_pretrained(reload_model_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(reload_model_dir)\n",
    "    necessary_files = [\n",
    "        \"model.safetensors\",\n",
    "        \"config.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\",\n",
    "        \"special_tokens_map.json\"\n",
    "    ]\n",
    "    for file in os.listdir(reload_model_dir):\n",
    "        if file not in necessary_files:\n",
    "            os.remove(os.path.join(reload_model_dir, file))\n",
    "    logger.info(f\"Saved model and tokenizer to {reload_model_dir}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save model/tokenizer to {reload_model_dir}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddc48eb1-b347-4850-a1d9-02f07a073bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 19:36:56,669 - INFO - Starting prediction on test set...\n",
      "2025-07-24 20:19:05,396 - INFO - Using default tokenizer.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-07-24 20:20:19,691 - INFO - Prediction completed in 43.38 minutes.\n",
      "2025-07-24 20:20:19,696 - INFO - Saved test results to D:\\A_CSE499\\outputLarge_B_phase2\\test_results.csv\n",
      "2025-07-24 20:20:19,696 - INFO - Decoding predictions...\n",
      "Decoding Predictions: 100%|██████████| 10000/10000 [00:00<00:00, 999643.45it/s]\n",
      "2025-07-24 20:20:19,865 - INFO - Decoding references...\n",
      "Decoding References: 100%|██████████| 10000/10000 [00:00<00:00, 1466796.29it/s]\n",
      "2025-07-24 20:20:20,032 - INFO - Decoding original context inputs...\n",
      "Decoding Contexts: 100%|██████████| 10000/10000 [00:01<00:00, 5434.00it/s]\n",
      "2025-07-24 20:20:21,935 - INFO - Saved test predictions to D:\\A_CSE499\\outputLarge_B_phase2\\test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set output directory\n",
    "OUTPUT_DIR = r\"D:\\A_CSE499\\outputLarge_B_phase2\"  # Use raw string to handle backslashes\n",
    "\n",
    "# Evaluate on test set and save results\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logger.info(\"Starting prediction on test set...\")\n",
    "start = time.time()\n",
    "test_results = trainer.predict(processed_test_dataset)\n",
    "end = time.time()\n",
    "logger.info(f\"Prediction completed in {(end - start)/60:.2f} minutes.\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Save test metrics\n",
    "test_metrics = test_results.metrics\n",
    "test_results_df = pd.DataFrame([test_metrics])\n",
    "test_results_path = os.path.join(OUTPUT_DIR, \"test_results.csv\")\n",
    "try:\n",
    "    test_results_df.to_csv(test_results_path, index=False)\n",
    "    logger.info(f\"Saved test results to {test_results_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save test results: {e}\")\n",
    "    raise\n",
    "\n",
    "# Handle predictions\n",
    "test_preds = test_results.predictions[0] if isinstance(test_results.predictions, tuple) else test_results.predictions\n",
    "test_preds = np.clip(test_preds, 0, tokenizer.vocab_size - 1)\n",
    "\n",
    "# Decode with progress bars\n",
    "logger.info(\"Decoding predictions...\")\n",
    "decoded_preds = tokenizer.batch_decode(\n",
    "    list(tqdm(test_preds, desc=\"Decoding Predictions\")), skip_special_tokens=True\n",
    ")\n",
    "\n",
    "label_ids = np.clip(test_results.label_ids, 0, tokenizer.vocab_size - 1)\n",
    "logger.info(\"Decoding references...\")\n",
    "decoded_refs = tokenizer.batch_decode(\n",
    "    list(tqdm(label_ids, desc=\"Decoding References\")), skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Reconstruct context from input_ids\n",
    "logger.info(\"Decoding original context inputs...\")\n",
    "decoded_contexts = [\n",
    "    tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "    for sample in tqdm(processed_test_dataset, desc=\"Decoding Contexts\")\n",
    "]\n",
    "\n",
    "# Save predictions and references\n",
    "pred_ref_df = pd.DataFrame({\n",
    "    \"context\": decoded_contexts,\n",
    "    \"predicted_question\": decoded_preds,\n",
    "    \"reference_question\": decoded_refs\n",
    "})\n",
    "pred_ref_path = os.path.join(OUTPUT_DIR, \"test_predictions.csv\")\n",
    "try:\n",
    "    pred_ref_df.to_csv(pred_ref_path, index=False)\n",
    "    logger.info(f\"Saved test predictions to {pred_ref_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save test predictions: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f130299-aa43-4ea2-aabb-a963fddd5518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 20:33:57,316 - INFO - \n",
      "Example Predictions and References:\n",
      "2025-07-24 20:33:57,316 - INFO - \n",
      "Example 1:\n",
      "2025-07-24 20:33:57,317 - INFO - Context: we used it on our ford f250, it runs off the truck battery — lifts 1000lbs easily probably more, works great, goes flat with ground and brings whatever level w truck bed.\n",
      "2025-07-24 20:33:57,318 - INFO - Prediction: What may happen after seeing JM?\n",
      "2025-07-24 20:33:57,318 - INFO - Reference: How would you feel if you two decided to be more than friends?\n",
      "2025-07-24 20:33:57,319 - INFO - \n",
      "Example 2:\n",
      "2025-07-24 20:33:57,319 - INFO - Context: Michelle Trachtenberg: Michelle Christine Trachtenberg (born October 11, 1985) is an American actress. She is known for portraying Nona F. Mecklenberg in\"The Adventures of Pete & Pete\"(1994 — 96), Dawn Summers in\"Buffy the Vampire Slayer\"(2000 —\n",
      "2025-07-24 20:33:57,320 - INFO - Prediction: Are Erin Wiedner and Jessie Bear both directors?\n",
      "2025-07-24 20:33:57,320 - INFO - Reference: Which director is American, Erin Wiedner or Dodo Abashidze?\n",
      "2025-07-24 20:33:57,320 - INFO - \n",
      "Example 3:\n",
      "2025-07-24 20:33:57,321 - INFO - Context: Irma Raush: Irma Yakovlevna Raush (Russian: Ирма Яковлевна Рауш; born 21 April 1938) is a Russian actress and the first wife of film director Andrei Tarkovsky. She is best\n",
      "2025-07-24 20:33:57,321 - INFO - Prediction: What did Curtis want to be when he grew up?\n",
      "2025-07-24 20:33:57,321 - INFO - Reference: Where did Curtis live?\n",
      "2025-07-24 20:33:57,322 - INFO - \n",
      "Example 4:\n",
      "2025-07-24 20:33:57,322 - INFO - Context: Fortissimo (song):\"Fortissimo\"is a 1966 song brought to success by Rita Pavone. The music was composed by Bruno Canfora, while the lyrics were written by director and screenwriter Lina Wertmüller, at the time a close collaborator of Pavone, after\n",
      "2025-07-24 20:33:57,323 - INFO - Prediction: What can you tell about population of the city where the Tinley Park — Oak Park Avenue Station is located?\n",
      "2025-07-24 20:33:57,323 - INFO - Reference: 432 Park Avenue and One Vanderbilt, are skyscrapers in which city?\n",
      "2025-07-24 20:33:57,324 - INFO - \n",
      "Example 5:\n",
      "2025-07-24 20:33:57,325 - INFO - Context: PlayStation 3 uses the Cell microprocessor, designed by Sony, Toshiba and IBM, as its CPU, which is made up of one 3. 2 GHz PowerPC — based\"Power Processing Element\"(PPE) and eight Synergistic Processing Elements (SPEs). The eighth SPE is disabled to\n",
      "2025-07-24 20:33:57,325 - INFO - Prediction: Who was born first, Sidney Meyers or James Agee?\n",
      "2025-07-24 20:33:57,326 - INFO - Reference: Agee is a 1980 American documentary film about a writer born in which year?\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_examples = min(5, len(decoded_preds), len(decoded_refs), len(decoded_contexts))\n",
    "    logger.info(\"\\nExample Predictions and References:\")\n",
    "    for i in range(num_examples):\n",
    "        logger.info(f\"\\nExample {i+1}:\")\n",
    "        logger.info(f\"Context: {decoded_contexts[i]}\")\n",
    "        logger.info(f\"Prediction: {decoded_preds[i]}\")\n",
    "        logger.info(f\"Reference: {decoded_refs[i]}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to print example predictions: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f98111-e7f9-4738-a277-61ad034547bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
